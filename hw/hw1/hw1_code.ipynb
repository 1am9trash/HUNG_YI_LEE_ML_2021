{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQdDrCZqIdYT"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mktJJgX5IuCV"
      },
      "source": [
        "Author: Chih-Yuan Chuang (r09921006)\n",
        "\n",
        "Slides: https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.pdf  \n",
        "Video: TBA\n",
        "\n",
        "Objectives:\n",
        "* Solve a regression problem with deep neural networks (DNN).\n",
        "* Understand basic DNN training tips.\n",
        "* Get familiar with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaHTcvp9aTXx"
      },
      "source": [
        "# **基於baseline的分析與改善**\n",
        "\n",
        "* **feature選取**\n",
        "  * 用panda觀察fature分佈，分析feature跟target間的相關度\n",
        "  * 方案一：保留前四十個one hot feature跟相關度大於0.5的feature做training\n",
        "  * 方案二：僅用相關度大於0.5的feature做training\n",
        "  * 在合適的參數下，兩者都可達到strong baseline\n",
        "* **損失函數**\n",
        "  * 將原本的MSE改為RMSE\n",
        "  * 實作後發現基本上不影響的training結果\n",
        "* **L1/L2正則，避免overfitting**\n",
        "  * 在很大程度下，減少了train loss跟valid loss的差距，大概可以從誤差0.5減小到0.01\n",
        "  * L1正則的表現相較L2更好一些，但在合適的alpha下，都可以逼近strong baseline\n",
        "* **network架構**\n",
        "  * 在層數上的修改，基本上一定會overfitting\n",
        "  * 在寬度上的修改則沒有太大的影響（一方面是我懶得慢慢調參）\n",
        "* **超參數**\n",
        "  * batch size不作調整，基本上只影響收斂速度跟震盪\n",
        "  * learning rate改得更小，主要是方便我觀察變化\n",
        "  * optimizer試了下adam，結果更差，其他就都沒什麼調了，~~主要也是懶~~\n",
        "* **歸一化**\n",
        "  * 原本範例code中是不同的dataset分別使用自己的mean跟std做歸一，這是不合理的，應該使用全部資料的mean跟data做歸一，才不會有太大的偏差\n",
        "* **Train跟Valid dataset切分**\n",
        "  * 改為隨機切分，保證資料分布相同\n",
        "  * 實作後發現沒有太大的差距\n",
        "\n",
        "# **Result**\n",
        "* 將結果上傳kaggle後，分數為0.87850，成功pass strong baseline\n",
        "* 事實上如果你要邪教操作，overfit public LB的話，可以達到0.86出頭的loss，但基於不理會test data的原則，基本上是按local loss來調整模型的，也因此事實上上傳的並非在test data上最好的結果\n",
        "\n",
        "# **總結**\n",
        "* 原本以為第一個作業不會太難，因此隨便用keras寫了一個model，結果loss直接飆到1.9，只好好好的分析，於是我暑假開始的第一個週末就沒了\n",
        "* 誠如李宏毅李宏毅教授所言，誰會在禮拜五晚上學ML啊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pumi6wOJ0sV"
      },
      "source": [
        "# **Download Data**\n",
        "\n",
        "If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTARgCX8O82R",
        "outputId": "b0634a96-6682-4bc7-eaaf-c4fdc01b304d"
      },
      "source": [
        "train_path = \"covid.train.csv\"\n",
        "test_path = \"covid.test.csv\"\n",
        "\n",
        "!gdown --id \"19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\" --output covid.train.csv\n",
        "!gdown --id \"1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\" --output covid.test.csv"
      ],
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 9.34MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 3.98MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXAu4AojJzHp"
      },
      "source": [
        "# **Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRKwNwqKc4O"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 讀取測資\n",
        "train_data = pd.read_csv(train_path)\n",
        "test_data = pd.read_csv(test_path)"
      ],
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBiMcHKDLoPP",
        "outputId": "cb947b4e-7fa5-4067-f149-517ce43291bb"
      },
      "source": [
        "# 資料筆數\n",
        "print (\"Train: {:4d}\".format(len(train_data)))\n",
        "print (\" Test: {:4d}\".format(len(test_data)))"
      ],
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 2700\n",
            " Test:  893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pcDywhpLbhA",
        "outputId": "01b0f7b6-bb08-4d23-ee6e-a2fa0258cd06"
      },
      "source": [
        "# feature名稱、型別、空值\n",
        "print (train_data.info())"
      ],
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2700 entries, 0 to 2699\n",
            "Data columns (total 95 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   id                      2700 non-null   int64  \n",
            " 1   AL                      2700 non-null   float64\n",
            " 2   AK                      2700 non-null   float64\n",
            " 3   AZ                      2700 non-null   float64\n",
            " 4   AR                      2700 non-null   float64\n",
            " 5   CA                      2700 non-null   float64\n",
            " 6   CO                      2700 non-null   float64\n",
            " 7   CT                      2700 non-null   float64\n",
            " 8   FL                      2700 non-null   float64\n",
            " 9   GA                      2700 non-null   float64\n",
            " 10  ID                      2700 non-null   float64\n",
            " 11  IL                      2700 non-null   float64\n",
            " 12  IN                      2700 non-null   float64\n",
            " 13  IA                      2700 non-null   float64\n",
            " 14  KS                      2700 non-null   float64\n",
            " 15  KY                      2700 non-null   float64\n",
            " 16  LA                      2700 non-null   float64\n",
            " 17  MD                      2700 non-null   float64\n",
            " 18  MA                      2700 non-null   float64\n",
            " 19  MI                      2700 non-null   float64\n",
            " 20  MN                      2700 non-null   float64\n",
            " 21  MS                      2700 non-null   float64\n",
            " 22  MO                      2700 non-null   float64\n",
            " 23  NE                      2700 non-null   float64\n",
            " 24  NV                      2700 non-null   float64\n",
            " 25  NJ                      2700 non-null   float64\n",
            " 26  NM                      2700 non-null   float64\n",
            " 27  NY                      2700 non-null   float64\n",
            " 28  NC                      2700 non-null   float64\n",
            " 29  OH                      2700 non-null   float64\n",
            " 30  OK                      2700 non-null   float64\n",
            " 31  OR                      2700 non-null   float64\n",
            " 32  PA                      2700 non-null   float64\n",
            " 33  RI                      2700 non-null   float64\n",
            " 34  SC                      2700 non-null   float64\n",
            " 35  TX                      2700 non-null   float64\n",
            " 36  UT                      2700 non-null   float64\n",
            " 37  VA                      2700 non-null   float64\n",
            " 38  WA                      2700 non-null   float64\n",
            " 39  WV                      2700 non-null   float64\n",
            " 40  WI                      2700 non-null   float64\n",
            " 41  cli                     2700 non-null   float64\n",
            " 42  ili                     2700 non-null   float64\n",
            " 43  hh_cmnty_cli            2700 non-null   float64\n",
            " 44  nohh_cmnty_cli          2700 non-null   float64\n",
            " 45  wearing_mask            2700 non-null   float64\n",
            " 46  travel_outside_state    2700 non-null   float64\n",
            " 47  work_outside_home       2700 non-null   float64\n",
            " 48  shop                    2700 non-null   float64\n",
            " 49  restaurant              2700 non-null   float64\n",
            " 50  spent_time              2700 non-null   float64\n",
            " 51  large_event             2700 non-null   float64\n",
            " 52  public_transit          2700 non-null   float64\n",
            " 53  anxious                 2700 non-null   float64\n",
            " 54  depressed               2700 non-null   float64\n",
            " 55  felt_isolated           2700 non-null   float64\n",
            " 56  worried_become_ill      2700 non-null   float64\n",
            " 57  worried_finances        2700 non-null   float64\n",
            " 58  tested_positive         2700 non-null   float64\n",
            " 59  cli.1                   2700 non-null   float64\n",
            " 60  ili.1                   2700 non-null   float64\n",
            " 61  hh_cmnty_cli.1          2700 non-null   float64\n",
            " 62  nohh_cmnty_cli.1        2700 non-null   float64\n",
            " 63  wearing_mask.1          2700 non-null   float64\n",
            " 64  travel_outside_state.1  2700 non-null   float64\n",
            " 65  work_outside_home.1     2700 non-null   float64\n",
            " 66  shop.1                  2700 non-null   float64\n",
            " 67  restaurant.1            2700 non-null   float64\n",
            " 68  spent_time.1            2700 non-null   float64\n",
            " 69  large_event.1           2700 non-null   float64\n",
            " 70  public_transit.1        2700 non-null   float64\n",
            " 71  anxious.1               2700 non-null   float64\n",
            " 72  depressed.1             2700 non-null   float64\n",
            " 73  felt_isolated.1         2700 non-null   float64\n",
            " 74  worried_become_ill.1    2700 non-null   float64\n",
            " 75  worried_finances.1      2700 non-null   float64\n",
            " 76  tested_positive.1       2700 non-null   float64\n",
            " 77  cli.2                   2700 non-null   float64\n",
            " 78  ili.2                   2700 non-null   float64\n",
            " 79  hh_cmnty_cli.2          2700 non-null   float64\n",
            " 80  nohh_cmnty_cli.2        2700 non-null   float64\n",
            " 81  wearing_mask.2          2700 non-null   float64\n",
            " 82  travel_outside_state.2  2700 non-null   float64\n",
            " 83  work_outside_home.2     2700 non-null   float64\n",
            " 84  shop.2                  2700 non-null   float64\n",
            " 85  restaurant.2            2700 non-null   float64\n",
            " 86  spent_time.2            2700 non-null   float64\n",
            " 87  large_event.2           2700 non-null   float64\n",
            " 88  public_transit.2        2700 non-null   float64\n",
            " 89  anxious.2               2700 non-null   float64\n",
            " 90  depressed.2             2700 non-null   float64\n",
            " 91  felt_isolated.2         2700 non-null   float64\n",
            " 92  worried_become_ill.2    2700 non-null   float64\n",
            " 93  worried_finances.2      2700 non-null   float64\n",
            " 94  tested_positive.2       2700 non-null   float64\n",
            "dtypes: float64(94), int64(1)\n",
            "memory usage: 2.0 MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "BC4cEigEMHPY",
        "outputId": "bd506a29-420f-44ae-fe90-1f3f03ee7453"
      },
      "source": [
        "# 觀察上下屆及均值，去掉id跟state資料\n",
        "train_data.iloc[:, 41:].describe()"
      ],
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cli</th>\n",
              "      <th>ili</th>\n",
              "      <th>hh_cmnty_cli</th>\n",
              "      <th>nohh_cmnty_cli</th>\n",
              "      <th>wearing_mask</th>\n",
              "      <th>travel_outside_state</th>\n",
              "      <th>work_outside_home</th>\n",
              "      <th>shop</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>spent_time</th>\n",
              "      <th>large_event</th>\n",
              "      <th>public_transit</th>\n",
              "      <th>anxious</th>\n",
              "      <th>depressed</th>\n",
              "      <th>felt_isolated</th>\n",
              "      <th>worried_become_ill</th>\n",
              "      <th>worried_finances</th>\n",
              "      <th>tested_positive</th>\n",
              "      <th>cli.1</th>\n",
              "      <th>ili.1</th>\n",
              "      <th>hh_cmnty_cli.1</th>\n",
              "      <th>nohh_cmnty_cli.1</th>\n",
              "      <th>wearing_mask.1</th>\n",
              "      <th>travel_outside_state.1</th>\n",
              "      <th>work_outside_home.1</th>\n",
              "      <th>shop.1</th>\n",
              "      <th>restaurant.1</th>\n",
              "      <th>spent_time.1</th>\n",
              "      <th>large_event.1</th>\n",
              "      <th>public_transit.1</th>\n",
              "      <th>anxious.1</th>\n",
              "      <th>depressed.1</th>\n",
              "      <th>felt_isolated.1</th>\n",
              "      <th>worried_become_ill.1</th>\n",
              "      <th>worried_finances.1</th>\n",
              "      <th>tested_positive.1</th>\n",
              "      <th>cli.2</th>\n",
              "      <th>ili.2</th>\n",
              "      <th>hh_cmnty_cli.2</th>\n",
              "      <th>nohh_cmnty_cli.2</th>\n",
              "      <th>wearing_mask.2</th>\n",
              "      <th>travel_outside_state.2</th>\n",
              "      <th>work_outside_home.2</th>\n",
              "      <th>shop.2</th>\n",
              "      <th>restaurant.2</th>\n",
              "      <th>spent_time.2</th>\n",
              "      <th>large_event.2</th>\n",
              "      <th>public_transit.2</th>\n",
              "      <th>anxious.2</th>\n",
              "      <th>depressed.2</th>\n",
              "      <th>felt_isolated.2</th>\n",
              "      <th>worried_become_ill.2</th>\n",
              "      <th>worried_finances.2</th>\n",
              "      <th>tested_positive.2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "      <td>2700.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.991587</td>\n",
              "      <td>1.016136</td>\n",
              "      <td>29.442496</td>\n",
              "      <td>24.323054</td>\n",
              "      <td>89.682322</td>\n",
              "      <td>8.894498</td>\n",
              "      <td>31.703307</td>\n",
              "      <td>55.277153</td>\n",
              "      <td>16.694342</td>\n",
              "      <td>36.283177</td>\n",
              "      <td>10.352273</td>\n",
              "      <td>2.393285</td>\n",
              "      <td>18.074684</td>\n",
              "      <td>13.075498</td>\n",
              "      <td>19.213321</td>\n",
              "      <td>64.633769</td>\n",
              "      <td>44.519474</td>\n",
              "      <td>16.300893</td>\n",
              "      <td>0.994568</td>\n",
              "      <td>1.019135</td>\n",
              "      <td>29.529305</td>\n",
              "      <td>24.402875</td>\n",
              "      <td>89.736737</td>\n",
              "      <td>8.861371</td>\n",
              "      <td>31.664651</td>\n",
              "      <td>55.198075</td>\n",
              "      <td>16.635440</td>\n",
              "      <td>36.176886</td>\n",
              "      <td>10.304595</td>\n",
              "      <td>2.389372</td>\n",
              "      <td>18.071667</td>\n",
              "      <td>13.067127</td>\n",
              "      <td>19.228457</td>\n",
              "      <td>64.734139</td>\n",
              "      <td>44.544124</td>\n",
              "      <td>16.366695</td>\n",
              "      <td>0.997986</td>\n",
              "      <td>1.022472</td>\n",
              "      <td>29.610807</td>\n",
              "      <td>24.477913</td>\n",
              "      <td>89.790227</td>\n",
              "      <td>8.830759</td>\n",
              "      <td>31.624272</td>\n",
              "      <td>55.119903</td>\n",
              "      <td>16.578290</td>\n",
              "      <td>36.074941</td>\n",
              "      <td>10.257474</td>\n",
              "      <td>2.385735</td>\n",
              "      <td>18.067635</td>\n",
              "      <td>13.058828</td>\n",
              "      <td>19.243283</td>\n",
              "      <td>64.834307</td>\n",
              "      <td>44.568440</td>\n",
              "      <td>16.431280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.420296</td>\n",
              "      <td>0.423629</td>\n",
              "      <td>9.093738</td>\n",
              "      <td>8.446750</td>\n",
              "      <td>5.380027</td>\n",
              "      <td>3.404027</td>\n",
              "      <td>4.928902</td>\n",
              "      <td>4.525917</td>\n",
              "      <td>5.668479</td>\n",
              "      <td>6.675206</td>\n",
              "      <td>4.698705</td>\n",
              "      <td>1.053270</td>\n",
              "      <td>2.248750</td>\n",
              "      <td>1.621328</td>\n",
              "      <td>2.706605</td>\n",
              "      <td>6.232239</td>\n",
              "      <td>5.265787</td>\n",
              "      <td>7.637823</td>\n",
              "      <td>0.420114</td>\n",
              "      <td>0.423538</td>\n",
              "      <td>9.082940</td>\n",
              "      <td>8.443146</td>\n",
              "      <td>5.366067</td>\n",
              "      <td>3.389310</td>\n",
              "      <td>4.916168</td>\n",
              "      <td>4.524887</td>\n",
              "      <td>5.660085</td>\n",
              "      <td>6.664218</td>\n",
              "      <td>4.692479</td>\n",
              "      <td>1.053237</td>\n",
              "      <td>2.249864</td>\n",
              "      <td>1.625269</td>\n",
              "      <td>2.707148</td>\n",
              "      <td>6.226622</td>\n",
              "      <td>5.248787</td>\n",
              "      <td>7.627538</td>\n",
              "      <td>0.420205</td>\n",
              "      <td>0.423705</td>\n",
              "      <td>9.070537</td>\n",
              "      <td>8.437044</td>\n",
              "      <td>5.351574</td>\n",
              "      <td>3.377722</td>\n",
              "      <td>4.901857</td>\n",
              "      <td>4.524442</td>\n",
              "      <td>5.651583</td>\n",
              "      <td>6.655166</td>\n",
              "      <td>4.686263</td>\n",
              "      <td>1.053147</td>\n",
              "      <td>2.250081</td>\n",
              "      <td>1.628589</td>\n",
              "      <td>2.708339</td>\n",
              "      <td>6.220087</td>\n",
              "      <td>5.232030</td>\n",
              "      <td>7.619354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.126321</td>\n",
              "      <td>0.132470</td>\n",
              "      <td>9.961640</td>\n",
              "      <td>6.857181</td>\n",
              "      <td>70.950912</td>\n",
              "      <td>1.252983</td>\n",
              "      <td>18.311941</td>\n",
              "      <td>43.220187</td>\n",
              "      <td>3.637414</td>\n",
              "      <td>21.485815</td>\n",
              "      <td>2.118674</td>\n",
              "      <td>0.728770</td>\n",
              "      <td>12.980786</td>\n",
              "      <td>8.370536</td>\n",
              "      <td>13.400399</td>\n",
              "      <td>48.225603</td>\n",
              "      <td>33.113882</td>\n",
              "      <td>2.338708</td>\n",
              "      <td>0.126321</td>\n",
              "      <td>0.132470</td>\n",
              "      <td>9.961640</td>\n",
              "      <td>6.857181</td>\n",
              "      <td>72.330064</td>\n",
              "      <td>1.252983</td>\n",
              "      <td>18.311941</td>\n",
              "      <td>43.220187</td>\n",
              "      <td>3.637414</td>\n",
              "      <td>21.485815</td>\n",
              "      <td>2.118674</td>\n",
              "      <td>0.728770</td>\n",
              "      <td>12.980786</td>\n",
              "      <td>8.370536</td>\n",
              "      <td>13.400399</td>\n",
              "      <td>48.225603</td>\n",
              "      <td>33.113882</td>\n",
              "      <td>2.338708</td>\n",
              "      <td>0.126321</td>\n",
              "      <td>0.132470</td>\n",
              "      <td>9.961640</td>\n",
              "      <td>6.857181</td>\n",
              "      <td>72.356322</td>\n",
              "      <td>1.252983</td>\n",
              "      <td>18.311941</td>\n",
              "      <td>43.220187</td>\n",
              "      <td>3.637414</td>\n",
              "      <td>21.485815</td>\n",
              "      <td>2.118674</td>\n",
              "      <td>0.728770</td>\n",
              "      <td>12.980786</td>\n",
              "      <td>8.370536</td>\n",
              "      <td>13.400399</td>\n",
              "      <td>48.225603</td>\n",
              "      <td>33.113882</td>\n",
              "      <td>2.338708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.673929</td>\n",
              "      <td>0.697515</td>\n",
              "      <td>23.203165</td>\n",
              "      <td>18.539153</td>\n",
              "      <td>86.309537</td>\n",
              "      <td>6.177754</td>\n",
              "      <td>28.247865</td>\n",
              "      <td>51.547206</td>\n",
              "      <td>13.311050</td>\n",
              "      <td>30.740931</td>\n",
              "      <td>6.653427</td>\n",
              "      <td>1.720601</td>\n",
              "      <td>16.420485</td>\n",
              "      <td>11.943953</td>\n",
              "      <td>17.292063</td>\n",
              "      <td>59.529326</td>\n",
              "      <td>40.520369</td>\n",
              "      <td>10.200722</td>\n",
              "      <td>0.676205</td>\n",
              "      <td>0.699773</td>\n",
              "      <td>23.264324</td>\n",
              "      <td>18.607342</td>\n",
              "      <td>86.386111</td>\n",
              "      <td>6.168986</td>\n",
              "      <td>28.202745</td>\n",
              "      <td>51.403036</td>\n",
              "      <td>13.248788</td>\n",
              "      <td>30.646955</td>\n",
              "      <td>6.605724</td>\n",
              "      <td>1.715372</td>\n",
              "      <td>16.423140</td>\n",
              "      <td>11.933745</td>\n",
              "      <td>17.303887</td>\n",
              "      <td>59.703583</td>\n",
              "      <td>40.533768</td>\n",
              "      <td>10.251453</td>\n",
              "      <td>0.680065</td>\n",
              "      <td>0.703390</td>\n",
              "      <td>23.307794</td>\n",
              "      <td>18.644297</td>\n",
              "      <td>86.436468</td>\n",
              "      <td>6.159286</td>\n",
              "      <td>28.187875</td>\n",
              "      <td>51.262363</td>\n",
              "      <td>13.200532</td>\n",
              "      <td>30.606711</td>\n",
              "      <td>6.532543</td>\n",
              "      <td>1.714080</td>\n",
              "      <td>16.420485</td>\n",
              "      <td>11.914167</td>\n",
              "      <td>17.322912</td>\n",
              "      <td>59.782876</td>\n",
              "      <td>40.549987</td>\n",
              "      <td>10.327314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.912747</td>\n",
              "      <td>0.940295</td>\n",
              "      <td>28.955738</td>\n",
              "      <td>23.819761</td>\n",
              "      <td>90.819435</td>\n",
              "      <td>8.288288</td>\n",
              "      <td>32.143140</td>\n",
              "      <td>55.257262</td>\n",
              "      <td>16.371699</td>\n",
              "      <td>36.267966</td>\n",
              "      <td>9.802380</td>\n",
              "      <td>2.204258</td>\n",
              "      <td>17.685476</td>\n",
              "      <td>12.963659</td>\n",
              "      <td>18.735807</td>\n",
              "      <td>65.688024</td>\n",
              "      <td>43.911769</td>\n",
              "      <td>15.479766</td>\n",
              "      <td>0.917343</td>\n",
              "      <td>0.942587</td>\n",
              "      <td>29.061296</td>\n",
              "      <td>23.905188</td>\n",
              "      <td>90.859943</td>\n",
              "      <td>8.274067</td>\n",
              "      <td>32.108420</td>\n",
              "      <td>55.129326</td>\n",
              "      <td>16.293314</td>\n",
              "      <td>36.169954</td>\n",
              "      <td>9.738629</td>\n",
              "      <td>2.203602</td>\n",
              "      <td>17.684970</td>\n",
              "      <td>12.956723</td>\n",
              "      <td>18.745824</td>\n",
              "      <td>65.783579</td>\n",
              "      <td>43.947131</td>\n",
              "      <td>15.572281</td>\n",
              "      <td>0.920815</td>\n",
              "      <td>0.948001</td>\n",
              "      <td>29.137273</td>\n",
              "      <td>24.010817</td>\n",
              "      <td>90.912271</td>\n",
              "      <td>8.251691</td>\n",
              "      <td>32.051128</td>\n",
              "      <td>54.990445</td>\n",
              "      <td>16.227010</td>\n",
              "      <td>36.041389</td>\n",
              "      <td>9.700368</td>\n",
              "      <td>2.199521</td>\n",
              "      <td>17.684197</td>\n",
              "      <td>12.948749</td>\n",
              "      <td>18.760267</td>\n",
              "      <td>65.932258</td>\n",
              "      <td>43.997637</td>\n",
              "      <td>15.646480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.266849</td>\n",
              "      <td>1.302040</td>\n",
              "      <td>36.109114</td>\n",
              "      <td>30.238061</td>\n",
              "      <td>93.937119</td>\n",
              "      <td>11.582209</td>\n",
              "      <td>35.387315</td>\n",
              "      <td>58.866130</td>\n",
              "      <td>21.396971</td>\n",
              "      <td>41.659971</td>\n",
              "      <td>13.734197</td>\n",
              "      <td>2.745406</td>\n",
              "      <td>19.501218</td>\n",
              "      <td>14.214320</td>\n",
              "      <td>20.665840</td>\n",
              "      <td>69.497484</td>\n",
              "      <td>48.098224</td>\n",
              "      <td>22.503685</td>\n",
              "      <td>1.268148</td>\n",
              "      <td>1.301877</td>\n",
              "      <td>36.233383</td>\n",
              "      <td>30.318671</td>\n",
              "      <td>93.955966</td>\n",
              "      <td>11.525572</td>\n",
              "      <td>35.362666</td>\n",
              "      <td>58.797715</td>\n",
              "      <td>21.333613</td>\n",
              "      <td>41.562070</td>\n",
              "      <td>13.684985</td>\n",
              "      <td>2.734372</td>\n",
              "      <td>19.503419</td>\n",
              "      <td>14.214320</td>\n",
              "      <td>20.693846</td>\n",
              "      <td>69.578458</td>\n",
              "      <td>48.108341</td>\n",
              "      <td>22.527315</td>\n",
              "      <td>1.269136</td>\n",
              "      <td>1.304112</td>\n",
              "      <td>36.345667</td>\n",
              "      <td>30.459044</td>\n",
              "      <td>93.975501</td>\n",
              "      <td>11.477910</td>\n",
              "      <td>35.299957</td>\n",
              "      <td>58.752924</td>\n",
              "      <td>21.207162</td>\n",
              "      <td>41.508520</td>\n",
              "      <td>13.602566</td>\n",
              "      <td>2.730469</td>\n",
              "      <td>19.503419</td>\n",
              "      <td>14.214320</td>\n",
              "      <td>20.713638</td>\n",
              "      <td>69.719651</td>\n",
              "      <td>48.118283</td>\n",
              "      <td>22.535165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.597732</td>\n",
              "      <td>2.625885</td>\n",
              "      <td>56.832289</td>\n",
              "      <td>51.550450</td>\n",
              "      <td>98.087160</td>\n",
              "      <td>18.552325</td>\n",
              "      <td>42.359074</td>\n",
              "      <td>65.673889</td>\n",
              "      <td>28.488220</td>\n",
              "      <td>50.606465</td>\n",
              "      <td>24.496711</td>\n",
              "      <td>8.162275</td>\n",
              "      <td>28.574091</td>\n",
              "      <td>18.715944</td>\n",
              "      <td>28.366270</td>\n",
              "      <td>77.701014</td>\n",
              "      <td>58.433600</td>\n",
              "      <td>38.670000</td>\n",
              "      <td>2.597732</td>\n",
              "      <td>2.625885</td>\n",
              "      <td>56.832289</td>\n",
              "      <td>51.550450</td>\n",
              "      <td>98.087160</td>\n",
              "      <td>18.552325</td>\n",
              "      <td>42.359074</td>\n",
              "      <td>65.673889</td>\n",
              "      <td>28.488220</td>\n",
              "      <td>50.606465</td>\n",
              "      <td>24.496711</td>\n",
              "      <td>8.162275</td>\n",
              "      <td>28.574091</td>\n",
              "      <td>18.715944</td>\n",
              "      <td>28.366270</td>\n",
              "      <td>77.701014</td>\n",
              "      <td>58.433600</td>\n",
              "      <td>40.959495</td>\n",
              "      <td>2.597732</td>\n",
              "      <td>2.625885</td>\n",
              "      <td>56.832289</td>\n",
              "      <td>51.550450</td>\n",
              "      <td>98.087160</td>\n",
              "      <td>18.552325</td>\n",
              "      <td>42.359074</td>\n",
              "      <td>65.673889</td>\n",
              "      <td>28.488220</td>\n",
              "      <td>50.606465</td>\n",
              "      <td>24.496711</td>\n",
              "      <td>8.162275</td>\n",
              "      <td>28.574091</td>\n",
              "      <td>18.715944</td>\n",
              "      <td>28.366270</td>\n",
              "      <td>77.701014</td>\n",
              "      <td>58.433600</td>\n",
              "      <td>40.959495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               cli          ili  ...  worried_finances.2  tested_positive.2\n",
              "count  2700.000000  2700.000000  ...         2700.000000        2700.000000\n",
              "mean      0.991587     1.016136  ...           44.568440          16.431280\n",
              "std       0.420296     0.423629  ...            5.232030           7.619354\n",
              "min       0.126321     0.132470  ...           33.113882           2.338708\n",
              "25%       0.673929     0.697515  ...           40.549987          10.327314\n",
              "50%       0.912747     0.940295  ...           43.997637          15.646480\n",
              "75%       1.266849     1.302040  ...           48.118283          22.535165\n",
              "max       2.597732     2.625885  ...           58.433600          40.959495\n",
              "\n",
              "[8 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 427
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "ZOJ2-niZNUCt",
        "outputId": "61b12b84-1f0d-4d3a-cc31-44453d3a9fd2"
      },
      "source": [
        "# 與train資料做對比，判斷是否來自同一分佈\n",
        "test_data.iloc[:, 41:].describe()"
      ],
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cli</th>\n",
              "      <th>ili</th>\n",
              "      <th>hh_cmnty_cli</th>\n",
              "      <th>nohh_cmnty_cli</th>\n",
              "      <th>wearing_mask</th>\n",
              "      <th>travel_outside_state</th>\n",
              "      <th>work_outside_home</th>\n",
              "      <th>shop</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>spent_time</th>\n",
              "      <th>large_event</th>\n",
              "      <th>public_transit</th>\n",
              "      <th>anxious</th>\n",
              "      <th>depressed</th>\n",
              "      <th>felt_isolated</th>\n",
              "      <th>worried_become_ill</th>\n",
              "      <th>worried_finances</th>\n",
              "      <th>tested_positive</th>\n",
              "      <th>cli.1</th>\n",
              "      <th>ili.1</th>\n",
              "      <th>hh_cmnty_cli.1</th>\n",
              "      <th>nohh_cmnty_cli.1</th>\n",
              "      <th>wearing_mask.1</th>\n",
              "      <th>travel_outside_state.1</th>\n",
              "      <th>work_outside_home.1</th>\n",
              "      <th>shop.1</th>\n",
              "      <th>restaurant.1</th>\n",
              "      <th>spent_time.1</th>\n",
              "      <th>large_event.1</th>\n",
              "      <th>public_transit.1</th>\n",
              "      <th>anxious.1</th>\n",
              "      <th>depressed.1</th>\n",
              "      <th>felt_isolated.1</th>\n",
              "      <th>worried_become_ill.1</th>\n",
              "      <th>worried_finances.1</th>\n",
              "      <th>tested_positive.1</th>\n",
              "      <th>cli.2</th>\n",
              "      <th>ili.2</th>\n",
              "      <th>hh_cmnty_cli.2</th>\n",
              "      <th>nohh_cmnty_cli.2</th>\n",
              "      <th>wearing_mask.2</th>\n",
              "      <th>travel_outside_state.2</th>\n",
              "      <th>work_outside_home.2</th>\n",
              "      <th>shop.2</th>\n",
              "      <th>restaurant.2</th>\n",
              "      <th>spent_time.2</th>\n",
              "      <th>large_event.2</th>\n",
              "      <th>public_transit.2</th>\n",
              "      <th>anxious.2</th>\n",
              "      <th>depressed.2</th>\n",
              "      <th>felt_isolated.2</th>\n",
              "      <th>worried_become_ill.2</th>\n",
              "      <th>worried_finances.2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "      <td>893.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.972457</td>\n",
              "      <td>0.991809</td>\n",
              "      <td>29.075682</td>\n",
              "      <td>24.018729</td>\n",
              "      <td>89.637506</td>\n",
              "      <td>9.001325</td>\n",
              "      <td>31.620607</td>\n",
              "      <td>55.422982</td>\n",
              "      <td>16.554387</td>\n",
              "      <td>36.371653</td>\n",
              "      <td>10.356177</td>\n",
              "      <td>2.382769</td>\n",
              "      <td>18.030215</td>\n",
              "      <td>13.020293</td>\n",
              "      <td>19.230715</td>\n",
              "      <td>64.406944</td>\n",
              "      <td>44.379019</td>\n",
              "      <td>15.976544</td>\n",
              "      <td>0.977508</td>\n",
              "      <td>0.997195</td>\n",
              "      <td>29.133016</td>\n",
              "      <td>24.076375</td>\n",
              "      <td>89.715077</td>\n",
              "      <td>8.955668</td>\n",
              "      <td>31.541307</td>\n",
              "      <td>55.360132</td>\n",
              "      <td>16.510614</td>\n",
              "      <td>36.268780</td>\n",
              "      <td>10.309059</td>\n",
              "      <td>2.376621</td>\n",
              "      <td>18.013300</td>\n",
              "      <td>13.007566</td>\n",
              "      <td>19.220921</td>\n",
              "      <td>64.527609</td>\n",
              "      <td>44.386619</td>\n",
              "      <td>15.989196</td>\n",
              "      <td>0.981119</td>\n",
              "      <td>1.000032</td>\n",
              "      <td>29.192015</td>\n",
              "      <td>24.117403</td>\n",
              "      <td>89.765373</td>\n",
              "      <td>8.917700</td>\n",
              "      <td>31.513665</td>\n",
              "      <td>55.268628</td>\n",
              "      <td>16.444916</td>\n",
              "      <td>36.165898</td>\n",
              "      <td>10.248975</td>\n",
              "      <td>2.369115</td>\n",
              "      <td>17.988147</td>\n",
              "      <td>12.993830</td>\n",
              "      <td>19.238723</td>\n",
              "      <td>64.619920</td>\n",
              "      <td>44.411505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.411997</td>\n",
              "      <td>0.415468</td>\n",
              "      <td>9.596290</td>\n",
              "      <td>8.988245</td>\n",
              "      <td>4.733549</td>\n",
              "      <td>3.655616</td>\n",
              "      <td>4.754570</td>\n",
              "      <td>4.366780</td>\n",
              "      <td>5.688802</td>\n",
              "      <td>6.203232</td>\n",
              "      <td>4.521531</td>\n",
              "      <td>1.114568</td>\n",
              "      <td>2.235211</td>\n",
              "      <td>1.715389</td>\n",
              "      <td>2.689158</td>\n",
              "      <td>5.721753</td>\n",
              "      <td>4.579553</td>\n",
              "      <td>7.813659</td>\n",
              "      <td>0.413665</td>\n",
              "      <td>0.418835</td>\n",
              "      <td>9.527793</td>\n",
              "      <td>8.920860</td>\n",
              "      <td>4.708376</td>\n",
              "      <td>3.651532</td>\n",
              "      <td>4.734470</td>\n",
              "      <td>4.374390</td>\n",
              "      <td>5.686128</td>\n",
              "      <td>6.195079</td>\n",
              "      <td>4.508985</td>\n",
              "      <td>1.119861</td>\n",
              "      <td>2.210427</td>\n",
              "      <td>1.705901</td>\n",
              "      <td>2.674568</td>\n",
              "      <td>5.694758</td>\n",
              "      <td>4.612057</td>\n",
              "      <td>7.786780</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.418081</td>\n",
              "      <td>9.467570</td>\n",
              "      <td>8.865726</td>\n",
              "      <td>4.692231</td>\n",
              "      <td>3.637221</td>\n",
              "      <td>4.733639</td>\n",
              "      <td>4.350540</td>\n",
              "      <td>5.656828</td>\n",
              "      <td>6.192274</td>\n",
              "      <td>4.498845</td>\n",
              "      <td>1.114366</td>\n",
              "      <td>2.207022</td>\n",
              "      <td>1.713143</td>\n",
              "      <td>2.687435</td>\n",
              "      <td>5.685865</td>\n",
              "      <td>4.605268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.139558</td>\n",
              "      <td>0.159477</td>\n",
              "      <td>9.171315</td>\n",
              "      <td>6.014740</td>\n",
              "      <td>76.895278</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>18.299198</td>\n",
              "      <td>44.062442</td>\n",
              "      <td>3.800684</td>\n",
              "      <td>21.487077</td>\n",
              "      <td>2.324264</td>\n",
              "      <td>0.785854</td>\n",
              "      <td>12.853772</td>\n",
              "      <td>8.453722</td>\n",
              "      <td>13.269686</td>\n",
              "      <td>50.303447</td>\n",
              "      <td>35.432447</td>\n",
              "      <td>1.339310</td>\n",
              "      <td>0.152059</td>\n",
              "      <td>0.166967</td>\n",
              "      <td>9.207952</td>\n",
              "      <td>5.750693</td>\n",
              "      <td>76.895363</td>\n",
              "      <td>2.009744</td>\n",
              "      <td>18.154712</td>\n",
              "      <td>45.209752</td>\n",
              "      <td>3.549729</td>\n",
              "      <td>21.337579</td>\n",
              "      <td>2.285853</td>\n",
              "      <td>0.839549</td>\n",
              "      <td>12.968745</td>\n",
              "      <td>8.491336</td>\n",
              "      <td>13.177680</td>\n",
              "      <td>49.993134</td>\n",
              "      <td>34.663854</td>\n",
              "      <td>1.351363</td>\n",
              "      <td>0.049938</td>\n",
              "      <td>0.065544</td>\n",
              "      <td>9.459442</td>\n",
              "      <td>6.034050</td>\n",
              "      <td>77.025654</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>18.278377</td>\n",
              "      <td>44.671891</td>\n",
              "      <td>3.837441</td>\n",
              "      <td>21.338425</td>\n",
              "      <td>2.334654</td>\n",
              "      <td>0.873986</td>\n",
              "      <td>12.696977</td>\n",
              "      <td>8.462444</td>\n",
              "      <td>13.476209</td>\n",
              "      <td>50.212234</td>\n",
              "      <td>35.072577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.673327</td>\n",
              "      <td>0.689367</td>\n",
              "      <td>21.831730</td>\n",
              "      <td>17.385490</td>\n",
              "      <td>86.587475</td>\n",
              "      <td>7.055039</td>\n",
              "      <td>28.755178</td>\n",
              "      <td>51.726987</td>\n",
              "      <td>13.314242</td>\n",
              "      <td>31.427591</td>\n",
              "      <td>6.832898</td>\n",
              "      <td>1.786206</td>\n",
              "      <td>16.463262</td>\n",
              "      <td>11.800174</td>\n",
              "      <td>17.164105</td>\n",
              "      <td>60.070159</td>\n",
              "      <td>40.683578</td>\n",
              "      <td>9.982916</td>\n",
              "      <td>0.667296</td>\n",
              "      <td>0.684726</td>\n",
              "      <td>21.967645</td>\n",
              "      <td>17.602241</td>\n",
              "      <td>86.761239</td>\n",
              "      <td>7.043880</td>\n",
              "      <td>28.713787</td>\n",
              "      <td>51.686774</td>\n",
              "      <td>13.275460</td>\n",
              "      <td>31.324347</td>\n",
              "      <td>6.890855</td>\n",
              "      <td>1.775599</td>\n",
              "      <td>16.502816</td>\n",
              "      <td>11.806194</td>\n",
              "      <td>17.101370</td>\n",
              "      <td>60.267099</td>\n",
              "      <td>40.912076</td>\n",
              "      <td>10.070058</td>\n",
              "      <td>0.677422</td>\n",
              "      <td>0.691911</td>\n",
              "      <td>22.145670</td>\n",
              "      <td>17.687770</td>\n",
              "      <td>86.799638</td>\n",
              "      <td>6.908287</td>\n",
              "      <td>28.730951</td>\n",
              "      <td>51.594301</td>\n",
              "      <td>13.391769</td>\n",
              "      <td>31.330469</td>\n",
              "      <td>6.802860</td>\n",
              "      <td>1.760374</td>\n",
              "      <td>16.406397</td>\n",
              "      <td>11.777101</td>\n",
              "      <td>17.197313</td>\n",
              "      <td>60.358203</td>\n",
              "      <td>40.910546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.925230</td>\n",
              "      <td>0.936610</td>\n",
              "      <td>28.183014</td>\n",
              "      <td>23.035749</td>\n",
              "      <td>90.123133</td>\n",
              "      <td>8.773243</td>\n",
              "      <td>31.826385</td>\n",
              "      <td>55.750887</td>\n",
              "      <td>17.100556</td>\n",
              "      <td>36.692799</td>\n",
              "      <td>9.734692</td>\n",
              "      <td>2.173884</td>\n",
              "      <td>17.750598</td>\n",
              "      <td>12.819747</td>\n",
              "      <td>19.154800</td>\n",
              "      <td>64.744199</td>\n",
              "      <td>44.459526</td>\n",
              "      <td>15.435832</td>\n",
              "      <td>0.919533</td>\n",
              "      <td>0.944818</td>\n",
              "      <td>28.177370</td>\n",
              "      <td>23.094211</td>\n",
              "      <td>90.142711</td>\n",
              "      <td>8.727310</td>\n",
              "      <td>31.647384</td>\n",
              "      <td>55.784308</td>\n",
              "      <td>17.064074</td>\n",
              "      <td>36.405847</td>\n",
              "      <td>9.712576</td>\n",
              "      <td>2.155570</td>\n",
              "      <td>17.783846</td>\n",
              "      <td>12.824065</td>\n",
              "      <td>19.094616</td>\n",
              "      <td>64.967013</td>\n",
              "      <td>44.485451</td>\n",
              "      <td>15.381420</td>\n",
              "      <td>0.931789</td>\n",
              "      <td>0.944038</td>\n",
              "      <td>28.137863</td>\n",
              "      <td>23.116177</td>\n",
              "      <td>90.182055</td>\n",
              "      <td>8.682130</td>\n",
              "      <td>31.525946</td>\n",
              "      <td>55.490325</td>\n",
              "      <td>16.975410</td>\n",
              "      <td>36.213594</td>\n",
              "      <td>9.550393</td>\n",
              "      <td>2.146468</td>\n",
              "      <td>17.719760</td>\n",
              "      <td>12.805424</td>\n",
              "      <td>19.068658</td>\n",
              "      <td>65.148128</td>\n",
              "      <td>44.504010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.251219</td>\n",
              "      <td>1.267463</td>\n",
              "      <td>36.813772</td>\n",
              "      <td>31.141866</td>\n",
              "      <td>93.387952</td>\n",
              "      <td>10.452262</td>\n",
              "      <td>35.184926</td>\n",
              "      <td>59.185350</td>\n",
              "      <td>20.919961</td>\n",
              "      <td>41.265159</td>\n",
              "      <td>13.637503</td>\n",
              "      <td>2.650302</td>\n",
              "      <td>19.455838</td>\n",
              "      <td>14.153693</td>\n",
              "      <td>21.129580</td>\n",
              "      <td>68.826027</td>\n",
              "      <td>47.217995</td>\n",
              "      <td>21.395513</td>\n",
              "      <td>1.253486</td>\n",
              "      <td>1.264377</td>\n",
              "      <td>36.938802</td>\n",
              "      <td>31.125779</td>\n",
              "      <td>93.464698</td>\n",
              "      <td>10.361459</td>\n",
              "      <td>35.106697</td>\n",
              "      <td>59.127759</td>\n",
              "      <td>20.797971</td>\n",
              "      <td>41.113748</td>\n",
              "      <td>13.465480</td>\n",
              "      <td>2.666154</td>\n",
              "      <td>19.466619</td>\n",
              "      <td>14.129555</td>\n",
              "      <td>21.233162</td>\n",
              "      <td>68.900357</td>\n",
              "      <td>47.244684</td>\n",
              "      <td>21.451636</td>\n",
              "      <td>1.250863</td>\n",
              "      <td>1.275584</td>\n",
              "      <td>36.762927</td>\n",
              "      <td>31.015791</td>\n",
              "      <td>93.482444</td>\n",
              "      <td>10.422368</td>\n",
              "      <td>35.072704</td>\n",
              "      <td>59.078475</td>\n",
              "      <td>20.584376</td>\n",
              "      <td>41.071035</td>\n",
              "      <td>13.372731</td>\n",
              "      <td>2.645314</td>\n",
              "      <td>19.423720</td>\n",
              "      <td>14.091551</td>\n",
              "      <td>21.205695</td>\n",
              "      <td>68.994309</td>\n",
              "      <td>47.172065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.488967</td>\n",
              "      <td>2.522263</td>\n",
              "      <td>53.184067</td>\n",
              "      <td>48.142433</td>\n",
              "      <td>97.843221</td>\n",
              "      <td>26.598752</td>\n",
              "      <td>42.887263</td>\n",
              "      <td>63.979007</td>\n",
              "      <td>27.438286</td>\n",
              "      <td>53.513289</td>\n",
              "      <td>22.278912</td>\n",
              "      <td>9.054438</td>\n",
              "      <td>27.926575</td>\n",
              "      <td>19.377685</td>\n",
              "      <td>26.159011</td>\n",
              "      <td>77.227806</td>\n",
              "      <td>56.288410</td>\n",
              "      <td>40.746942</td>\n",
              "      <td>2.509106</td>\n",
              "      <td>2.532059</td>\n",
              "      <td>53.690218</td>\n",
              "      <td>48.916631</td>\n",
              "      <td>97.845669</td>\n",
              "      <td>26.438426</td>\n",
              "      <td>42.639474</td>\n",
              "      <td>63.790457</td>\n",
              "      <td>27.145540</td>\n",
              "      <td>52.621101</td>\n",
              "      <td>22.871782</td>\n",
              "      <td>9.189612</td>\n",
              "      <td>26.832552</td>\n",
              "      <td>18.798445</td>\n",
              "      <td>25.817923</td>\n",
              "      <td>77.476644</td>\n",
              "      <td>56.269653</td>\n",
              "      <td>41.645746</td>\n",
              "      <td>2.491521</td>\n",
              "      <td>2.522978</td>\n",
              "      <td>52.906363</td>\n",
              "      <td>48.000709</td>\n",
              "      <td>97.935455</td>\n",
              "      <td>26.016608</td>\n",
              "      <td>43.105181</td>\n",
              "      <td>63.771097</td>\n",
              "      <td>27.362321</td>\n",
              "      <td>52.045373</td>\n",
              "      <td>23.305630</td>\n",
              "      <td>9.118302</td>\n",
              "      <td>27.003564</td>\n",
              "      <td>18.964157</td>\n",
              "      <td>26.007557</td>\n",
              "      <td>76.871053</td>\n",
              "      <td>56.442135</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              cli         ili  ...  worried_become_ill.2  worried_finances.2\n",
              "count  893.000000  893.000000  ...            893.000000          893.000000\n",
              "mean     0.972457    0.991809  ...             64.619920           44.411505\n",
              "std      0.411997    0.415468  ...              5.685865            4.605268\n",
              "min      0.139558    0.159477  ...             50.212234           35.072577\n",
              "25%      0.673327    0.689367  ...             60.358203           40.910546\n",
              "50%      0.925230    0.936610  ...             65.148128           44.504010\n",
              "75%      1.251219    1.267463  ...             68.994309           47.172065\n",
              "max      2.488967    2.522263  ...             76.871053           56.442135\n",
              "\n",
              "[8 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 428
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjAiqoNR8-O",
        "outputId": "325957d9-d29f-4aa4-b6e1-69313e4ab624"
      },
      "source": [
        "# 判斷feature跟target之間的相關度\n",
        "# 挑出相關度大於0.5的feature\n",
        "\n",
        "corr = train_data.iloc[:, 41:].corr().iloc[-1]\n",
        "features = corr[abs(corr) > 0.5]\n",
        "features_col = features.index.to_list()[:-1]\n",
        "features_id = np.array([train_data.columns.to_list().index(i) for i in features_col]) - 1\n",
        "\n",
        "print (features)\n",
        "print (\"\\nfeatures' id:\", features_id)"
      ],
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cli                  0.838504\n",
            "ili                  0.830527\n",
            "hh_cmnty_cli         0.879724\n",
            "nohh_cmnty_cli       0.869938\n",
            "tested_positive      0.981165\n",
            "cli.1                0.838224\n",
            "ili.1                0.829200\n",
            "hh_cmnty_cli.1       0.879438\n",
            "nohh_cmnty_cli.1     0.869278\n",
            "tested_positive.1    0.991012\n",
            "cli.2                0.835751\n",
            "ili.2                0.826075\n",
            "hh_cmnty_cli.2       0.878218\n",
            "nohh_cmnty_cli.2     0.867535\n",
            "tested_positive.2    1.000000\n",
            "Name: tested_positive.2, dtype: float64\n",
            "\n",
            "features' id: [40 41 42 43 57 58 59 60 61 75 76 77 78 79]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2EC_f4EgkIy"
      },
      "source": [
        "# **Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf6oARhbPMZu"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "my_seed = 0\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(my_seed)\n",
        "torch.manual_seed(my_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(my_seed)"
      ],
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oGLM6nqg2Tv"
      },
      "source": [
        "# **Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq70jrJPPixU"
      },
      "source": [
        "def get_device():\n",
        "    \"\"\" Get device (if GPU is available, use GPU) \"\"\"\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3BDjR7_hB84"
      },
      "source": [
        "# **Dataset**\n",
        "\n",
        "The `COVID19Dataset` below does:\n",
        "* read `.csv` files\n",
        "* extract features\n",
        "* split `covid.train.csv` into train/dev sets\n",
        "* normalize features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4tMEVA2Pvu4"
      },
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    \"\"\" Dataset for loading and preprocessing the COVID19 dataset \"\"\"\n",
        "    def __init__(self, path, mode, valid_rate, mean, std):\n",
        "        with open(path, \"r\") as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "\n",
        "        choose_features = [i for i in range(40)]\n",
        "        # choose_features = []\n",
        "        choose_features.extend([40, 41, 42, 43, 57, 58, 59, 60, 61, 75, 76, 77, 78, 79])\n",
        "\n",
        "        if mode == \"test\":        \n",
        "            self.x = torch.FloatTensor(data[:, choose_features])\n",
        "        else:\n",
        "            train_indices, valid_indices = train_test_split([i for i in range(data.shape[0])], test_size=valid_rate, random_state=0)\n",
        "            if mode == \"train\":\n",
        "                self.x = torch.FloatTensor(data[train_indices, :])\n",
        "                self.y = torch.FloatTensor(data[train_indices, 93])\n",
        "            elif mode == \"valid\":\n",
        "                self.x = torch.FloatTensor(data[valid_indices, :])\n",
        "                self.y = torch.FloatTensor(data[valid_indices, 93])\n",
        "            self.x = self.x[:, choose_features]\n",
        "        \n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "            \n",
        "        self.x[:,40:] = (self.x[:, 40:] - self.mean) / self.std\n",
        "        # self.x[:, :] = (self.x[:, :] - self.mean) / self.std\n",
        "        self.mode = mode\n",
        "        self.features_num = self.x.shape[1]\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode in [\"train\", \"valid\"]:\n",
        "            return self.x[index], self.y[index]\n",
        "        else:\n",
        "            return self.x[index]\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWk2CfvUWN55"
      },
      "source": [
        "# **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1xvk4yhUuAv"
      },
      "source": [
        "def prepare_dataloader(path, mode, valid_rate, batch_size, jobs_num, mean, std):\n",
        "    dataset = COVID19Dataset(path, mode, valid_rate, mean, std)\n",
        "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode==\"train\"), drop_last=False, num_workers=jobs_num, pin_memory=True)\n",
        "    return dataloader"
      ],
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVUMUUiLWd6v"
      },
      "source": [
        "# **Deep Neural Network**\n",
        "\n",
        "`NeuralNet` is an `nn.Module` designed for regression.\n",
        "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
        "This module also included a function `cal_loss` for calculating loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqBf2tkJVpJf"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, features_num):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(features_num, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self.criterion = nn.MSELoss(reduction=\"mean\")\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Given input of size (batch_size x input_dim), compute output of the network \"\"\"\n",
        "        return self.network(x).squeeze(1)\n",
        "\n",
        "\n",
        "    def cal_loss(self, y, y_hat):\n",
        "        \"\"\" Calculate loss \"\"\"\n",
        "        loss = torch.sqrt(self.criterion(y, y_hat))\n",
        "        # l2 = 0\n",
        "        # for i in self.parameters():\n",
        "        #     l2 += torch.sum(torch.pow(i, 2))\n",
        "        # return loss + 0.01 * l2, loss  \n",
        "        l1 = 0\n",
        "        for i in self.parameters():\n",
        "            l1 += torch.sum(abs(i))\n",
        "        return loss + 0.005 * l1, loss  "
      ],
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRuuIS8EWl2A"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr9vwk0MWZ57"
      },
      "source": [
        "def train(train_dataloader, valid_dataloader, model, config, device):\n",
        "    optimizer = getattr(torch.optim, config[\"optimizer\"])(model.parameters(), **config[\"optimizer_hparas\"])\n",
        "\n",
        "    rmse_min = float(\"Inf\")\n",
        "    not_better_cnt = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < config[\"epochs_num\"]:\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x, y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            rmse_l2_loss, rmse_loss = model.cal_loss(y, y_hat)\n",
        "            rmse_l2_loss.backward()\n",
        "            optimizer.step()  \n",
        "            train_loss += rmse_loss.detach().cpu().item() * x.shape[0]\n",
        "        train_loss /= len(train_dataloader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        for x, y in valid_dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():   \n",
        "                y_hat = model(x)\n",
        "                rmse_l2_loss, rmse_loss = model.cal_loss(y, y_hat)\n",
        "            valid_loss += rmse_loss.detach().cpu().item() * x.shape[0]\n",
        "        valid_loss /= len(valid_dataloader.dataset)\n",
        "    \n",
        "        print (\"Epoch: {:4d}, Train Loss: {:.4f}, Valid Loss: {:.4f}\".format(epoch + 1, train_loss, valid_loss))\n",
        "\n",
        "        if valid_loss < rmse_min:\n",
        "            rmse_min = valid_loss\n",
        "            torch.save(model.state_dict(), config[\"save_path\"]) \n",
        "            not_better_cnt = 0\n",
        "        else:\n",
        "            not_better_cnt += 1\n",
        "        \n",
        "        if not_better_cnt > config[\"early_stop\"]:\n",
        "            print (\"Early stop at epoch {:4d}.\".format(epoch + 1))\n",
        "            break\n",
        "        epoch += 1"
      ],
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2QLHWvvWxmk"
      },
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMbpnbQYaT8w"
      },
      "source": [
        "device = get_device()\n",
        "os.system(\"mkdir models\")\n",
        "\n",
        "config = {\n",
        "    \"epochs_num\": 20000,\n",
        "    \"batch_size\": 270,\n",
        "    \"optimizer\": \"SGD\", \n",
        "    \"optimizer_hparas\": {  \n",
        "        \"lr\": 0.0001,\n",
        "        \"momentum\": 0.9\n",
        "    },\n",
        "    \"early_stop\": 200,\n",
        "    \"save_path\": \"models/model.pth\" \n",
        "}"
      ],
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXsMJ-UQW0SJ"
      },
      "source": [
        "# **Load data and model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pD0zAffa9Zh"
      },
      "source": [
        "with open(train_path, \"r\") as fp:\n",
        "    train_data = list(csv.reader(fp))\n",
        "    train_data = np.array(train_data[1:])[:, 1:-1].astype(float)\n",
        "with open(test_path, \"r\") as fp:\n",
        "    test_data = list(csv.reader(fp))\n",
        "    test_data = np.array(test_data[1:])[:, 1:].astype(float)\n",
        "all_data = np.vstack([train_data, test_data])\n",
        "id = [40, 41, 42, 43, 57, 58, 59, 60, 61, 75, 76, 77, 78, 79]\n",
        "mean = torch.FloatTensor(train_data[:, id]).mean(dim=0, keepdim=True)\n",
        "std = torch.FloatTensor(train_data[:, id]).std(dim=0, keepdim=True)\n",
        "\n",
        "train_dataloader = prepare_dataloader(train_path, \"train\", 0.1, config[\"batch_size\"], 0, mean, std)\n",
        "valid_dataloader = prepare_dataloader(train_path, \"valid\", 0.1, config[\"batch_size\"], 0, mean, std)\n",
        "test_dataloader = prepare_dataloader(test_path, \"test\", None, config[\"batch_size\"], 0, mean, std)"
      ],
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdeB92UwcatY"
      },
      "source": [
        "model = NeuralNetwork(train_dataloader.dataset.features_num).to(device) "
      ],
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkyVD_PgW7b7"
      },
      "source": [
        "# **Start Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqDychiFdApF",
        "outputId": "48c87773-8463-419f-a8f6-6570098de6c1"
      },
      "source": [
        "train(train_dataloader, valid_dataloader, model, config, device)"
      ],
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    1, Train Loss: 18.3341, Valid Loss: 17.8830\n",
            "Epoch:    2, Train Loss: 18.3303, Valid Loss: 17.8698\n",
            "Epoch:    3, Train Loss: 18.3136, Valid Loss: 17.8541\n",
            "Epoch:    4, Train Loss: 18.2952, Valid Loss: 17.8375\n",
            "Epoch:    5, Train Loss: 18.2702, Valid Loss: 17.8206\n",
            "Epoch:    6, Train Loss: 18.2565, Valid Loss: 17.8036\n",
            "Epoch:    7, Train Loss: 18.2383, Valid Loss: 17.7867\n",
            "Epoch:    8, Train Loss: 18.2204, Valid Loss: 17.7697\n",
            "Epoch:    9, Train Loss: 18.2007, Valid Loss: 17.7527\n",
            "Epoch:   10, Train Loss: 18.1765, Valid Loss: 17.7358\n",
            "Epoch:   11, Train Loss: 18.1625, Valid Loss: 17.7188\n",
            "Epoch:   12, Train Loss: 18.1428, Valid Loss: 17.7018\n",
            "Epoch:   13, Train Loss: 18.1262, Valid Loss: 17.6848\n",
            "Epoch:   14, Train Loss: 18.1070, Valid Loss: 17.6678\n",
            "Epoch:   15, Train Loss: 18.0925, Valid Loss: 17.6508\n",
            "Epoch:   16, Train Loss: 18.0737, Valid Loss: 17.6337\n",
            "Epoch:   17, Train Loss: 18.0559, Valid Loss: 17.6166\n",
            "Epoch:   18, Train Loss: 18.0316, Valid Loss: 17.5993\n",
            "Epoch:   19, Train Loss: 18.0160, Valid Loss: 17.5820\n",
            "Epoch:   20, Train Loss: 17.9935, Valid Loss: 17.5645\n",
            "Epoch:   21, Train Loss: 17.9784, Valid Loss: 17.5469\n",
            "Epoch:   22, Train Loss: 17.9588, Valid Loss: 17.5292\n",
            "Epoch:   23, Train Loss: 17.9408, Valid Loss: 17.5114\n",
            "Epoch:   24, Train Loss: 17.9210, Valid Loss: 17.4935\n",
            "Epoch:   25, Train Loss: 17.8982, Valid Loss: 17.4754\n",
            "Epoch:   26, Train Loss: 17.8833, Valid Loss: 17.4570\n",
            "Epoch:   27, Train Loss: 17.8636, Valid Loss: 17.4384\n",
            "Epoch:   28, Train Loss: 17.8404, Valid Loss: 17.4197\n",
            "Epoch:   29, Train Loss: 17.8195, Valid Loss: 17.4007\n",
            "Epoch:   30, Train Loss: 17.7972, Valid Loss: 17.3815\n",
            "Epoch:   31, Train Loss: 17.7750, Valid Loss: 17.3619\n",
            "Epoch:   32, Train Loss: 17.7537, Valid Loss: 17.3422\n",
            "Epoch:   33, Train Loss: 17.7360, Valid Loss: 17.3222\n",
            "Epoch:   34, Train Loss: 17.7107, Valid Loss: 17.3019\n",
            "Epoch:   35, Train Loss: 17.6859, Valid Loss: 17.2813\n",
            "Epoch:   36, Train Loss: 17.6684, Valid Loss: 17.2604\n",
            "Epoch:   37, Train Loss: 17.6424, Valid Loss: 17.2390\n",
            "Epoch:   38, Train Loss: 17.6193, Valid Loss: 17.2174\n",
            "Epoch:   39, Train Loss: 17.5923, Valid Loss: 17.1952\n",
            "Epoch:   40, Train Loss: 17.5668, Valid Loss: 17.1729\n",
            "Epoch:   41, Train Loss: 17.5427, Valid Loss: 17.1499\n",
            "Epoch:   42, Train Loss: 17.5204, Valid Loss: 17.1265\n",
            "Epoch:   43, Train Loss: 17.4944, Valid Loss: 17.1029\n",
            "Epoch:   44, Train Loss: 17.4678, Valid Loss: 17.0785\n",
            "Epoch:   45, Train Loss: 17.4389, Valid Loss: 17.0537\n",
            "Epoch:   46, Train Loss: 17.4089, Valid Loss: 17.0285\n",
            "Epoch:   47, Train Loss: 17.3834, Valid Loss: 17.0027\n",
            "Epoch:   48, Train Loss: 17.3494, Valid Loss: 16.9763\n",
            "Epoch:   49, Train Loss: 17.3212, Valid Loss: 16.9494\n",
            "Epoch:   50, Train Loss: 17.2943, Valid Loss: 16.9217\n",
            "Epoch:   51, Train Loss: 17.2637, Valid Loss: 16.8934\n",
            "Epoch:   52, Train Loss: 17.2318, Valid Loss: 16.8646\n",
            "Epoch:   53, Train Loss: 17.1997, Valid Loss: 16.8348\n",
            "Epoch:   54, Train Loss: 17.1635, Valid Loss: 16.8046\n",
            "Epoch:   55, Train Loss: 17.1309, Valid Loss: 16.7733\n",
            "Epoch:   56, Train Loss: 17.0944, Valid Loss: 16.7417\n",
            "Epoch:   57, Train Loss: 17.0570, Valid Loss: 16.7090\n",
            "Epoch:   58, Train Loss: 17.0162, Valid Loss: 16.6756\n",
            "Epoch:   59, Train Loss: 16.9805, Valid Loss: 16.6412\n",
            "Epoch:   60, Train Loss: 16.9363, Valid Loss: 16.6060\n",
            "Epoch:   61, Train Loss: 16.8979, Valid Loss: 16.5699\n",
            "Epoch:   62, Train Loss: 16.8585, Valid Loss: 16.5331\n",
            "Epoch:   63, Train Loss: 16.8174, Valid Loss: 16.4951\n",
            "Epoch:   64, Train Loss: 16.7721, Valid Loss: 16.4562\n",
            "Epoch:   65, Train Loss: 16.7263, Valid Loss: 16.4160\n",
            "Epoch:   66, Train Loss: 16.6804, Valid Loss: 16.3752\n",
            "Epoch:   67, Train Loss: 16.6355, Valid Loss: 16.3329\n",
            "Epoch:   68, Train Loss: 16.5844, Valid Loss: 16.2897\n",
            "Epoch:   69, Train Loss: 16.5357, Valid Loss: 16.2448\n",
            "Epoch:   70, Train Loss: 16.4783, Valid Loss: 16.1992\n",
            "Epoch:   71, Train Loss: 16.4279, Valid Loss: 16.1520\n",
            "Epoch:   72, Train Loss: 16.3752, Valid Loss: 16.1037\n",
            "Epoch:   73, Train Loss: 16.3203, Valid Loss: 16.0541\n",
            "Epoch:   74, Train Loss: 16.2580, Valid Loss: 16.0032\n",
            "Epoch:   75, Train Loss: 16.2023, Valid Loss: 15.9504\n",
            "Epoch:   76, Train Loss: 16.1418, Valid Loss: 15.8969\n",
            "Epoch:   77, Train Loss: 16.0791, Valid Loss: 15.8417\n",
            "Epoch:   78, Train Loss: 16.0156, Valid Loss: 15.7845\n",
            "Epoch:   79, Train Loss: 15.9479, Valid Loss: 15.7261\n",
            "Epoch:   80, Train Loss: 15.8797, Valid Loss: 15.6662\n",
            "Epoch:   81, Train Loss: 15.8100, Valid Loss: 15.6039\n",
            "Epoch:   82, Train Loss: 15.7395, Valid Loss: 15.5406\n",
            "Epoch:   83, Train Loss: 15.6621, Valid Loss: 15.4760\n",
            "Epoch:   84, Train Loss: 15.5882, Valid Loss: 15.4094\n",
            "Epoch:   85, Train Loss: 15.5104, Valid Loss: 15.3411\n",
            "Epoch:   86, Train Loss: 15.4280, Valid Loss: 15.2709\n",
            "Epoch:   87, Train Loss: 15.3474, Valid Loss: 15.1988\n",
            "Epoch:   88, Train Loss: 15.2585, Valid Loss: 15.1244\n",
            "Epoch:   89, Train Loss: 15.1761, Valid Loss: 15.0487\n",
            "Epoch:   90, Train Loss: 15.0883, Valid Loss: 14.9712\n",
            "Epoch:   91, Train Loss: 14.9947, Valid Loss: 14.8913\n",
            "Epoch:   92, Train Loss: 14.9036, Valid Loss: 14.8100\n",
            "Epoch:   93, Train Loss: 14.8036, Valid Loss: 14.7269\n",
            "Epoch:   94, Train Loss: 14.7112, Valid Loss: 14.6417\n",
            "Epoch:   95, Train Loss: 14.6108, Valid Loss: 14.5547\n",
            "Epoch:   96, Train Loss: 14.5107, Valid Loss: 14.4650\n",
            "Epoch:   97, Train Loss: 14.4043, Valid Loss: 14.3744\n",
            "Epoch:   98, Train Loss: 14.2990, Valid Loss: 14.2815\n",
            "Epoch:   99, Train Loss: 14.1908, Valid Loss: 14.1862\n",
            "Epoch:  100, Train Loss: 14.0783, Valid Loss: 14.0894\n",
            "Epoch:  101, Train Loss: 13.9678, Valid Loss: 13.9917\n",
            "Epoch:  102, Train Loss: 13.8534, Valid Loss: 13.8916\n",
            "Epoch:  103, Train Loss: 13.7379, Valid Loss: 13.7899\n",
            "Epoch:  104, Train Loss: 13.6195, Valid Loss: 13.6866\n",
            "Epoch:  105, Train Loss: 13.4985, Valid Loss: 13.5825\n",
            "Epoch:  106, Train Loss: 13.3767, Valid Loss: 13.4762\n",
            "Epoch:  107, Train Loss: 13.2533, Valid Loss: 13.3686\n",
            "Epoch:  108, Train Loss: 13.1300, Valid Loss: 13.2609\n",
            "Epoch:  109, Train Loss: 13.0028, Valid Loss: 13.1518\n",
            "Epoch:  110, Train Loss: 12.8795, Valid Loss: 13.0407\n",
            "Epoch:  111, Train Loss: 12.7512, Valid Loss: 12.9300\n",
            "Epoch:  112, Train Loss: 12.6252, Valid Loss: 12.8182\n",
            "Epoch:  113, Train Loss: 12.4978, Valid Loss: 12.7064\n",
            "Epoch:  114, Train Loss: 12.3691, Valid Loss: 12.5947\n",
            "Epoch:  115, Train Loss: 12.2415, Valid Loss: 12.4834\n",
            "Epoch:  116, Train Loss: 12.1135, Valid Loss: 12.3719\n",
            "Epoch:  117, Train Loss: 11.9907, Valid Loss: 12.2607\n",
            "Epoch:  118, Train Loss: 11.8658, Valid Loss: 12.1501\n",
            "Epoch:  119, Train Loss: 11.7442, Valid Loss: 12.0407\n",
            "Epoch:  120, Train Loss: 11.6191, Valid Loss: 11.9326\n",
            "Epoch:  121, Train Loss: 11.5010, Valid Loss: 11.8257\n",
            "Epoch:  122, Train Loss: 11.3841, Valid Loss: 11.7205\n",
            "Epoch:  123, Train Loss: 11.2681, Valid Loss: 11.6179\n",
            "Epoch:  124, Train Loss: 11.1546, Valid Loss: 11.5160\n",
            "Epoch:  125, Train Loss: 11.0442, Valid Loss: 11.4174\n",
            "Epoch:  126, Train Loss: 10.9419, Valid Loss: 11.3213\n",
            "Epoch:  127, Train Loss: 10.8385, Valid Loss: 11.2279\n",
            "Epoch:  128, Train Loss: 10.7417, Valid Loss: 11.1372\n",
            "Epoch:  129, Train Loss: 10.6458, Valid Loss: 11.0497\n",
            "Epoch:  130, Train Loss: 10.5537, Valid Loss: 10.9648\n",
            "Epoch:  131, Train Loss: 10.4651, Valid Loss: 10.8824\n",
            "Epoch:  132, Train Loss: 10.3849, Valid Loss: 10.8046\n",
            "Epoch:  133, Train Loss: 10.3078, Valid Loss: 10.7283\n",
            "Epoch:  134, Train Loss: 10.2307, Valid Loss: 10.6563\n",
            "Epoch:  135, Train Loss: 10.1602, Valid Loss: 10.5870\n",
            "Epoch:  136, Train Loss: 10.0932, Valid Loss: 10.5212\n",
            "Epoch:  137, Train Loss: 10.0312, Valid Loss: 10.4582\n",
            "Epoch:  138, Train Loss: 9.9693, Valid Loss: 10.3987\n",
            "Epoch:  139, Train Loss: 9.9117, Valid Loss: 10.3412\n",
            "Epoch:  140, Train Loss: 9.8605, Valid Loss: 10.2860\n",
            "Epoch:  141, Train Loss: 9.8100, Valid Loss: 10.2344\n",
            "Epoch:  142, Train Loss: 9.7621, Valid Loss: 10.1846\n",
            "Epoch:  143, Train Loss: 9.7169, Valid Loss: 10.1373\n",
            "Epoch:  144, Train Loss: 9.6755, Valid Loss: 10.0922\n",
            "Epoch:  145, Train Loss: 9.6346, Valid Loss: 10.0489\n",
            "Epoch:  146, Train Loss: 9.5987, Valid Loss: 10.0072\n",
            "Epoch:  147, Train Loss: 9.5547, Valid Loss: 9.9680\n",
            "Epoch:  148, Train Loss: 9.5286, Valid Loss: 9.9304\n",
            "Epoch:  149, Train Loss: 9.4950, Valid Loss: 9.8939\n",
            "Epoch:  150, Train Loss: 9.4647, Valid Loss: 9.8587\n",
            "Epoch:  151, Train Loss: 9.4304, Valid Loss: 9.8252\n",
            "Epoch:  152, Train Loss: 9.3999, Valid Loss: 9.7928\n",
            "Epoch:  153, Train Loss: 9.3752, Valid Loss: 9.7612\n",
            "Epoch:  154, Train Loss: 9.3503, Valid Loss: 9.7301\n",
            "Epoch:  155, Train Loss: 9.3231, Valid Loss: 9.7001\n",
            "Epoch:  156, Train Loss: 9.2968, Valid Loss: 9.6714\n",
            "Epoch:  157, Train Loss: 9.2686, Valid Loss: 9.6424\n",
            "Epoch:  158, Train Loss: 9.2436, Valid Loss: 9.6142\n",
            "Epoch:  159, Train Loss: 9.2216, Valid Loss: 9.5870\n",
            "Epoch:  160, Train Loss: 9.1979, Valid Loss: 9.5602\n",
            "Epoch:  161, Train Loss: 9.1762, Valid Loss: 9.5339\n",
            "Epoch:  162, Train Loss: 9.1512, Valid Loss: 9.5081\n",
            "Epoch:  163, Train Loss: 9.1281, Valid Loss: 9.4831\n",
            "Epoch:  164, Train Loss: 9.1062, Valid Loss: 9.4577\n",
            "Epoch:  165, Train Loss: 9.0825, Valid Loss: 9.4322\n",
            "Epoch:  166, Train Loss: 9.0597, Valid Loss: 9.4074\n",
            "Epoch:  167, Train Loss: 9.0367, Valid Loss: 9.3829\n",
            "Epoch:  168, Train Loss: 9.0169, Valid Loss: 9.3586\n",
            "Epoch:  169, Train Loss: 8.9927, Valid Loss: 9.3343\n",
            "Epoch:  170, Train Loss: 8.9692, Valid Loss: 9.3104\n",
            "Epoch:  171, Train Loss: 8.9460, Valid Loss: 9.2867\n",
            "Epoch:  172, Train Loss: 8.9232, Valid Loss: 9.2632\n",
            "Epoch:  173, Train Loss: 8.8998, Valid Loss: 9.2396\n",
            "Epoch:  174, Train Loss: 8.8799, Valid Loss: 9.2158\n",
            "Epoch:  175, Train Loss: 8.8559, Valid Loss: 9.1920\n",
            "Epoch:  176, Train Loss: 8.8361, Valid Loss: 9.1687\n",
            "Epoch:  177, Train Loss: 8.8132, Valid Loss: 9.1453\n",
            "Epoch:  178, Train Loss: 8.7943, Valid Loss: 9.1220\n",
            "Epoch:  179, Train Loss: 8.7712, Valid Loss: 9.0994\n",
            "Epoch:  180, Train Loss: 8.7470, Valid Loss: 9.0762\n",
            "Epoch:  181, Train Loss: 8.7251, Valid Loss: 9.0534\n",
            "Epoch:  182, Train Loss: 8.7011, Valid Loss: 9.0305\n",
            "Epoch:  183, Train Loss: 8.6815, Valid Loss: 9.0075\n",
            "Epoch:  184, Train Loss: 8.6591, Valid Loss: 8.9847\n",
            "Epoch:  185, Train Loss: 8.6383, Valid Loss: 8.9619\n",
            "Epoch:  186, Train Loss: 8.6143, Valid Loss: 8.9388\n",
            "Epoch:  187, Train Loss: 8.5927, Valid Loss: 8.9165\n",
            "Epoch:  188, Train Loss: 8.5708, Valid Loss: 8.8933\n",
            "Epoch:  189, Train Loss: 8.5475, Valid Loss: 8.8704\n",
            "Epoch:  190, Train Loss: 8.5270, Valid Loss: 8.8478\n",
            "Epoch:  191, Train Loss: 8.5049, Valid Loss: 8.8252\n",
            "Epoch:  192, Train Loss: 8.4827, Valid Loss: 8.8030\n",
            "Epoch:  193, Train Loss: 8.4598, Valid Loss: 8.7807\n",
            "Epoch:  194, Train Loss: 8.4379, Valid Loss: 8.7578\n",
            "Epoch:  195, Train Loss: 8.4165, Valid Loss: 8.7353\n",
            "Epoch:  196, Train Loss: 8.3920, Valid Loss: 8.7128\n",
            "Epoch:  197, Train Loss: 8.3724, Valid Loss: 8.6908\n",
            "Epoch:  198, Train Loss: 8.3513, Valid Loss: 8.6681\n",
            "Epoch:  199, Train Loss: 8.3282, Valid Loss: 8.6452\n",
            "Epoch:  200, Train Loss: 8.3047, Valid Loss: 8.6236\n",
            "Epoch:  201, Train Loss: 8.2850, Valid Loss: 8.6015\n",
            "Epoch:  202, Train Loss: 8.2639, Valid Loss: 8.5791\n",
            "Epoch:  203, Train Loss: 8.2355, Valid Loss: 8.5566\n",
            "Epoch:  204, Train Loss: 8.2192, Valid Loss: 8.5345\n",
            "Epoch:  205, Train Loss: 8.1982, Valid Loss: 8.5122\n",
            "Epoch:  206, Train Loss: 8.1762, Valid Loss: 8.4900\n",
            "Epoch:  207, Train Loss: 8.1519, Valid Loss: 8.4683\n",
            "Epoch:  208, Train Loss: 8.1343, Valid Loss: 8.4461\n",
            "Epoch:  209, Train Loss: 8.1130, Valid Loss: 8.4243\n",
            "Epoch:  210, Train Loss: 8.0879, Valid Loss: 8.4025\n",
            "Epoch:  211, Train Loss: 8.0679, Valid Loss: 8.3807\n",
            "Epoch:  212, Train Loss: 8.0455, Valid Loss: 8.3589\n",
            "Epoch:  213, Train Loss: 8.0277, Valid Loss: 8.3370\n",
            "Epoch:  214, Train Loss: 8.0081, Valid Loss: 8.3152\n",
            "Epoch:  215, Train Loss: 7.9846, Valid Loss: 8.2936\n",
            "Epoch:  216, Train Loss: 7.9651, Valid Loss: 8.2720\n",
            "Epoch:  217, Train Loss: 7.9461, Valid Loss: 8.2507\n",
            "Epoch:  218, Train Loss: 7.9214, Valid Loss: 8.2289\n",
            "Epoch:  219, Train Loss: 7.9033, Valid Loss: 8.2072\n",
            "Epoch:  220, Train Loss: 7.8801, Valid Loss: 8.1859\n",
            "Epoch:  221, Train Loss: 7.8578, Valid Loss: 8.1641\n",
            "Epoch:  222, Train Loss: 7.8420, Valid Loss: 8.1431\n",
            "Epoch:  223, Train Loss: 7.8173, Valid Loss: 8.1221\n",
            "Epoch:  224, Train Loss: 7.7982, Valid Loss: 8.1008\n",
            "Epoch:  225, Train Loss: 7.7783, Valid Loss: 8.0791\n",
            "Epoch:  226, Train Loss: 7.7605, Valid Loss: 8.0575\n",
            "Epoch:  227, Train Loss: 7.7382, Valid Loss: 8.0364\n",
            "Epoch:  228, Train Loss: 7.7178, Valid Loss: 8.0155\n",
            "Epoch:  229, Train Loss: 7.6970, Valid Loss: 7.9946\n",
            "Epoch:  230, Train Loss: 7.6803, Valid Loss: 7.9734\n",
            "Epoch:  231, Train Loss: 7.6547, Valid Loss: 7.9528\n",
            "Epoch:  232, Train Loss: 7.6383, Valid Loss: 7.9321\n",
            "Epoch:  233, Train Loss: 7.6210, Valid Loss: 7.9113\n",
            "Epoch:  234, Train Loss: 7.6010, Valid Loss: 7.8905\n",
            "Epoch:  235, Train Loss: 7.5817, Valid Loss: 7.8695\n",
            "Epoch:  236, Train Loss: 7.5581, Valid Loss: 7.8489\n",
            "Epoch:  237, Train Loss: 7.5400, Valid Loss: 7.8280\n",
            "Epoch:  238, Train Loss: 7.5192, Valid Loss: 7.8072\n",
            "Epoch:  239, Train Loss: 7.5025, Valid Loss: 7.7866\n",
            "Epoch:  240, Train Loss: 7.4803, Valid Loss: 7.7660\n",
            "Epoch:  241, Train Loss: 7.4621, Valid Loss: 7.7454\n",
            "Epoch:  242, Train Loss: 7.4424, Valid Loss: 7.7251\n",
            "Epoch:  243, Train Loss: 7.4158, Valid Loss: 7.7043\n",
            "Epoch:  244, Train Loss: 7.4041, Valid Loss: 7.6838\n",
            "Epoch:  245, Train Loss: 7.3847, Valid Loss: 7.6630\n",
            "Epoch:  246, Train Loss: 7.3630, Valid Loss: 7.6423\n",
            "Epoch:  247, Train Loss: 7.3432, Valid Loss: 7.6217\n",
            "Epoch:  248, Train Loss: 7.3243, Valid Loss: 7.6017\n",
            "Epoch:  249, Train Loss: 7.3068, Valid Loss: 7.5811\n",
            "Epoch:  250, Train Loss: 7.2854, Valid Loss: 7.5600\n",
            "Epoch:  251, Train Loss: 7.2682, Valid Loss: 7.5393\n",
            "Epoch:  252, Train Loss: 7.2488, Valid Loss: 7.5189\n",
            "Epoch:  253, Train Loss: 7.2278, Valid Loss: 7.4981\n",
            "Epoch:  254, Train Loss: 7.2061, Valid Loss: 7.4777\n",
            "Epoch:  255, Train Loss: 7.1906, Valid Loss: 7.4570\n",
            "Epoch:  256, Train Loss: 7.1712, Valid Loss: 7.4364\n",
            "Epoch:  257, Train Loss: 7.1508, Valid Loss: 7.4154\n",
            "Epoch:  258, Train Loss: 7.1324, Valid Loss: 7.3951\n",
            "Epoch:  259, Train Loss: 7.1122, Valid Loss: 7.3748\n",
            "Epoch:  260, Train Loss: 7.0931, Valid Loss: 7.3544\n",
            "Epoch:  261, Train Loss: 7.0734, Valid Loss: 7.3331\n",
            "Epoch:  262, Train Loss: 7.0546, Valid Loss: 7.3121\n",
            "Epoch:  263, Train Loss: 7.0354, Valid Loss: 7.2912\n",
            "Epoch:  264, Train Loss: 7.0122, Valid Loss: 7.2704\n",
            "Epoch:  265, Train Loss: 6.9947, Valid Loss: 7.2497\n",
            "Epoch:  266, Train Loss: 6.9744, Valid Loss: 7.2287\n",
            "Epoch:  267, Train Loss: 6.9552, Valid Loss: 7.2075\n",
            "Epoch:  268, Train Loss: 6.9334, Valid Loss: 7.1864\n",
            "Epoch:  269, Train Loss: 6.9150, Valid Loss: 7.1651\n",
            "Epoch:  270, Train Loss: 6.8963, Valid Loss: 7.1442\n",
            "Epoch:  271, Train Loss: 6.8740, Valid Loss: 7.1224\n",
            "Epoch:  272, Train Loss: 6.8566, Valid Loss: 7.1013\n",
            "Epoch:  273, Train Loss: 6.8353, Valid Loss: 7.0798\n",
            "Epoch:  274, Train Loss: 6.8151, Valid Loss: 7.0580\n",
            "Epoch:  275, Train Loss: 6.7926, Valid Loss: 7.0361\n",
            "Epoch:  276, Train Loss: 6.7686, Valid Loss: 7.0141\n",
            "Epoch:  277, Train Loss: 6.7475, Valid Loss: 6.9922\n",
            "Epoch:  278, Train Loss: 6.7308, Valid Loss: 6.9701\n",
            "Epoch:  279, Train Loss: 6.7122, Valid Loss: 6.9484\n",
            "Epoch:  280, Train Loss: 6.6916, Valid Loss: 6.9260\n",
            "Epoch:  281, Train Loss: 6.6694, Valid Loss: 6.9037\n",
            "Epoch:  282, Train Loss: 6.6487, Valid Loss: 6.8810\n",
            "Epoch:  283, Train Loss: 6.6294, Valid Loss: 6.8586\n",
            "Epoch:  284, Train Loss: 6.6081, Valid Loss: 6.8359\n",
            "Epoch:  285, Train Loss: 6.5857, Valid Loss: 6.8133\n",
            "Epoch:  286, Train Loss: 6.5668, Valid Loss: 6.7903\n",
            "Epoch:  287, Train Loss: 6.5419, Valid Loss: 6.7670\n",
            "Epoch:  288, Train Loss: 6.5215, Valid Loss: 6.7439\n",
            "Epoch:  289, Train Loss: 6.5015, Valid Loss: 6.7202\n",
            "Epoch:  290, Train Loss: 6.4761, Valid Loss: 6.6968\n",
            "Epoch:  291, Train Loss: 6.4547, Valid Loss: 6.6729\n",
            "Epoch:  292, Train Loss: 6.4336, Valid Loss: 6.6483\n",
            "Epoch:  293, Train Loss: 6.4115, Valid Loss: 6.6238\n",
            "Epoch:  294, Train Loss: 6.3901, Valid Loss: 6.5996\n",
            "Epoch:  295, Train Loss: 6.3658, Valid Loss: 6.5754\n",
            "Epoch:  296, Train Loss: 6.3417, Valid Loss: 6.5500\n",
            "Epoch:  297, Train Loss: 6.3182, Valid Loss: 6.5257\n",
            "Epoch:  298, Train Loss: 6.2965, Valid Loss: 6.5003\n",
            "Epoch:  299, Train Loss: 6.2728, Valid Loss: 6.4751\n",
            "Epoch:  300, Train Loss: 6.2483, Valid Loss: 6.4497\n",
            "Epoch:  301, Train Loss: 6.2263, Valid Loss: 6.4240\n",
            "Epoch:  302, Train Loss: 6.2017, Valid Loss: 6.3981\n",
            "Epoch:  303, Train Loss: 6.1788, Valid Loss: 6.3724\n",
            "Epoch:  304, Train Loss: 6.1541, Valid Loss: 6.3461\n",
            "Epoch:  305, Train Loss: 6.1288, Valid Loss: 6.3197\n",
            "Epoch:  306, Train Loss: 6.1041, Valid Loss: 6.2933\n",
            "Epoch:  307, Train Loss: 6.0813, Valid Loss: 6.2666\n",
            "Epoch:  308, Train Loss: 6.0546, Valid Loss: 6.2397\n",
            "Epoch:  309, Train Loss: 6.0291, Valid Loss: 6.2124\n",
            "Epoch:  310, Train Loss: 6.0035, Valid Loss: 6.1851\n",
            "Epoch:  311, Train Loss: 5.9790, Valid Loss: 6.1580\n",
            "Epoch:  312, Train Loss: 5.9531, Valid Loss: 6.1303\n",
            "Epoch:  313, Train Loss: 5.9282, Valid Loss: 6.1024\n",
            "Epoch:  314, Train Loss: 5.9022, Valid Loss: 6.0744\n",
            "Epoch:  315, Train Loss: 5.8750, Valid Loss: 6.0460\n",
            "Epoch:  316, Train Loss: 5.8469, Valid Loss: 6.0175\n",
            "Epoch:  317, Train Loss: 5.8221, Valid Loss: 5.9889\n",
            "Epoch:  318, Train Loss: 5.7956, Valid Loss: 5.9600\n",
            "Epoch:  319, Train Loss: 5.7684, Valid Loss: 5.9309\n",
            "Epoch:  320, Train Loss: 5.7419, Valid Loss: 5.9021\n",
            "Epoch:  321, Train Loss: 5.7124, Valid Loss: 5.8730\n",
            "Epoch:  322, Train Loss: 5.6882, Valid Loss: 5.8434\n",
            "Epoch:  323, Train Loss: 5.6580, Valid Loss: 5.8143\n",
            "Epoch:  324, Train Loss: 5.6335, Valid Loss: 5.7849\n",
            "Epoch:  325, Train Loss: 5.6062, Valid Loss: 5.7560\n",
            "Epoch:  326, Train Loss: 5.5788, Valid Loss: 5.7267\n",
            "Epoch:  327, Train Loss: 5.5503, Valid Loss: 5.6969\n",
            "Epoch:  328, Train Loss: 5.5246, Valid Loss: 5.6670\n",
            "Epoch:  329, Train Loss: 5.4949, Valid Loss: 5.6370\n",
            "Epoch:  330, Train Loss: 5.4676, Valid Loss: 5.6073\n",
            "Epoch:  331, Train Loss: 5.4401, Valid Loss: 5.5769\n",
            "Epoch:  332, Train Loss: 5.4115, Valid Loss: 5.5465\n",
            "Epoch:  333, Train Loss: 5.3806, Valid Loss: 5.5156\n",
            "Epoch:  334, Train Loss: 5.3527, Valid Loss: 5.4845\n",
            "Epoch:  335, Train Loss: 5.3236, Valid Loss: 5.4533\n",
            "Epoch:  336, Train Loss: 5.2929, Valid Loss: 5.4214\n",
            "Epoch:  337, Train Loss: 5.2638, Valid Loss: 5.3896\n",
            "Epoch:  338, Train Loss: 5.2317, Valid Loss: 5.3576\n",
            "Epoch:  339, Train Loss: 5.2031, Valid Loss: 5.3252\n",
            "Epoch:  340, Train Loss: 5.1710, Valid Loss: 5.2926\n",
            "Epoch:  341, Train Loss: 5.1401, Valid Loss: 5.2591\n",
            "Epoch:  342, Train Loss: 5.1082, Valid Loss: 5.2262\n",
            "Epoch:  343, Train Loss: 5.0764, Valid Loss: 5.1923\n",
            "Epoch:  344, Train Loss: 5.0444, Valid Loss: 5.1585\n",
            "Epoch:  345, Train Loss: 5.0130, Valid Loss: 5.1243\n",
            "Epoch:  346, Train Loss: 4.9805, Valid Loss: 5.0898\n",
            "Epoch:  347, Train Loss: 4.9469, Valid Loss: 5.0548\n",
            "Epoch:  348, Train Loss: 4.9128, Valid Loss: 5.0193\n",
            "Epoch:  349, Train Loss: 4.8792, Valid Loss: 4.9838\n",
            "Epoch:  350, Train Loss: 4.8449, Valid Loss: 4.9476\n",
            "Epoch:  351, Train Loss: 4.8090, Valid Loss: 4.9110\n",
            "Epoch:  352, Train Loss: 4.7744, Valid Loss: 4.8740\n",
            "Epoch:  353, Train Loss: 4.7391, Valid Loss: 4.8365\n",
            "Epoch:  354, Train Loss: 4.7026, Valid Loss: 4.7986\n",
            "Epoch:  355, Train Loss: 4.6628, Valid Loss: 4.7603\n",
            "Epoch:  356, Train Loss: 4.6285, Valid Loss: 4.7210\n",
            "Epoch:  357, Train Loss: 4.5904, Valid Loss: 4.6811\n",
            "Epoch:  358, Train Loss: 4.5517, Valid Loss: 4.6408\n",
            "Epoch:  359, Train Loss: 4.5132, Valid Loss: 4.5997\n",
            "Epoch:  360, Train Loss: 4.4706, Valid Loss: 4.5584\n",
            "Epoch:  361, Train Loss: 4.4331, Valid Loss: 4.5163\n",
            "Epoch:  362, Train Loss: 4.3904, Valid Loss: 4.4737\n",
            "Epoch:  363, Train Loss: 4.3503, Valid Loss: 4.4307\n",
            "Epoch:  364, Train Loss: 4.3063, Valid Loss: 4.3874\n",
            "Epoch:  365, Train Loss: 4.2648, Valid Loss: 4.3430\n",
            "Epoch:  366, Train Loss: 4.2227, Valid Loss: 4.2987\n",
            "Epoch:  367, Train Loss: 4.1790, Valid Loss: 4.2533\n",
            "Epoch:  368, Train Loss: 4.1337, Valid Loss: 4.2079\n",
            "Epoch:  369, Train Loss: 4.0909, Valid Loss: 4.1616\n",
            "Epoch:  370, Train Loss: 4.0451, Valid Loss: 4.1145\n",
            "Epoch:  371, Train Loss: 4.0012, Valid Loss: 4.0673\n",
            "Epoch:  372, Train Loss: 3.9538, Valid Loss: 4.0193\n",
            "Epoch:  373, Train Loss: 3.9074, Valid Loss: 3.9710\n",
            "Epoch:  374, Train Loss: 3.8599, Valid Loss: 3.9221\n",
            "Epoch:  375, Train Loss: 3.8139, Valid Loss: 3.8727\n",
            "Epoch:  376, Train Loss: 3.7655, Valid Loss: 3.8229\n",
            "Epoch:  377, Train Loss: 3.7173, Valid Loss: 3.7722\n",
            "Epoch:  378, Train Loss: 3.6691, Valid Loss: 3.7212\n",
            "Epoch:  379, Train Loss: 3.6224, Valid Loss: 3.6699\n",
            "Epoch:  380, Train Loss: 3.5729, Valid Loss: 3.6184\n",
            "Epoch:  381, Train Loss: 3.5236, Valid Loss: 3.5667\n",
            "Epoch:  382, Train Loss: 3.4751, Valid Loss: 3.5149\n",
            "Epoch:  383, Train Loss: 3.4249, Valid Loss: 3.4627\n",
            "Epoch:  384, Train Loss: 3.3754, Valid Loss: 3.4100\n",
            "Epoch:  385, Train Loss: 3.3268, Valid Loss: 3.3573\n",
            "Epoch:  386, Train Loss: 3.2755, Valid Loss: 3.3044\n",
            "Epoch:  387, Train Loss: 3.2273, Valid Loss: 3.2514\n",
            "Epoch:  388, Train Loss: 3.1769, Valid Loss: 3.1984\n",
            "Epoch:  389, Train Loss: 3.1282, Valid Loss: 3.1454\n",
            "Epoch:  390, Train Loss: 3.0795, Valid Loss: 3.0927\n",
            "Epoch:  391, Train Loss: 3.0292, Valid Loss: 3.0401\n",
            "Epoch:  392, Train Loss: 2.9799, Valid Loss: 2.9874\n",
            "Epoch:  393, Train Loss: 2.9305, Valid Loss: 2.9351\n",
            "Epoch:  394, Train Loss: 2.8811, Valid Loss: 2.8829\n",
            "Epoch:  395, Train Loss: 2.8319, Valid Loss: 2.8311\n",
            "Epoch:  396, Train Loss: 2.7840, Valid Loss: 2.7800\n",
            "Epoch:  397, Train Loss: 2.7360, Valid Loss: 2.7291\n",
            "Epoch:  398, Train Loss: 2.6914, Valid Loss: 2.6787\n",
            "Epoch:  399, Train Loss: 2.6442, Valid Loss: 2.6288\n",
            "Epoch:  400, Train Loss: 2.5986, Valid Loss: 2.5795\n",
            "Epoch:  401, Train Loss: 2.5522, Valid Loss: 2.5315\n",
            "Epoch:  402, Train Loss: 2.5079, Valid Loss: 2.4838\n",
            "Epoch:  403, Train Loss: 2.4621, Valid Loss: 2.4367\n",
            "Epoch:  404, Train Loss: 2.4196, Valid Loss: 2.3905\n",
            "Epoch:  405, Train Loss: 2.3761, Valid Loss: 2.3452\n",
            "Epoch:  406, Train Loss: 2.3346, Valid Loss: 2.3004\n",
            "Epoch:  407, Train Loss: 2.2930, Valid Loss: 2.2567\n",
            "Epoch:  408, Train Loss: 2.2531, Valid Loss: 2.2141\n",
            "Epoch:  409, Train Loss: 2.2134, Valid Loss: 2.1722\n",
            "Epoch:  410, Train Loss: 2.1741, Valid Loss: 2.1312\n",
            "Epoch:  411, Train Loss: 2.1374, Valid Loss: 2.0914\n",
            "Epoch:  412, Train Loss: 2.1005, Valid Loss: 2.0528\n",
            "Epoch:  413, Train Loss: 2.0632, Valid Loss: 2.0157\n",
            "Epoch:  414, Train Loss: 2.0280, Valid Loss: 1.9786\n",
            "Epoch:  415, Train Loss: 1.9908, Valid Loss: 1.9429\n",
            "Epoch:  416, Train Loss: 1.9576, Valid Loss: 1.9083\n",
            "Epoch:  417, Train Loss: 1.9272, Valid Loss: 1.8746\n",
            "Epoch:  418, Train Loss: 1.8941, Valid Loss: 1.8426\n",
            "Epoch:  419, Train Loss: 1.8633, Valid Loss: 1.8116\n",
            "Epoch:  420, Train Loss: 1.8328, Valid Loss: 1.7813\n",
            "Epoch:  421, Train Loss: 1.8051, Valid Loss: 1.7523\n",
            "Epoch:  422, Train Loss: 1.7786, Valid Loss: 1.7247\n",
            "Epoch:  423, Train Loss: 1.7524, Valid Loss: 1.6985\n",
            "Epoch:  424, Train Loss: 1.7265, Valid Loss: 1.6730\n",
            "Epoch:  425, Train Loss: 1.7024, Valid Loss: 1.6491\n",
            "Epoch:  426, Train Loss: 1.6780, Valid Loss: 1.6261\n",
            "Epoch:  427, Train Loss: 1.6551, Valid Loss: 1.6046\n",
            "Epoch:  428, Train Loss: 1.6351, Valid Loss: 1.5831\n",
            "Epoch:  429, Train Loss: 1.6139, Valid Loss: 1.5631\n",
            "Epoch:  430, Train Loss: 1.5956, Valid Loss: 1.5440\n",
            "Epoch:  431, Train Loss: 1.5759, Valid Loss: 1.5262\n",
            "Epoch:  432, Train Loss: 1.5580, Valid Loss: 1.5087\n",
            "Epoch:  433, Train Loss: 1.5398, Valid Loss: 1.4926\n",
            "Epoch:  434, Train Loss: 1.5231, Valid Loss: 1.4755\n",
            "Epoch:  435, Train Loss: 1.5061, Valid Loss: 1.4599\n",
            "Epoch:  436, Train Loss: 1.4906, Valid Loss: 1.4459\n",
            "Epoch:  437, Train Loss: 1.4743, Valid Loss: 1.4313\n",
            "Epoch:  438, Train Loss: 1.4595, Valid Loss: 1.4177\n",
            "Epoch:  439, Train Loss: 1.4431, Valid Loss: 1.4048\n",
            "Epoch:  440, Train Loss: 1.4298, Valid Loss: 1.3919\n",
            "Epoch:  441, Train Loss: 1.4163, Valid Loss: 1.3795\n",
            "Epoch:  442, Train Loss: 1.4028, Valid Loss: 1.3681\n",
            "Epoch:  443, Train Loss: 1.3887, Valid Loss: 1.3572\n",
            "Epoch:  444, Train Loss: 1.3765, Valid Loss: 1.3462\n",
            "Epoch:  445, Train Loss: 1.3662, Valid Loss: 1.3363\n",
            "Epoch:  446, Train Loss: 1.3544, Valid Loss: 1.3260\n",
            "Epoch:  447, Train Loss: 1.3420, Valid Loss: 1.3164\n",
            "Epoch:  448, Train Loss: 1.3326, Valid Loss: 1.3074\n",
            "Epoch:  449, Train Loss: 1.3230, Valid Loss: 1.2985\n",
            "Epoch:  450, Train Loss: 1.3134, Valid Loss: 1.2893\n",
            "Epoch:  451, Train Loss: 1.3024, Valid Loss: 1.2813\n",
            "Epoch:  452, Train Loss: 1.2930, Valid Loss: 1.2728\n",
            "Epoch:  453, Train Loss: 1.2859, Valid Loss: 1.2660\n",
            "Epoch:  454, Train Loss: 1.2777, Valid Loss: 1.2598\n",
            "Epoch:  455, Train Loss: 1.2696, Valid Loss: 1.2538\n",
            "Epoch:  456, Train Loss: 1.2621, Valid Loss: 1.2449\n",
            "Epoch:  457, Train Loss: 1.2534, Valid Loss: 1.2382\n",
            "Epoch:  458, Train Loss: 1.2469, Valid Loss: 1.2326\n",
            "Epoch:  459, Train Loss: 1.2412, Valid Loss: 1.2283\n",
            "Epoch:  460, Train Loss: 1.2338, Valid Loss: 1.2219\n",
            "Epoch:  461, Train Loss: 1.2282, Valid Loss: 1.2182\n",
            "Epoch:  462, Train Loss: 1.2220, Valid Loss: 1.2122\n",
            "Epoch:  463, Train Loss: 1.2165, Valid Loss: 1.2081\n",
            "Epoch:  464, Train Loss: 1.2104, Valid Loss: 1.2042\n",
            "Epoch:  465, Train Loss: 1.2043, Valid Loss: 1.1999\n",
            "Epoch:  466, Train Loss: 1.2013, Valid Loss: 1.1969\n",
            "Epoch:  467, Train Loss: 1.1957, Valid Loss: 1.1939\n",
            "Epoch:  468, Train Loss: 1.1922, Valid Loss: 1.1892\n",
            "Epoch:  469, Train Loss: 1.1871, Valid Loss: 1.1861\n",
            "Epoch:  470, Train Loss: 1.1840, Valid Loss: 1.1829\n",
            "Epoch:  471, Train Loss: 1.1799, Valid Loss: 1.1812\n",
            "Epoch:  472, Train Loss: 1.1763, Valid Loss: 1.1780\n",
            "Epoch:  473, Train Loss: 1.1721, Valid Loss: 1.1746\n",
            "Epoch:  474, Train Loss: 1.1703, Valid Loss: 1.1720\n",
            "Epoch:  475, Train Loss: 1.1674, Valid Loss: 1.1726\n",
            "Epoch:  476, Train Loss: 1.1631, Valid Loss: 1.1685\n",
            "Epoch:  477, Train Loss: 1.1616, Valid Loss: 1.1654\n",
            "Epoch:  478, Train Loss: 1.1582, Valid Loss: 1.1646\n",
            "Epoch:  479, Train Loss: 1.1553, Valid Loss: 1.1636\n",
            "Epoch:  480, Train Loss: 1.1535, Valid Loss: 1.1608\n",
            "Epoch:  481, Train Loss: 1.1509, Valid Loss: 1.1600\n",
            "Epoch:  482, Train Loss: 1.1491, Valid Loss: 1.1569\n",
            "Epoch:  483, Train Loss: 1.1460, Valid Loss: 1.1548\n",
            "Epoch:  484, Train Loss: 1.1451, Valid Loss: 1.1555\n",
            "Epoch:  485, Train Loss: 1.1435, Valid Loss: 1.1534\n",
            "Epoch:  486, Train Loss: 1.1401, Valid Loss: 1.1522\n",
            "Epoch:  487, Train Loss: 1.1372, Valid Loss: 1.1502\n",
            "Epoch:  488, Train Loss: 1.1355, Valid Loss: 1.1478\n",
            "Epoch:  489, Train Loss: 1.1338, Valid Loss: 1.1480\n",
            "Epoch:  490, Train Loss: 1.1334, Valid Loss: 1.1466\n",
            "Epoch:  491, Train Loss: 1.1327, Valid Loss: 1.1445\n",
            "Epoch:  492, Train Loss: 1.1300, Valid Loss: 1.1440\n",
            "Epoch:  493, Train Loss: 1.1288, Valid Loss: 1.1441\n",
            "Epoch:  494, Train Loss: 1.1276, Valid Loss: 1.1405\n",
            "Epoch:  495, Train Loss: 1.1253, Valid Loss: 1.1402\n",
            "Epoch:  496, Train Loss: 1.1245, Valid Loss: 1.1397\n",
            "Epoch:  497, Train Loss: 1.1211, Valid Loss: 1.1388\n",
            "Epoch:  498, Train Loss: 1.1216, Valid Loss: 1.1366\n",
            "Epoch:  499, Train Loss: 1.1197, Valid Loss: 1.1370\n",
            "Epoch:  500, Train Loss: 1.1193, Valid Loss: 1.1358\n",
            "Epoch:  501, Train Loss: 1.1157, Valid Loss: 1.1336\n",
            "Epoch:  502, Train Loss: 1.1143, Valid Loss: 1.1337\n",
            "Epoch:  503, Train Loss: 1.1156, Valid Loss: 1.1327\n",
            "Epoch:  504, Train Loss: 1.1142, Valid Loss: 1.1323\n",
            "Epoch:  505, Train Loss: 1.1129, Valid Loss: 1.1308\n",
            "Epoch:  506, Train Loss: 1.1112, Valid Loss: 1.1295\n",
            "Epoch:  507, Train Loss: 1.1107, Valid Loss: 1.1286\n",
            "Epoch:  508, Train Loss: 1.1090, Valid Loss: 1.1278\n",
            "Epoch:  509, Train Loss: 1.1086, Valid Loss: 1.1267\n",
            "Epoch:  510, Train Loss: 1.1064, Valid Loss: 1.1264\n",
            "Epoch:  511, Train Loss: 1.1058, Valid Loss: 1.1266\n",
            "Epoch:  512, Train Loss: 1.1057, Valid Loss: 1.1242\n",
            "Epoch:  513, Train Loss: 1.1033, Valid Loss: 1.1241\n",
            "Epoch:  514, Train Loss: 1.1032, Valid Loss: 1.1233\n",
            "Epoch:  515, Train Loss: 1.1014, Valid Loss: 1.1232\n",
            "Epoch:  516, Train Loss: 1.1017, Valid Loss: 1.1206\n",
            "Epoch:  517, Train Loss: 1.1009, Valid Loss: 1.1206\n",
            "Epoch:  518, Train Loss: 1.0996, Valid Loss: 1.1210\n",
            "Epoch:  519, Train Loss: 1.0993, Valid Loss: 1.1214\n",
            "Epoch:  520, Train Loss: 1.0959, Valid Loss: 1.1188\n",
            "Epoch:  521, Train Loss: 1.0967, Valid Loss: 1.1173\n",
            "Epoch:  522, Train Loss: 1.0962, Valid Loss: 1.1167\n",
            "Epoch:  523, Train Loss: 1.0949, Valid Loss: 1.1171\n",
            "Epoch:  524, Train Loss: 1.0940, Valid Loss: 1.1164\n",
            "Epoch:  525, Train Loss: 1.0907, Valid Loss: 1.1152\n",
            "Epoch:  526, Train Loss: 1.0931, Valid Loss: 1.1153\n",
            "Epoch:  527, Train Loss: 1.0922, Valid Loss: 1.1143\n",
            "Epoch:  528, Train Loss: 1.0916, Valid Loss: 1.1149\n",
            "Epoch:  529, Train Loss: 1.0903, Valid Loss: 1.1132\n",
            "Epoch:  530, Train Loss: 1.0891, Valid Loss: 1.1126\n",
            "Epoch:  531, Train Loss: 1.0889, Valid Loss: 1.1117\n",
            "Epoch:  532, Train Loss: 1.0886, Valid Loss: 1.1118\n",
            "Epoch:  533, Train Loss: 1.0870, Valid Loss: 1.1117\n",
            "Epoch:  534, Train Loss: 1.0870, Valid Loss: 1.1110\n",
            "Epoch:  535, Train Loss: 1.0869, Valid Loss: 1.1098\n",
            "Epoch:  536, Train Loss: 1.0847, Valid Loss: 1.1103\n",
            "Epoch:  537, Train Loss: 1.0848, Valid Loss: 1.1095\n",
            "Epoch:  538, Train Loss: 1.0843, Valid Loss: 1.1077\n",
            "Epoch:  539, Train Loss: 1.0822, Valid Loss: 1.1076\n",
            "Epoch:  540, Train Loss: 1.0820, Valid Loss: 1.1084\n",
            "Epoch:  541, Train Loss: 1.0822, Valid Loss: 1.1080\n",
            "Epoch:  542, Train Loss: 1.0817, Valid Loss: 1.1079\n",
            "Epoch:  543, Train Loss: 1.0814, Valid Loss: 1.1055\n",
            "Epoch:  544, Train Loss: 1.0808, Valid Loss: 1.1060\n",
            "Epoch:  545, Train Loss: 1.0781, Valid Loss: 1.1061\n",
            "Epoch:  546, Train Loss: 1.0793, Valid Loss: 1.1060\n",
            "Epoch:  547, Train Loss: 1.0785, Valid Loss: 1.1048\n",
            "Epoch:  548, Train Loss: 1.0781, Valid Loss: 1.1044\n",
            "Epoch:  549, Train Loss: 1.0764, Valid Loss: 1.1032\n",
            "Epoch:  550, Train Loss: 1.0768, Valid Loss: 1.1035\n",
            "Epoch:  551, Train Loss: 1.0752, Valid Loss: 1.1028\n",
            "Epoch:  552, Train Loss: 1.0743, Valid Loss: 1.1024\n",
            "Epoch:  553, Train Loss: 1.0757, Valid Loss: 1.1024\n",
            "Epoch:  554, Train Loss: 1.0748, Valid Loss: 1.1032\n",
            "Epoch:  555, Train Loss: 1.0742, Valid Loss: 1.1011\n",
            "Epoch:  556, Train Loss: 1.0730, Valid Loss: 1.1014\n",
            "Epoch:  557, Train Loss: 1.0726, Valid Loss: 1.1012\n",
            "Epoch:  558, Train Loss: 1.0718, Valid Loss: 1.0996\n",
            "Epoch:  559, Train Loss: 1.0723, Valid Loss: 1.1007\n",
            "Epoch:  560, Train Loss: 1.0703, Valid Loss: 1.0992\n",
            "Epoch:  561, Train Loss: 1.0712, Valid Loss: 1.0986\n",
            "Epoch:  562, Train Loss: 1.0697, Valid Loss: 1.0990\n",
            "Epoch:  563, Train Loss: 1.0689, Valid Loss: 1.0996\n",
            "Epoch:  564, Train Loss: 1.0699, Valid Loss: 1.0999\n",
            "Epoch:  565, Train Loss: 1.0685, Valid Loss: 1.0959\n",
            "Epoch:  566, Train Loss: 1.0676, Valid Loss: 1.0967\n",
            "Epoch:  567, Train Loss: 1.0681, Valid Loss: 1.0964\n",
            "Epoch:  568, Train Loss: 1.0683, Valid Loss: 1.0963\n",
            "Epoch:  569, Train Loss: 1.0671, Valid Loss: 1.0974\n",
            "Epoch:  570, Train Loss: 1.0666, Valid Loss: 1.0960\n",
            "Epoch:  571, Train Loss: 1.0647, Valid Loss: 1.0961\n",
            "Epoch:  572, Train Loss: 1.0656, Valid Loss: 1.0962\n",
            "Epoch:  573, Train Loss: 1.0658, Valid Loss: 1.0960\n",
            "Epoch:  574, Train Loss: 1.0649, Valid Loss: 1.0946\n",
            "Epoch:  575, Train Loss: 1.0634, Valid Loss: 1.0943\n",
            "Epoch:  576, Train Loss: 1.0636, Valid Loss: 1.0941\n",
            "Epoch:  577, Train Loss: 1.0638, Valid Loss: 1.0935\n",
            "Epoch:  578, Train Loss: 1.0613, Valid Loss: 1.0942\n",
            "Epoch:  579, Train Loss: 1.0623, Valid Loss: 1.0926\n",
            "Epoch:  580, Train Loss: 1.0609, Valid Loss: 1.0935\n",
            "Epoch:  581, Train Loss: 1.0603, Valid Loss: 1.0940\n",
            "Epoch:  582, Train Loss: 1.0617, Valid Loss: 1.0933\n",
            "Epoch:  583, Train Loss: 1.0610, Valid Loss: 1.0908\n",
            "Epoch:  584, Train Loss: 1.0608, Valid Loss: 1.0914\n",
            "Epoch:  585, Train Loss: 1.0600, Valid Loss: 1.0927\n",
            "Epoch:  586, Train Loss: 1.0603, Valid Loss: 1.0907\n",
            "Epoch:  587, Train Loss: 1.0592, Valid Loss: 1.0909\n",
            "Epoch:  588, Train Loss: 1.0577, Valid Loss: 1.0901\n",
            "Epoch:  589, Train Loss: 1.0576, Valid Loss: 1.0894\n",
            "Epoch:  590, Train Loss: 1.0578, Valid Loss: 1.0915\n",
            "Epoch:  591, Train Loss: 1.0572, Valid Loss: 1.0901\n",
            "Epoch:  592, Train Loss: 1.0559, Valid Loss: 1.0894\n",
            "Epoch:  593, Train Loss: 1.0548, Valid Loss: 1.0903\n",
            "Epoch:  594, Train Loss: 1.0557, Valid Loss: 1.0890\n",
            "Epoch:  595, Train Loss: 1.0552, Valid Loss: 1.0881\n",
            "Epoch:  596, Train Loss: 1.0545, Valid Loss: 1.0898\n",
            "Epoch:  597, Train Loss: 1.0552, Valid Loss: 1.0875\n",
            "Epoch:  598, Train Loss: 1.0549, Valid Loss: 1.0865\n",
            "Epoch:  599, Train Loss: 1.0546, Valid Loss: 1.0884\n",
            "Epoch:  600, Train Loss: 1.0542, Valid Loss: 1.0893\n",
            "Epoch:  601, Train Loss: 1.0544, Valid Loss: 1.0868\n",
            "Epoch:  602, Train Loss: 1.0531, Valid Loss: 1.0860\n",
            "Epoch:  603, Train Loss: 1.0529, Valid Loss: 1.0872\n",
            "Epoch:  604, Train Loss: 1.0532, Valid Loss: 1.0866\n",
            "Epoch:  605, Train Loss: 1.0514, Valid Loss: 1.0857\n",
            "Epoch:  606, Train Loss: 1.0519, Valid Loss: 1.0870\n",
            "Epoch:  607, Train Loss: 1.0521, Valid Loss: 1.0861\n",
            "Epoch:  608, Train Loss: 1.0493, Valid Loss: 1.0846\n",
            "Epoch:  609, Train Loss: 1.0507, Valid Loss: 1.0854\n",
            "Epoch:  610, Train Loss: 1.0512, Valid Loss: 1.0853\n",
            "Epoch:  611, Train Loss: 1.0504, Valid Loss: 1.0864\n",
            "Epoch:  612, Train Loss: 1.0495, Valid Loss: 1.0840\n",
            "Epoch:  613, Train Loss: 1.0499, Valid Loss: 1.0827\n",
            "Epoch:  614, Train Loss: 1.0479, Valid Loss: 1.0843\n",
            "Epoch:  615, Train Loss: 1.0490, Valid Loss: 1.0829\n",
            "Epoch:  616, Train Loss: 1.0482, Valid Loss: 1.0823\n",
            "Epoch:  617, Train Loss: 1.0480, Valid Loss: 1.0834\n",
            "Epoch:  618, Train Loss: 1.0472, Valid Loss: 1.0835\n",
            "Epoch:  619, Train Loss: 1.0470, Valid Loss: 1.0834\n",
            "Epoch:  620, Train Loss: 1.0476, Valid Loss: 1.0828\n",
            "Epoch:  621, Train Loss: 1.0467, Valid Loss: 1.0829\n",
            "Epoch:  622, Train Loss: 1.0469, Valid Loss: 1.0813\n",
            "Epoch:  623, Train Loss: 1.0459, Valid Loss: 1.0811\n",
            "Epoch:  624, Train Loss: 1.0452, Valid Loss: 1.0822\n",
            "Epoch:  625, Train Loss: 1.0460, Valid Loss: 1.0823\n",
            "Epoch:  626, Train Loss: 1.0442, Valid Loss: 1.0815\n",
            "Epoch:  627, Train Loss: 1.0449, Valid Loss: 1.0798\n",
            "Epoch:  628, Train Loss: 1.0445, Valid Loss: 1.0793\n",
            "Epoch:  629, Train Loss: 1.0449, Valid Loss: 1.0825\n",
            "Epoch:  630, Train Loss: 1.0437, Valid Loss: 1.0819\n",
            "Epoch:  631, Train Loss: 1.0446, Valid Loss: 1.0791\n",
            "Epoch:  632, Train Loss: 1.0428, Valid Loss: 1.0797\n",
            "Epoch:  633, Train Loss: 1.0423, Valid Loss: 1.0793\n",
            "Epoch:  634, Train Loss: 1.0432, Valid Loss: 1.0804\n",
            "Epoch:  635, Train Loss: 1.0432, Valid Loss: 1.0795\n",
            "Epoch:  636, Train Loss: 1.0420, Valid Loss: 1.0785\n",
            "Epoch:  637, Train Loss: 1.0406, Valid Loss: 1.0797\n",
            "Epoch:  638, Train Loss: 1.0412, Valid Loss: 1.0774\n",
            "Epoch:  639, Train Loss: 1.0419, Valid Loss: 1.0785\n",
            "Epoch:  640, Train Loss: 1.0401, Valid Loss: 1.0785\n",
            "Epoch:  641, Train Loss: 1.0402, Valid Loss: 1.0777\n",
            "Epoch:  642, Train Loss: 1.0405, Valid Loss: 1.0782\n",
            "Epoch:  643, Train Loss: 1.0408, Valid Loss: 1.0787\n",
            "Epoch:  644, Train Loss: 1.0390, Valid Loss: 1.0771\n",
            "Epoch:  645, Train Loss: 1.0382, Valid Loss: 1.0770\n",
            "Epoch:  646, Train Loss: 1.0396, Valid Loss: 1.0769\n",
            "Epoch:  647, Train Loss: 1.0391, Valid Loss: 1.0770\n",
            "Epoch:  648, Train Loss: 1.0372, Valid Loss: 1.0764\n",
            "Epoch:  649, Train Loss: 1.0370, Valid Loss: 1.0758\n",
            "Epoch:  650, Train Loss: 1.0373, Valid Loss: 1.0762\n",
            "Epoch:  651, Train Loss: 1.0376, Valid Loss: 1.0765\n",
            "Epoch:  652, Train Loss: 1.0381, Valid Loss: 1.0757\n",
            "Epoch:  653, Train Loss: 1.0376, Valid Loss: 1.0764\n",
            "Epoch:  654, Train Loss: 1.0360, Valid Loss: 1.0748\n",
            "Epoch:  655, Train Loss: 1.0359, Valid Loss: 1.0746\n",
            "Epoch:  656, Train Loss: 1.0369, Valid Loss: 1.0753\n",
            "Epoch:  657, Train Loss: 1.0365, Valid Loss: 1.0755\n",
            "Epoch:  658, Train Loss: 1.0353, Valid Loss: 1.0744\n",
            "Epoch:  659, Train Loss: 1.0356, Valid Loss: 1.0743\n",
            "Epoch:  660, Train Loss: 1.0350, Valid Loss: 1.0740\n",
            "Epoch:  661, Train Loss: 1.0352, Valid Loss: 1.0737\n",
            "Epoch:  662, Train Loss: 1.0350, Valid Loss: 1.0732\n",
            "Epoch:  663, Train Loss: 1.0348, Valid Loss: 1.0746\n",
            "Epoch:  664, Train Loss: 1.0345, Valid Loss: 1.0736\n",
            "Epoch:  665, Train Loss: 1.0336, Valid Loss: 1.0757\n",
            "Epoch:  666, Train Loss: 1.0325, Valid Loss: 1.0731\n",
            "Epoch:  667, Train Loss: 1.0332, Valid Loss: 1.0714\n",
            "Epoch:  668, Train Loss: 1.0331, Valid Loss: 1.0721\n",
            "Epoch:  669, Train Loss: 1.0325, Valid Loss: 1.0721\n",
            "Epoch:  670, Train Loss: 1.0315, Valid Loss: 1.0732\n",
            "Epoch:  671, Train Loss: 1.0322, Valid Loss: 1.0725\n",
            "Epoch:  672, Train Loss: 1.0316, Valid Loss: 1.0720\n",
            "Epoch:  673, Train Loss: 1.0307, Valid Loss: 1.0717\n",
            "Epoch:  674, Train Loss: 1.0310, Valid Loss: 1.0699\n",
            "Epoch:  675, Train Loss: 1.0321, Valid Loss: 1.0716\n",
            "Epoch:  676, Train Loss: 1.0293, Valid Loss: 1.0723\n",
            "Epoch:  677, Train Loss: 1.0305, Valid Loss: 1.0719\n",
            "Epoch:  678, Train Loss: 1.0306, Valid Loss: 1.0709\n",
            "Epoch:  679, Train Loss: 1.0292, Valid Loss: 1.0705\n",
            "Epoch:  680, Train Loss: 1.0286, Valid Loss: 1.0695\n",
            "Epoch:  681, Train Loss: 1.0297, Valid Loss: 1.0715\n",
            "Epoch:  682, Train Loss: 1.0271, Valid Loss: 1.0707\n",
            "Epoch:  683, Train Loss: 1.0291, Valid Loss: 1.0696\n",
            "Epoch:  684, Train Loss: 1.0287, Valid Loss: 1.0699\n",
            "Epoch:  685, Train Loss: 1.0282, Valid Loss: 1.0697\n",
            "Epoch:  686, Train Loss: 1.0291, Valid Loss: 1.0685\n",
            "Epoch:  687, Train Loss: 1.0289, Valid Loss: 1.0691\n",
            "Epoch:  688, Train Loss: 1.0285, Valid Loss: 1.0677\n",
            "Epoch:  689, Train Loss: 1.0274, Valid Loss: 1.0704\n",
            "Epoch:  690, Train Loss: 1.0278, Valid Loss: 1.0693\n",
            "Epoch:  691, Train Loss: 1.0272, Valid Loss: 1.0692\n",
            "Epoch:  692, Train Loss: 1.0271, Valid Loss: 1.0683\n",
            "Epoch:  693, Train Loss: 1.0274, Valid Loss: 1.0675\n",
            "Epoch:  694, Train Loss: 1.0270, Valid Loss: 1.0680\n",
            "Epoch:  695, Train Loss: 1.0267, Valid Loss: 1.0672\n",
            "Epoch:  696, Train Loss: 1.0269, Valid Loss: 1.0680\n",
            "Epoch:  697, Train Loss: 1.0266, Valid Loss: 1.0690\n",
            "Epoch:  698, Train Loss: 1.0260, Valid Loss: 1.0673\n",
            "Epoch:  699, Train Loss: 1.0256, Valid Loss: 1.0673\n",
            "Epoch:  700, Train Loss: 1.0253, Valid Loss: 1.0668\n",
            "Epoch:  701, Train Loss: 1.0259, Valid Loss: 1.0665\n",
            "Epoch:  702, Train Loss: 1.0250, Valid Loss: 1.0668\n",
            "Epoch:  703, Train Loss: 1.0246, Valid Loss: 1.0679\n",
            "Epoch:  704, Train Loss: 1.0238, Valid Loss: 1.0660\n",
            "Epoch:  705, Train Loss: 1.0229, Valid Loss: 1.0665\n",
            "Epoch:  706, Train Loss: 1.0243, Valid Loss: 1.0657\n",
            "Epoch:  707, Train Loss: 1.0223, Valid Loss: 1.0654\n",
            "Epoch:  708, Train Loss: 1.0222, Valid Loss: 1.0654\n",
            "Epoch:  709, Train Loss: 1.0221, Valid Loss: 1.0664\n",
            "Epoch:  710, Train Loss: 1.0230, Valid Loss: 1.0650\n",
            "Epoch:  711, Train Loss: 1.0222, Valid Loss: 1.0646\n",
            "Epoch:  712, Train Loss: 1.0229, Valid Loss: 1.0664\n",
            "Epoch:  713, Train Loss: 1.0221, Valid Loss: 1.0654\n",
            "Epoch:  714, Train Loss: 1.0206, Valid Loss: 1.0640\n",
            "Epoch:  715, Train Loss: 1.0226, Valid Loss: 1.0638\n",
            "Epoch:  716, Train Loss: 1.0204, Valid Loss: 1.0655\n",
            "Epoch:  717, Train Loss: 1.0211, Valid Loss: 1.0649\n",
            "Epoch:  718, Train Loss: 1.0200, Valid Loss: 1.0647\n",
            "Epoch:  719, Train Loss: 1.0203, Valid Loss: 1.0637\n",
            "Epoch:  720, Train Loss: 1.0213, Valid Loss: 1.0626\n",
            "Epoch:  721, Train Loss: 1.0201, Valid Loss: 1.0642\n",
            "Epoch:  722, Train Loss: 1.0197, Valid Loss: 1.0640\n",
            "Epoch:  723, Train Loss: 1.0194, Valid Loss: 1.0631\n",
            "Epoch:  724, Train Loss: 1.0184, Valid Loss: 1.0643\n",
            "Epoch:  725, Train Loss: 1.0203, Valid Loss: 1.0632\n",
            "Epoch:  726, Train Loss: 1.0186, Valid Loss: 1.0614\n",
            "Epoch:  727, Train Loss: 1.0189, Valid Loss: 1.0619\n",
            "Epoch:  728, Train Loss: 1.0188, Valid Loss: 1.0629\n",
            "Epoch:  729, Train Loss: 1.0193, Valid Loss: 1.0650\n",
            "Epoch:  730, Train Loss: 1.0183, Valid Loss: 1.0621\n",
            "Epoch:  731, Train Loss: 1.0182, Valid Loss: 1.0620\n",
            "Epoch:  732, Train Loss: 1.0176, Valid Loss: 1.0610\n",
            "Epoch:  733, Train Loss: 1.0174, Valid Loss: 1.0604\n",
            "Epoch:  734, Train Loss: 1.0184, Valid Loss: 1.0622\n",
            "Epoch:  735, Train Loss: 1.0181, Valid Loss: 1.0622\n",
            "Epoch:  736, Train Loss: 1.0168, Valid Loss: 1.0598\n",
            "Epoch:  737, Train Loss: 1.0178, Valid Loss: 1.0619\n",
            "Epoch:  738, Train Loss: 1.0150, Valid Loss: 1.0595\n",
            "Epoch:  739, Train Loss: 1.0165, Valid Loss: 1.0604\n",
            "Epoch:  740, Train Loss: 1.0164, Valid Loss: 1.0615\n",
            "Epoch:  741, Train Loss: 1.0159, Valid Loss: 1.0603\n",
            "Epoch:  742, Train Loss: 1.0148, Valid Loss: 1.0605\n",
            "Epoch:  743, Train Loss: 1.0149, Valid Loss: 1.0596\n",
            "Epoch:  744, Train Loss: 1.0153, Valid Loss: 1.0592\n",
            "Epoch:  745, Train Loss: 1.0155, Valid Loss: 1.0602\n",
            "Epoch:  746, Train Loss: 1.0158, Valid Loss: 1.0590\n",
            "Epoch:  747, Train Loss: 1.0139, Valid Loss: 1.0609\n",
            "Epoch:  748, Train Loss: 1.0151, Valid Loss: 1.0594\n",
            "Epoch:  749, Train Loss: 1.0138, Valid Loss: 1.0596\n",
            "Epoch:  750, Train Loss: 1.0151, Valid Loss: 1.0581\n",
            "Epoch:  751, Train Loss: 1.0147, Valid Loss: 1.0587\n",
            "Epoch:  752, Train Loss: 1.0143, Valid Loss: 1.0582\n",
            "Epoch:  753, Train Loss: 1.0136, Valid Loss: 1.0587\n",
            "Epoch:  754, Train Loss: 1.0142, Valid Loss: 1.0585\n",
            "Epoch:  755, Train Loss: 1.0135, Valid Loss: 1.0599\n",
            "Epoch:  756, Train Loss: 1.0141, Valid Loss: 1.0580\n",
            "Epoch:  757, Train Loss: 1.0128, Valid Loss: 1.0585\n",
            "Epoch:  758, Train Loss: 1.0137, Valid Loss: 1.0566\n",
            "Epoch:  759, Train Loss: 1.0124, Valid Loss: 1.0565\n",
            "Epoch:  760, Train Loss: 1.0123, Valid Loss: 1.0578\n",
            "Epoch:  761, Train Loss: 1.0124, Valid Loss: 1.0586\n",
            "Epoch:  762, Train Loss: 1.0120, Valid Loss: 1.0582\n",
            "Epoch:  763, Train Loss: 1.0118, Valid Loss: 1.0560\n",
            "Epoch:  764, Train Loss: 1.0114, Valid Loss: 1.0557\n",
            "Epoch:  765, Train Loss: 1.0110, Valid Loss: 1.0571\n",
            "Epoch:  766, Train Loss: 1.0119, Valid Loss: 1.0579\n",
            "Epoch:  767, Train Loss: 1.0101, Valid Loss: 1.0571\n",
            "Epoch:  768, Train Loss: 1.0102, Valid Loss: 1.0569\n",
            "Epoch:  769, Train Loss: 1.0107, Valid Loss: 1.0547\n",
            "Epoch:  770, Train Loss: 1.0103, Valid Loss: 1.0540\n",
            "Epoch:  771, Train Loss: 1.0107, Valid Loss: 1.0557\n",
            "Epoch:  772, Train Loss: 1.0099, Valid Loss: 1.0581\n",
            "Epoch:  773, Train Loss: 1.0107, Valid Loss: 1.0561\n",
            "Epoch:  774, Train Loss: 1.0096, Valid Loss: 1.0545\n",
            "Epoch:  775, Train Loss: 1.0092, Valid Loss: 1.0545\n",
            "Epoch:  776, Train Loss: 1.0102, Valid Loss: 1.0556\n",
            "Epoch:  777, Train Loss: 1.0090, Valid Loss: 1.0549\n",
            "Epoch:  778, Train Loss: 1.0074, Valid Loss: 1.0546\n",
            "Epoch:  779, Train Loss: 1.0092, Valid Loss: 1.0556\n",
            "Epoch:  780, Train Loss: 1.0092, Valid Loss: 1.0543\n",
            "Epoch:  781, Train Loss: 1.0081, Valid Loss: 1.0536\n",
            "Epoch:  782, Train Loss: 1.0083, Valid Loss: 1.0530\n",
            "Epoch:  783, Train Loss: 1.0095, Valid Loss: 1.0552\n",
            "Epoch:  784, Train Loss: 1.0079, Valid Loss: 1.0536\n",
            "Epoch:  785, Train Loss: 1.0079, Valid Loss: 1.0530\n",
            "Epoch:  786, Train Loss: 1.0082, Valid Loss: 1.0535\n",
            "Epoch:  787, Train Loss: 1.0072, Valid Loss: 1.0532\n",
            "Epoch:  788, Train Loss: 1.0078, Valid Loss: 1.0520\n",
            "Epoch:  789, Train Loss: 1.0072, Valid Loss: 1.0539\n",
            "Epoch:  790, Train Loss: 1.0077, Valid Loss: 1.0544\n",
            "Epoch:  791, Train Loss: 1.0075, Valid Loss: 1.0524\n",
            "Epoch:  792, Train Loss: 1.0069, Valid Loss: 1.0517\n",
            "Epoch:  793, Train Loss: 1.0073, Valid Loss: 1.0518\n",
            "Epoch:  794, Train Loss: 1.0050, Valid Loss: 1.0525\n",
            "Epoch:  795, Train Loss: 1.0058, Valid Loss: 1.0522\n",
            "Epoch:  796, Train Loss: 1.0067, Valid Loss: 1.0522\n",
            "Epoch:  797, Train Loss: 1.0043, Valid Loss: 1.0524\n",
            "Epoch:  798, Train Loss: 1.0061, Valid Loss: 1.0518\n",
            "Epoch:  799, Train Loss: 1.0054, Valid Loss: 1.0517\n",
            "Epoch:  800, Train Loss: 1.0053, Valid Loss: 1.0500\n",
            "Epoch:  801, Train Loss: 1.0051, Valid Loss: 1.0507\n",
            "Epoch:  802, Train Loss: 1.0043, Valid Loss: 1.0537\n",
            "Epoch:  803, Train Loss: 1.0054, Valid Loss: 1.0516\n",
            "Epoch:  804, Train Loss: 1.0049, Valid Loss: 1.0497\n",
            "Epoch:  805, Train Loss: 1.0036, Valid Loss: 1.0513\n",
            "Epoch:  806, Train Loss: 1.0051, Valid Loss: 1.0500\n",
            "Epoch:  807, Train Loss: 1.0049, Valid Loss: 1.0515\n",
            "Epoch:  808, Train Loss: 1.0047, Valid Loss: 1.0495\n",
            "Epoch:  809, Train Loss: 1.0029, Valid Loss: 1.0500\n",
            "Epoch:  810, Train Loss: 1.0040, Valid Loss: 1.0504\n",
            "Epoch:  811, Train Loss: 1.0036, Valid Loss: 1.0507\n",
            "Epoch:  812, Train Loss: 1.0030, Valid Loss: 1.0487\n",
            "Epoch:  813, Train Loss: 1.0030, Valid Loss: 1.0492\n",
            "Epoch:  814, Train Loss: 1.0028, Valid Loss: 1.0505\n",
            "Epoch:  815, Train Loss: 1.0024, Valid Loss: 1.0489\n",
            "Epoch:  816, Train Loss: 1.0032, Valid Loss: 1.0488\n",
            "Epoch:  817, Train Loss: 1.0026, Valid Loss: 1.0491\n",
            "Epoch:  818, Train Loss: 1.0024, Valid Loss: 1.0490\n",
            "Epoch:  819, Train Loss: 1.0031, Valid Loss: 1.0489\n",
            "Epoch:  820, Train Loss: 1.0022, Valid Loss: 1.0496\n",
            "Epoch:  821, Train Loss: 1.0022, Valid Loss: 1.0491\n",
            "Epoch:  822, Train Loss: 1.0017, Valid Loss: 1.0480\n",
            "Epoch:  823, Train Loss: 1.0017, Valid Loss: 1.0480\n",
            "Epoch:  824, Train Loss: 1.0004, Valid Loss: 1.0483\n",
            "Epoch:  825, Train Loss: 1.0019, Valid Loss: 1.0475\n",
            "Epoch:  826, Train Loss: 1.0016, Valid Loss: 1.0482\n",
            "Epoch:  827, Train Loss: 0.9994, Valid Loss: 1.0475\n",
            "Epoch:  828, Train Loss: 1.0003, Valid Loss: 1.0472\n",
            "Epoch:  829, Train Loss: 1.0010, Valid Loss: 1.0483\n",
            "Epoch:  830, Train Loss: 0.9998, Valid Loss: 1.0478\n",
            "Epoch:  831, Train Loss: 1.0008, Valid Loss: 1.0467\n",
            "Epoch:  832, Train Loss: 1.0003, Valid Loss: 1.0471\n",
            "Epoch:  833, Train Loss: 1.0001, Valid Loss: 1.0464\n",
            "Epoch:  834, Train Loss: 1.0006, Valid Loss: 1.0464\n",
            "Epoch:  835, Train Loss: 0.9997, Valid Loss: 1.0473\n",
            "Epoch:  836, Train Loss: 0.9971, Valid Loss: 1.0472\n",
            "Epoch:  837, Train Loss: 0.9987, Valid Loss: 1.0463\n",
            "Epoch:  838, Train Loss: 1.0000, Valid Loss: 1.0460\n",
            "Epoch:  839, Train Loss: 0.9992, Valid Loss: 1.0460\n",
            "Epoch:  840, Train Loss: 0.9987, Valid Loss: 1.0457\n",
            "Epoch:  841, Train Loss: 0.9983, Valid Loss: 1.0465\n",
            "Epoch:  842, Train Loss: 0.9989, Valid Loss: 1.0465\n",
            "Epoch:  843, Train Loss: 0.9969, Valid Loss: 1.0451\n",
            "Epoch:  844, Train Loss: 0.9974, Valid Loss: 1.0453\n",
            "Epoch:  845, Train Loss: 0.9973, Valid Loss: 1.0456\n",
            "Epoch:  846, Train Loss: 0.9983, Valid Loss: 1.0460\n",
            "Epoch:  847, Train Loss: 0.9986, Valid Loss: 1.0446\n",
            "Epoch:  848, Train Loss: 0.9975, Valid Loss: 1.0453\n",
            "Epoch:  849, Train Loss: 0.9977, Valid Loss: 1.0444\n",
            "Epoch:  850, Train Loss: 0.9972, Valid Loss: 1.0439\n",
            "Epoch:  851, Train Loss: 0.9977, Valid Loss: 1.0444\n",
            "Epoch:  852, Train Loss: 0.9970, Valid Loss: 1.0453\n",
            "Epoch:  853, Train Loss: 0.9971, Valid Loss: 1.0444\n",
            "Epoch:  854, Train Loss: 0.9965, Valid Loss: 1.0436\n",
            "Epoch:  855, Train Loss: 0.9960, Valid Loss: 1.0440\n",
            "Epoch:  856, Train Loss: 0.9967, Valid Loss: 1.0442\n",
            "Epoch:  857, Train Loss: 0.9968, Valid Loss: 1.0441\n",
            "Epoch:  858, Train Loss: 0.9965, Valid Loss: 1.0436\n",
            "Epoch:  859, Train Loss: 0.9960, Valid Loss: 1.0428\n",
            "Epoch:  860, Train Loss: 0.9961, Valid Loss: 1.0436\n",
            "Epoch:  861, Train Loss: 0.9964, Valid Loss: 1.0446\n",
            "Epoch:  862, Train Loss: 0.9970, Valid Loss: 1.0417\n",
            "Epoch:  863, Train Loss: 0.9952, Valid Loss: 1.0430\n",
            "Epoch:  864, Train Loss: 0.9952, Valid Loss: 1.0447\n",
            "Epoch:  865, Train Loss: 0.9951, Valid Loss: 1.0438\n",
            "Epoch:  866, Train Loss: 0.9958, Valid Loss: 1.0417\n",
            "Epoch:  867, Train Loss: 0.9949, Valid Loss: 1.0418\n",
            "Epoch:  868, Train Loss: 0.9943, Valid Loss: 1.0421\n",
            "Epoch:  869, Train Loss: 0.9943, Valid Loss: 1.0426\n",
            "Epoch:  870, Train Loss: 0.9945, Valid Loss: 1.0434\n",
            "Epoch:  871, Train Loss: 0.9940, Valid Loss: 1.0407\n",
            "Epoch:  872, Train Loss: 0.9944, Valid Loss: 1.0409\n",
            "Epoch:  873, Train Loss: 0.9948, Valid Loss: 1.0427\n",
            "Epoch:  874, Train Loss: 0.9945, Valid Loss: 1.0429\n",
            "Epoch:  875, Train Loss: 0.9946, Valid Loss: 1.0425\n",
            "Epoch:  876, Train Loss: 0.9941, Valid Loss: 1.0413\n",
            "Epoch:  877, Train Loss: 0.9943, Valid Loss: 1.0408\n",
            "Epoch:  878, Train Loss: 0.9936, Valid Loss: 1.0400\n",
            "Epoch:  879, Train Loss: 0.9942, Valid Loss: 1.0406\n",
            "Epoch:  880, Train Loss: 0.9930, Valid Loss: 1.0412\n",
            "Epoch:  881, Train Loss: 0.9941, Valid Loss: 1.0407\n",
            "Epoch:  882, Train Loss: 0.9940, Valid Loss: 1.0418\n",
            "Epoch:  883, Train Loss: 0.9926, Valid Loss: 1.0410\n",
            "Epoch:  884, Train Loss: 0.9931, Valid Loss: 1.0416\n",
            "Epoch:  885, Train Loss: 0.9931, Valid Loss: 1.0401\n",
            "Epoch:  886, Train Loss: 0.9933, Valid Loss: 1.0404\n",
            "Epoch:  887, Train Loss: 0.9928, Valid Loss: 1.0384\n",
            "Epoch:  888, Train Loss: 0.9929, Valid Loss: 1.0403\n",
            "Epoch:  889, Train Loss: 0.9925, Valid Loss: 1.0401\n",
            "Epoch:  890, Train Loss: 0.9920, Valid Loss: 1.0411\n",
            "Epoch:  891, Train Loss: 0.9919, Valid Loss: 1.0396\n",
            "Epoch:  892, Train Loss: 0.9927, Valid Loss: 1.0384\n",
            "Epoch:  893, Train Loss: 0.9917, Valid Loss: 1.0398\n",
            "Epoch:  894, Train Loss: 0.9916, Valid Loss: 1.0395\n",
            "Epoch:  895, Train Loss: 0.9907, Valid Loss: 1.0393\n",
            "Epoch:  896, Train Loss: 0.9916, Valid Loss: 1.0388\n",
            "Epoch:  897, Train Loss: 0.9913, Valid Loss: 1.0390\n",
            "Epoch:  898, Train Loss: 0.9908, Valid Loss: 1.0389\n",
            "Epoch:  899, Train Loss: 0.9907, Valid Loss: 1.0392\n",
            "Epoch:  900, Train Loss: 0.9907, Valid Loss: 1.0390\n",
            "Epoch:  901, Train Loss: 0.9908, Valid Loss: 1.0384\n",
            "Epoch:  902, Train Loss: 0.9911, Valid Loss: 1.0368\n",
            "Epoch:  903, Train Loss: 0.9908, Valid Loss: 1.0377\n",
            "Epoch:  904, Train Loss: 0.9905, Valid Loss: 1.0372\n",
            "Epoch:  905, Train Loss: 0.9899, Valid Loss: 1.0387\n",
            "Epoch:  906, Train Loss: 0.9907, Valid Loss: 1.0388\n",
            "Epoch:  907, Train Loss: 0.9895, Valid Loss: 1.0372\n",
            "Epoch:  908, Train Loss: 0.9898, Valid Loss: 1.0372\n",
            "Epoch:  909, Train Loss: 0.9903, Valid Loss: 1.0370\n",
            "Epoch:  910, Train Loss: 0.9894, Valid Loss: 1.0374\n",
            "Epoch:  911, Train Loss: 0.9888, Valid Loss: 1.0369\n",
            "Epoch:  912, Train Loss: 0.9891, Valid Loss: 1.0380\n",
            "Epoch:  913, Train Loss: 0.9893, Valid Loss: 1.0365\n",
            "Epoch:  914, Train Loss: 0.9895, Valid Loss: 1.0367\n",
            "Epoch:  915, Train Loss: 0.9899, Valid Loss: 1.0369\n",
            "Epoch:  916, Train Loss: 0.9895, Valid Loss: 1.0370\n",
            "Epoch:  917, Train Loss: 0.9882, Valid Loss: 1.0364\n",
            "Epoch:  918, Train Loss: 0.9876, Valid Loss: 1.0363\n",
            "Epoch:  919, Train Loss: 0.9889, Valid Loss: 1.0380\n",
            "Epoch:  920, Train Loss: 0.9888, Valid Loss: 1.0362\n",
            "Epoch:  921, Train Loss: 0.9876, Valid Loss: 1.0348\n",
            "Epoch:  922, Train Loss: 0.9887, Valid Loss: 1.0367\n",
            "Epoch:  923, Train Loss: 0.9880, Valid Loss: 1.0372\n",
            "Epoch:  924, Train Loss: 0.9871, Valid Loss: 1.0367\n",
            "Epoch:  925, Train Loss: 0.9880, Valid Loss: 1.0350\n",
            "Epoch:  926, Train Loss: 0.9881, Valid Loss: 1.0352\n",
            "Epoch:  927, Train Loss: 0.9880, Valid Loss: 1.0356\n",
            "Epoch:  928, Train Loss: 0.9865, Valid Loss: 1.0363\n",
            "Epoch:  929, Train Loss: 0.9874, Valid Loss: 1.0348\n",
            "Epoch:  930, Train Loss: 0.9878, Valid Loss: 1.0351\n",
            "Epoch:  931, Train Loss: 0.9870, Valid Loss: 1.0356\n",
            "Epoch:  932, Train Loss: 0.9870, Valid Loss: 1.0353\n",
            "Epoch:  933, Train Loss: 0.9868, Valid Loss: 1.0353\n",
            "Epoch:  934, Train Loss: 0.9862, Valid Loss: 1.0336\n",
            "Epoch:  935, Train Loss: 0.9861, Valid Loss: 1.0345\n",
            "Epoch:  936, Train Loss: 0.9866, Valid Loss: 1.0346\n",
            "Epoch:  937, Train Loss: 0.9868, Valid Loss: 1.0343\n",
            "Epoch:  938, Train Loss: 0.9857, Valid Loss: 1.0352\n",
            "Epoch:  939, Train Loss: 0.9863, Valid Loss: 1.0340\n",
            "Epoch:  940, Train Loss: 0.9855, Valid Loss: 1.0346\n",
            "Epoch:  941, Train Loss: 0.9853, Valid Loss: 1.0338\n",
            "Epoch:  942, Train Loss: 0.9852, Valid Loss: 1.0339\n",
            "Epoch:  943, Train Loss: 0.9857, Valid Loss: 1.0346\n",
            "Epoch:  944, Train Loss: 0.9852, Valid Loss: 1.0331\n",
            "Epoch:  945, Train Loss: 0.9855, Valid Loss: 1.0331\n",
            "Epoch:  946, Train Loss: 0.9837, Valid Loss: 1.0345\n",
            "Epoch:  947, Train Loss: 0.9856, Valid Loss: 1.0345\n",
            "Epoch:  948, Train Loss: 0.9833, Valid Loss: 1.0342\n",
            "Epoch:  949, Train Loss: 0.9854, Valid Loss: 1.0319\n",
            "Epoch:  950, Train Loss: 0.9825, Valid Loss: 1.0326\n",
            "Epoch:  951, Train Loss: 0.9843, Valid Loss: 1.0326\n",
            "Epoch:  952, Train Loss: 0.9846, Valid Loss: 1.0332\n",
            "Epoch:  953, Train Loss: 0.9856, Valid Loss: 1.0317\n",
            "Epoch:  954, Train Loss: 0.9844, Valid Loss: 1.0332\n",
            "Epoch:  955, Train Loss: 0.9840, Valid Loss: 1.0324\n",
            "Epoch:  956, Train Loss: 0.9842, Valid Loss: 1.0325\n",
            "Epoch:  957, Train Loss: 0.9841, Valid Loss: 1.0329\n",
            "Epoch:  958, Train Loss: 0.9843, Valid Loss: 1.0326\n",
            "Epoch:  959, Train Loss: 0.9836, Valid Loss: 1.0327\n",
            "Epoch:  960, Train Loss: 0.9833, Valid Loss: 1.0321\n",
            "Epoch:  961, Train Loss: 0.9837, Valid Loss: 1.0320\n",
            "Epoch:  962, Train Loss: 0.9827, Valid Loss: 1.0319\n",
            "Epoch:  963, Train Loss: 0.9834, Valid Loss: 1.0317\n",
            "Epoch:  964, Train Loss: 0.9824, Valid Loss: 1.0317\n",
            "Epoch:  965, Train Loss: 0.9830, Valid Loss: 1.0328\n",
            "Epoch:  966, Train Loss: 0.9819, Valid Loss: 1.0306\n",
            "Epoch:  967, Train Loss: 0.9829, Valid Loss: 1.0322\n",
            "Epoch:  968, Train Loss: 0.9836, Valid Loss: 1.0308\n",
            "Epoch:  969, Train Loss: 0.9822, Valid Loss: 1.0319\n",
            "Epoch:  970, Train Loss: 0.9831, Valid Loss: 1.0310\n",
            "Epoch:  971, Train Loss: 0.9833, Valid Loss: 1.0313\n",
            "Epoch:  972, Train Loss: 0.9831, Valid Loss: 1.0302\n",
            "Epoch:  973, Train Loss: 0.9829, Valid Loss: 1.0301\n",
            "Epoch:  974, Train Loss: 0.9825, Valid Loss: 1.0302\n",
            "Epoch:  975, Train Loss: 0.9818, Valid Loss: 1.0313\n",
            "Epoch:  976, Train Loss: 0.9824, Valid Loss: 1.0307\n",
            "Epoch:  977, Train Loss: 0.9823, Valid Loss: 1.0290\n",
            "Epoch:  978, Train Loss: 0.9824, Valid Loss: 1.0300\n",
            "Epoch:  979, Train Loss: 0.9807, Valid Loss: 1.0317\n",
            "Epoch:  980, Train Loss: 0.9813, Valid Loss: 1.0304\n",
            "Epoch:  981, Train Loss: 0.9814, Valid Loss: 1.0299\n",
            "Epoch:  982, Train Loss: 0.9817, Valid Loss: 1.0293\n",
            "Epoch:  983, Train Loss: 0.9811, Valid Loss: 1.0300\n",
            "Epoch:  984, Train Loss: 0.9803, Valid Loss: 1.0297\n",
            "Epoch:  985, Train Loss: 0.9812, Valid Loss: 1.0289\n",
            "Epoch:  986, Train Loss: 0.9815, Valid Loss: 1.0300\n",
            "Epoch:  987, Train Loss: 0.9812, Valid Loss: 1.0297\n",
            "Epoch:  988, Train Loss: 0.9815, Valid Loss: 1.0282\n",
            "Epoch:  989, Train Loss: 0.9799, Valid Loss: 1.0288\n",
            "Epoch:  990, Train Loss: 0.9803, Valid Loss: 1.0302\n",
            "Epoch:  991, Train Loss: 0.9801, Valid Loss: 1.0293\n",
            "Epoch:  992, Train Loss: 0.9804, Valid Loss: 1.0293\n",
            "Epoch:  993, Train Loss: 0.9804, Valid Loss: 1.0279\n",
            "Epoch:  994, Train Loss: 0.9788, Valid Loss: 1.0285\n",
            "Epoch:  995, Train Loss: 0.9807, Valid Loss: 1.0286\n",
            "Epoch:  996, Train Loss: 0.9784, Valid Loss: 1.0284\n",
            "Epoch:  997, Train Loss: 0.9798, Valid Loss: 1.0283\n",
            "Epoch:  998, Train Loss: 0.9796, Valid Loss: 1.0287\n",
            "Epoch:  999, Train Loss: 0.9800, Valid Loss: 1.0291\n",
            "Epoch: 1000, Train Loss: 0.9800, Valid Loss: 1.0278\n",
            "Epoch: 1001, Train Loss: 0.9788, Valid Loss: 1.0275\n",
            "Epoch: 1002, Train Loss: 0.9795, Valid Loss: 1.0284\n",
            "Epoch: 1003, Train Loss: 0.9795, Valid Loss: 1.0271\n",
            "Epoch: 1004, Train Loss: 0.9793, Valid Loss: 1.0282\n",
            "Epoch: 1005, Train Loss: 0.9795, Valid Loss: 1.0279\n",
            "Epoch: 1006, Train Loss: 0.9782, Valid Loss: 1.0283\n",
            "Epoch: 1007, Train Loss: 0.9790, Valid Loss: 1.0274\n",
            "Epoch: 1008, Train Loss: 0.9790, Valid Loss: 1.0260\n",
            "Epoch: 1009, Train Loss: 0.9789, Valid Loss: 1.0272\n",
            "Epoch: 1010, Train Loss: 0.9783, Valid Loss: 1.0283\n",
            "Epoch: 1011, Train Loss: 0.9780, Valid Loss: 1.0273\n",
            "Epoch: 1012, Train Loss: 0.9777, Valid Loss: 1.0264\n",
            "Epoch: 1013, Train Loss: 0.9786, Valid Loss: 1.0270\n",
            "Epoch: 1014, Train Loss: 0.9783, Valid Loss: 1.0270\n",
            "Epoch: 1015, Train Loss: 0.9772, Valid Loss: 1.0266\n",
            "Epoch: 1016, Train Loss: 0.9783, Valid Loss: 1.0261\n",
            "Epoch: 1017, Train Loss: 0.9775, Valid Loss: 1.0261\n",
            "Epoch: 1018, Train Loss: 0.9776, Valid Loss: 1.0266\n",
            "Epoch: 1019, Train Loss: 0.9778, Valid Loss: 1.0269\n",
            "Epoch: 1020, Train Loss: 0.9777, Valid Loss: 1.0269\n",
            "Epoch: 1021, Train Loss: 0.9776, Valid Loss: 1.0256\n",
            "Epoch: 1022, Train Loss: 0.9786, Valid Loss: 1.0267\n",
            "Epoch: 1023, Train Loss: 0.9761, Valid Loss: 1.0259\n",
            "Epoch: 1024, Train Loss: 0.9778, Valid Loss: 1.0261\n",
            "Epoch: 1025, Train Loss: 0.9777, Valid Loss: 1.0251\n",
            "Epoch: 1026, Train Loss: 0.9778, Valid Loss: 1.0261\n",
            "Epoch: 1027, Train Loss: 0.9764, Valid Loss: 1.0263\n",
            "Epoch: 1028, Train Loss: 0.9768, Valid Loss: 1.0260\n",
            "Epoch: 1029, Train Loss: 0.9774, Valid Loss: 1.0256\n",
            "Epoch: 1030, Train Loss: 0.9766, Valid Loss: 1.0250\n",
            "Epoch: 1031, Train Loss: 0.9770, Valid Loss: 1.0235\n",
            "Epoch: 1032, Train Loss: 0.9765, Valid Loss: 1.0269\n",
            "Epoch: 1033, Train Loss: 0.9768, Valid Loss: 1.0254\n",
            "Epoch: 1034, Train Loss: 0.9763, Valid Loss: 1.0245\n",
            "Epoch: 1035, Train Loss: 0.9766, Valid Loss: 1.0233\n",
            "Epoch: 1036, Train Loss: 0.9770, Valid Loss: 1.0248\n",
            "Epoch: 1037, Train Loss: 0.9763, Valid Loss: 1.0264\n",
            "Epoch: 1038, Train Loss: 0.9763, Valid Loss: 1.0234\n",
            "Epoch: 1039, Train Loss: 0.9765, Valid Loss: 1.0238\n",
            "Epoch: 1040, Train Loss: 0.9770, Valid Loss: 1.0248\n",
            "Epoch: 1041, Train Loss: 0.9755, Valid Loss: 1.0244\n",
            "Epoch: 1042, Train Loss: 0.9758, Valid Loss: 1.0236\n",
            "Epoch: 1043, Train Loss: 0.9764, Valid Loss: 1.0255\n",
            "Epoch: 1044, Train Loss: 0.9754, Valid Loss: 1.0241\n",
            "Epoch: 1045, Train Loss: 0.9751, Valid Loss: 1.0231\n",
            "Epoch: 1046, Train Loss: 0.9757, Valid Loss: 1.0227\n",
            "Epoch: 1047, Train Loss: 0.9753, Valid Loss: 1.0263\n",
            "Epoch: 1048, Train Loss: 0.9744, Valid Loss: 1.0241\n",
            "Epoch: 1049, Train Loss: 0.9751, Valid Loss: 1.0234\n",
            "Epoch: 1050, Train Loss: 0.9746, Valid Loss: 1.0232\n",
            "Epoch: 1051, Train Loss: 0.9759, Valid Loss: 1.0231\n",
            "Epoch: 1052, Train Loss: 0.9753, Valid Loss: 1.0238\n",
            "Epoch: 1053, Train Loss: 0.9751, Valid Loss: 1.0231\n",
            "Epoch: 1054, Train Loss: 0.9732, Valid Loss: 1.0226\n",
            "Epoch: 1055, Train Loss: 0.9741, Valid Loss: 1.0228\n",
            "Epoch: 1056, Train Loss: 0.9740, Valid Loss: 1.0227\n",
            "Epoch: 1057, Train Loss: 0.9746, Valid Loss: 1.0233\n",
            "Epoch: 1058, Train Loss: 0.9744, Valid Loss: 1.0223\n",
            "Epoch: 1059, Train Loss: 0.9752, Valid Loss: 1.0228\n",
            "Epoch: 1060, Train Loss: 0.9739, Valid Loss: 1.0222\n",
            "Epoch: 1061, Train Loss: 0.9746, Valid Loss: 1.0218\n",
            "Epoch: 1062, Train Loss: 0.9740, Valid Loss: 1.0238\n",
            "Epoch: 1063, Train Loss: 0.9747, Valid Loss: 1.0249\n",
            "Epoch: 1064, Train Loss: 0.9732, Valid Loss: 1.0206\n",
            "Epoch: 1065, Train Loss: 0.9745, Valid Loss: 1.0202\n",
            "Epoch: 1066, Train Loss: 0.9734, Valid Loss: 1.0224\n",
            "Epoch: 1067, Train Loss: 0.9730, Valid Loss: 1.0225\n",
            "Epoch: 1068, Train Loss: 0.9742, Valid Loss: 1.0220\n",
            "Epoch: 1069, Train Loss: 0.9731, Valid Loss: 1.0211\n",
            "Epoch: 1070, Train Loss: 0.9735, Valid Loss: 1.0218\n",
            "Epoch: 1071, Train Loss: 0.9728, Valid Loss: 1.0205\n",
            "Epoch: 1072, Train Loss: 0.9742, Valid Loss: 1.0217\n",
            "Epoch: 1073, Train Loss: 0.9737, Valid Loss: 1.0218\n",
            "Epoch: 1074, Train Loss: 0.9733, Valid Loss: 1.0206\n",
            "Epoch: 1075, Train Loss: 0.9738, Valid Loss: 1.0214\n",
            "Epoch: 1076, Train Loss: 0.9734, Valid Loss: 1.0212\n",
            "Epoch: 1077, Train Loss: 0.9738, Valid Loss: 1.0226\n",
            "Epoch: 1078, Train Loss: 0.9726, Valid Loss: 1.0211\n",
            "Epoch: 1079, Train Loss: 0.9735, Valid Loss: 1.0199\n",
            "Epoch: 1080, Train Loss: 0.9719, Valid Loss: 1.0210\n",
            "Epoch: 1081, Train Loss: 0.9733, Valid Loss: 1.0213\n",
            "Epoch: 1082, Train Loss: 0.9725, Valid Loss: 1.0220\n",
            "Epoch: 1083, Train Loss: 0.9728, Valid Loss: 1.0193\n",
            "Epoch: 1084, Train Loss: 0.9724, Valid Loss: 1.0206\n",
            "Epoch: 1085, Train Loss: 0.9725, Valid Loss: 1.0212\n",
            "Epoch: 1086, Train Loss: 0.9728, Valid Loss: 1.0200\n",
            "Epoch: 1087, Train Loss: 0.9729, Valid Loss: 1.0207\n",
            "Epoch: 1088, Train Loss: 0.9714, Valid Loss: 1.0201\n",
            "Epoch: 1089, Train Loss: 0.9730, Valid Loss: 1.0203\n",
            "Epoch: 1090, Train Loss: 0.9718, Valid Loss: 1.0201\n",
            "Epoch: 1091, Train Loss: 0.9723, Valid Loss: 1.0210\n",
            "Epoch: 1092, Train Loss: 0.9727, Valid Loss: 1.0209\n",
            "Epoch: 1093, Train Loss: 0.9721, Valid Loss: 1.0182\n",
            "Epoch: 1094, Train Loss: 0.9716, Valid Loss: 1.0194\n",
            "Epoch: 1095, Train Loss: 0.9720, Valid Loss: 1.0199\n",
            "Epoch: 1096, Train Loss: 0.9715, Valid Loss: 1.0197\n",
            "Epoch: 1097, Train Loss: 0.9716, Valid Loss: 1.0193\n",
            "Epoch: 1098, Train Loss: 0.9718, Valid Loss: 1.0190\n",
            "Epoch: 1099, Train Loss: 0.9714, Valid Loss: 1.0200\n",
            "Epoch: 1100, Train Loss: 0.9710, Valid Loss: 1.0199\n",
            "Epoch: 1101, Train Loss: 0.9712, Valid Loss: 1.0196\n",
            "Epoch: 1102, Train Loss: 0.9712, Valid Loss: 1.0192\n",
            "Epoch: 1103, Train Loss: 0.9718, Valid Loss: 1.0190\n",
            "Epoch: 1104, Train Loss: 0.9715, Valid Loss: 1.0182\n",
            "Epoch: 1105, Train Loss: 0.9694, Valid Loss: 1.0194\n",
            "Epoch: 1106, Train Loss: 0.9716, Valid Loss: 1.0198\n",
            "Epoch: 1107, Train Loss: 0.9698, Valid Loss: 1.0205\n",
            "Epoch: 1108, Train Loss: 0.9699, Valid Loss: 1.0176\n",
            "Epoch: 1109, Train Loss: 0.9708, Valid Loss: 1.0168\n",
            "Epoch: 1110, Train Loss: 0.9708, Valid Loss: 1.0193\n",
            "Epoch: 1111, Train Loss: 0.9705, Valid Loss: 1.0206\n",
            "Epoch: 1112, Train Loss: 0.9699, Valid Loss: 1.0171\n",
            "Epoch: 1113, Train Loss: 0.9705, Valid Loss: 1.0178\n",
            "Epoch: 1114, Train Loss: 0.9710, Valid Loss: 1.0172\n",
            "Epoch: 1115, Train Loss: 0.9701, Valid Loss: 1.0186\n",
            "Epoch: 1116, Train Loss: 0.9696, Valid Loss: 1.0189\n",
            "Epoch: 1117, Train Loss: 0.9696, Valid Loss: 1.0192\n",
            "Epoch: 1118, Train Loss: 0.9698, Valid Loss: 1.0173\n",
            "Epoch: 1119, Train Loss: 0.9701, Valid Loss: 1.0168\n",
            "Epoch: 1120, Train Loss: 0.9695, Valid Loss: 1.0174\n",
            "Epoch: 1121, Train Loss: 0.9692, Valid Loss: 1.0183\n",
            "Epoch: 1122, Train Loss: 0.9709, Valid Loss: 1.0173\n",
            "Epoch: 1123, Train Loss: 0.9695, Valid Loss: 1.0184\n",
            "Epoch: 1124, Train Loss: 0.9700, Valid Loss: 1.0174\n",
            "Epoch: 1125, Train Loss: 0.9695, Valid Loss: 1.0180\n",
            "Epoch: 1126, Train Loss: 0.9691, Valid Loss: 1.0167\n",
            "Epoch: 1127, Train Loss: 0.9698, Valid Loss: 1.0172\n",
            "Epoch: 1128, Train Loss: 0.9696, Valid Loss: 1.0176\n",
            "Epoch: 1129, Train Loss: 0.9691, Valid Loss: 1.0177\n",
            "Epoch: 1130, Train Loss: 0.9692, Valid Loss: 1.0172\n",
            "Epoch: 1131, Train Loss: 0.9687, Valid Loss: 1.0165\n",
            "Epoch: 1132, Train Loss: 0.9690, Valid Loss: 1.0166\n",
            "Epoch: 1133, Train Loss: 0.9698, Valid Loss: 1.0161\n",
            "Epoch: 1134, Train Loss: 0.9674, Valid Loss: 1.0174\n",
            "Epoch: 1135, Train Loss: 0.9697, Valid Loss: 1.0179\n",
            "Epoch: 1136, Train Loss: 0.9678, Valid Loss: 1.0156\n",
            "Epoch: 1137, Train Loss: 0.9691, Valid Loss: 1.0167\n",
            "Epoch: 1138, Train Loss: 0.9682, Valid Loss: 1.0168\n",
            "Epoch: 1139, Train Loss: 0.9680, Valid Loss: 1.0160\n",
            "Epoch: 1140, Train Loss: 0.9691, Valid Loss: 1.0153\n",
            "Epoch: 1141, Train Loss: 0.9678, Valid Loss: 1.0172\n",
            "Epoch: 1142, Train Loss: 0.9671, Valid Loss: 1.0165\n",
            "Epoch: 1143, Train Loss: 0.9688, Valid Loss: 1.0148\n",
            "Epoch: 1144, Train Loss: 0.9676, Valid Loss: 1.0170\n",
            "Epoch: 1145, Train Loss: 0.9685, Valid Loss: 1.0168\n",
            "Epoch: 1146, Train Loss: 0.9684, Valid Loss: 1.0142\n",
            "Epoch: 1147, Train Loss: 0.9669, Valid Loss: 1.0166\n",
            "Epoch: 1148, Train Loss: 0.9674, Valid Loss: 1.0166\n",
            "Epoch: 1149, Train Loss: 0.9680, Valid Loss: 1.0156\n",
            "Epoch: 1150, Train Loss: 0.9661, Valid Loss: 1.0140\n",
            "Epoch: 1151, Train Loss: 0.9684, Valid Loss: 1.0150\n",
            "Epoch: 1152, Train Loss: 0.9676, Valid Loss: 1.0152\n",
            "Epoch: 1153, Train Loss: 0.9665, Valid Loss: 1.0150\n",
            "Epoch: 1154, Train Loss: 0.9671, Valid Loss: 1.0156\n",
            "Epoch: 1155, Train Loss: 0.9684, Valid Loss: 1.0154\n",
            "Epoch: 1156, Train Loss: 0.9668, Valid Loss: 1.0140\n",
            "Epoch: 1157, Train Loss: 0.9680, Valid Loss: 1.0140\n",
            "Epoch: 1158, Train Loss: 0.9667, Valid Loss: 1.0162\n",
            "Epoch: 1159, Train Loss: 0.9658, Valid Loss: 1.0161\n",
            "Epoch: 1160, Train Loss: 0.9683, Valid Loss: 1.0131\n",
            "Epoch: 1161, Train Loss: 0.9673, Valid Loss: 1.0155\n",
            "Epoch: 1162, Train Loss: 0.9675, Valid Loss: 1.0147\n",
            "Epoch: 1163, Train Loss: 0.9637, Valid Loss: 1.0140\n",
            "Epoch: 1164, Train Loss: 0.9658, Valid Loss: 1.0149\n",
            "Epoch: 1165, Train Loss: 0.9660, Valid Loss: 1.0140\n",
            "Epoch: 1166, Train Loss: 0.9675, Valid Loss: 1.0142\n",
            "Epoch: 1167, Train Loss: 0.9660, Valid Loss: 1.0129\n",
            "Epoch: 1168, Train Loss: 0.9662, Valid Loss: 1.0139\n",
            "Epoch: 1169, Train Loss: 0.9662, Valid Loss: 1.0146\n",
            "Epoch: 1170, Train Loss: 0.9655, Valid Loss: 1.0138\n",
            "Epoch: 1171, Train Loss: 0.9663, Valid Loss: 1.0131\n",
            "Epoch: 1172, Train Loss: 0.9664, Valid Loss: 1.0131\n",
            "Epoch: 1173, Train Loss: 0.9661, Valid Loss: 1.0141\n",
            "Epoch: 1174, Train Loss: 0.9662, Valid Loss: 1.0143\n",
            "Epoch: 1175, Train Loss: 0.9663, Valid Loss: 1.0133\n",
            "Epoch: 1176, Train Loss: 0.9670, Valid Loss: 1.0118\n",
            "Epoch: 1177, Train Loss: 0.9658, Valid Loss: 1.0140\n",
            "Epoch: 1178, Train Loss: 0.9667, Valid Loss: 1.0136\n",
            "Epoch: 1179, Train Loss: 0.9650, Valid Loss: 1.0132\n",
            "Epoch: 1180, Train Loss: 0.9660, Valid Loss: 1.0141\n",
            "Epoch: 1181, Train Loss: 0.9665, Valid Loss: 1.0131\n",
            "Epoch: 1182, Train Loss: 0.9664, Valid Loss: 1.0128\n",
            "Epoch: 1183, Train Loss: 0.9655, Valid Loss: 1.0119\n",
            "Epoch: 1184, Train Loss: 0.9653, Valid Loss: 1.0137\n",
            "Epoch: 1185, Train Loss: 0.9656, Valid Loss: 1.0123\n",
            "Epoch: 1186, Train Loss: 0.9658, Valid Loss: 1.0125\n",
            "Epoch: 1187, Train Loss: 0.9654, Valid Loss: 1.0126\n",
            "Epoch: 1188, Train Loss: 0.9662, Valid Loss: 1.0132\n",
            "Epoch: 1189, Train Loss: 0.9646, Valid Loss: 1.0113\n",
            "Epoch: 1190, Train Loss: 0.9655, Valid Loss: 1.0116\n",
            "Epoch: 1191, Train Loss: 0.9655, Valid Loss: 1.0119\n",
            "Epoch: 1192, Train Loss: 0.9653, Valid Loss: 1.0137\n",
            "Epoch: 1193, Train Loss: 0.9646, Valid Loss: 1.0107\n",
            "Epoch: 1194, Train Loss: 0.9656, Valid Loss: 1.0124\n",
            "Epoch: 1195, Train Loss: 0.9661, Valid Loss: 1.0123\n",
            "Epoch: 1196, Train Loss: 0.9652, Valid Loss: 1.0127\n",
            "Epoch: 1197, Train Loss: 0.9654, Valid Loss: 1.0117\n",
            "Epoch: 1198, Train Loss: 0.9646, Valid Loss: 1.0111\n",
            "Epoch: 1199, Train Loss: 0.9656, Valid Loss: 1.0111\n",
            "Epoch: 1200, Train Loss: 0.9652, Valid Loss: 1.0103\n",
            "Epoch: 1201, Train Loss: 0.9639, Valid Loss: 1.0110\n",
            "Epoch: 1202, Train Loss: 0.9646, Valid Loss: 1.0126\n",
            "Epoch: 1203, Train Loss: 0.9635, Valid Loss: 1.0110\n",
            "Epoch: 1204, Train Loss: 0.9644, Valid Loss: 1.0097\n",
            "Epoch: 1205, Train Loss: 0.9641, Valid Loss: 1.0106\n",
            "Epoch: 1206, Train Loss: 0.9633, Valid Loss: 1.0125\n",
            "Epoch: 1207, Train Loss: 0.9652, Valid Loss: 1.0121\n",
            "Epoch: 1208, Train Loss: 0.9636, Valid Loss: 1.0104\n",
            "Epoch: 1209, Train Loss: 0.9646, Valid Loss: 1.0116\n",
            "Epoch: 1210, Train Loss: 0.9647, Valid Loss: 1.0108\n",
            "Epoch: 1211, Train Loss: 0.9633, Valid Loss: 1.0109\n",
            "Epoch: 1212, Train Loss: 0.9629, Valid Loss: 1.0104\n",
            "Epoch: 1213, Train Loss: 0.9647, Valid Loss: 1.0098\n",
            "Epoch: 1214, Train Loss: 0.9645, Valid Loss: 1.0105\n",
            "Epoch: 1215, Train Loss: 0.9651, Valid Loss: 1.0106\n",
            "Epoch: 1216, Train Loss: 0.9637, Valid Loss: 1.0115\n",
            "Epoch: 1217, Train Loss: 0.9650, Valid Loss: 1.0088\n",
            "Epoch: 1218, Train Loss: 0.9650, Valid Loss: 1.0098\n",
            "Epoch: 1219, Train Loss: 0.9638, Valid Loss: 1.0114\n",
            "Epoch: 1220, Train Loss: 0.9636, Valid Loss: 1.0105\n",
            "Epoch: 1221, Train Loss: 0.9638, Valid Loss: 1.0100\n",
            "Epoch: 1222, Train Loss: 0.9627, Valid Loss: 1.0101\n",
            "Epoch: 1223, Train Loss: 0.9641, Valid Loss: 1.0094\n",
            "Epoch: 1224, Train Loss: 0.9639, Valid Loss: 1.0093\n",
            "Epoch: 1225, Train Loss: 0.9632, Valid Loss: 1.0098\n",
            "Epoch: 1226, Train Loss: 0.9640, Valid Loss: 1.0104\n",
            "Epoch: 1227, Train Loss: 0.9628, Valid Loss: 1.0093\n",
            "Epoch: 1228, Train Loss: 0.9641, Valid Loss: 1.0091\n",
            "Epoch: 1229, Train Loss: 0.9629, Valid Loss: 1.0095\n",
            "Epoch: 1230, Train Loss: 0.9632, Valid Loss: 1.0088\n",
            "Epoch: 1231, Train Loss: 0.9627, Valid Loss: 1.0101\n",
            "Epoch: 1232, Train Loss: 0.9637, Valid Loss: 1.0092\n",
            "Epoch: 1233, Train Loss: 0.9635, Valid Loss: 1.0090\n",
            "Epoch: 1234, Train Loss: 0.9623, Valid Loss: 1.0081\n",
            "Epoch: 1235, Train Loss: 0.9628, Valid Loss: 1.0094\n",
            "Epoch: 1236, Train Loss: 0.9635, Valid Loss: 1.0092\n",
            "Epoch: 1237, Train Loss: 0.9630, Valid Loss: 1.0091\n",
            "Epoch: 1238, Train Loss: 0.9632, Valid Loss: 1.0093\n",
            "Epoch: 1239, Train Loss: 0.9632, Valid Loss: 1.0082\n",
            "Epoch: 1240, Train Loss: 0.9620, Valid Loss: 1.0075\n",
            "Epoch: 1241, Train Loss: 0.9626, Valid Loss: 1.0079\n",
            "Epoch: 1242, Train Loss: 0.9633, Valid Loss: 1.0087\n",
            "Epoch: 1243, Train Loss: 0.9610, Valid Loss: 1.0086\n",
            "Epoch: 1244, Train Loss: 0.9633, Valid Loss: 1.0076\n",
            "Epoch: 1245, Train Loss: 0.9622, Valid Loss: 1.0094\n",
            "Epoch: 1246, Train Loss: 0.9628, Valid Loss: 1.0080\n",
            "Epoch: 1247, Train Loss: 0.9613, Valid Loss: 1.0072\n",
            "Epoch: 1248, Train Loss: 0.9623, Valid Loss: 1.0080\n",
            "Epoch: 1249, Train Loss: 0.9618, Valid Loss: 1.0082\n",
            "Epoch: 1250, Train Loss: 0.9619, Valid Loss: 1.0079\n",
            "Epoch: 1251, Train Loss: 0.9621, Valid Loss: 1.0073\n",
            "Epoch: 1252, Train Loss: 0.9620, Valid Loss: 1.0075\n",
            "Epoch: 1253, Train Loss: 0.9614, Valid Loss: 1.0069\n",
            "Epoch: 1254, Train Loss: 0.9610, Valid Loss: 1.0081\n",
            "Epoch: 1255, Train Loss: 0.9622, Valid Loss: 1.0074\n",
            "Epoch: 1256, Train Loss: 0.9621, Valid Loss: 1.0071\n",
            "Epoch: 1257, Train Loss: 0.9615, Valid Loss: 1.0067\n",
            "Epoch: 1258, Train Loss: 0.9612, Valid Loss: 1.0077\n",
            "Epoch: 1259, Train Loss: 0.9616, Valid Loss: 1.0078\n",
            "Epoch: 1260, Train Loss: 0.9614, Valid Loss: 1.0062\n",
            "Epoch: 1261, Train Loss: 0.9612, Valid Loss: 1.0077\n",
            "Epoch: 1262, Train Loss: 0.9622, Valid Loss: 1.0070\n",
            "Epoch: 1263, Train Loss: 0.9619, Valid Loss: 1.0067\n",
            "Epoch: 1264, Train Loss: 0.9622, Valid Loss: 1.0059\n",
            "Epoch: 1265, Train Loss: 0.9616, Valid Loss: 1.0064\n",
            "Epoch: 1266, Train Loss: 0.9617, Valid Loss: 1.0071\n",
            "Epoch: 1267, Train Loss: 0.9619, Valid Loss: 1.0075\n",
            "Epoch: 1268, Train Loss: 0.9608, Valid Loss: 1.0060\n",
            "Epoch: 1269, Train Loss: 0.9607, Valid Loss: 1.0073\n",
            "Epoch: 1270, Train Loss: 0.9614, Valid Loss: 1.0062\n",
            "Epoch: 1271, Train Loss: 0.9613, Valid Loss: 1.0073\n",
            "Epoch: 1272, Train Loss: 0.9612, Valid Loss: 1.0061\n",
            "Epoch: 1273, Train Loss: 0.9617, Valid Loss: 1.0061\n",
            "Epoch: 1274, Train Loss: 0.9612, Valid Loss: 1.0065\n",
            "Epoch: 1275, Train Loss: 0.9611, Valid Loss: 1.0069\n",
            "Epoch: 1276, Train Loss: 0.9601, Valid Loss: 1.0065\n",
            "Epoch: 1277, Train Loss: 0.9609, Valid Loss: 1.0065\n",
            "Epoch: 1278, Train Loss: 0.9612, Valid Loss: 1.0060\n",
            "Epoch: 1279, Train Loss: 0.9606, Valid Loss: 1.0062\n",
            "Epoch: 1280, Train Loss: 0.9609, Valid Loss: 1.0064\n",
            "Epoch: 1281, Train Loss: 0.9611, Valid Loss: 1.0062\n",
            "Epoch: 1282, Train Loss: 0.9597, Valid Loss: 1.0060\n",
            "Epoch: 1283, Train Loss: 0.9603, Valid Loss: 1.0046\n",
            "Epoch: 1284, Train Loss: 0.9592, Valid Loss: 1.0067\n",
            "Epoch: 1285, Train Loss: 0.9612, Valid Loss: 1.0062\n",
            "Epoch: 1286, Train Loss: 0.9604, Valid Loss: 1.0055\n",
            "Epoch: 1287, Train Loss: 0.9591, Valid Loss: 1.0044\n",
            "Epoch: 1288, Train Loss: 0.9603, Valid Loss: 1.0050\n",
            "Epoch: 1289, Train Loss: 0.9592, Valid Loss: 1.0060\n",
            "Epoch: 1290, Train Loss: 0.9596, Valid Loss: 1.0058\n",
            "Epoch: 1291, Train Loss: 0.9601, Valid Loss: 1.0046\n",
            "Epoch: 1292, Train Loss: 0.9603, Valid Loss: 1.0050\n",
            "Epoch: 1293, Train Loss: 0.9598, Valid Loss: 1.0048\n",
            "Epoch: 1294, Train Loss: 0.9605, Valid Loss: 1.0055\n",
            "Epoch: 1295, Train Loss: 0.9602, Valid Loss: 1.0042\n",
            "Epoch: 1296, Train Loss: 0.9606, Valid Loss: 1.0055\n",
            "Epoch: 1297, Train Loss: 0.9579, Valid Loss: 1.0049\n",
            "Epoch: 1298, Train Loss: 0.9586, Valid Loss: 1.0052\n",
            "Epoch: 1299, Train Loss: 0.9588, Valid Loss: 1.0049\n",
            "Epoch: 1300, Train Loss: 0.9602, Valid Loss: 1.0047\n",
            "Epoch: 1301, Train Loss: 0.9592, Valid Loss: 1.0037\n",
            "Epoch: 1302, Train Loss: 0.9593, Valid Loss: 1.0050\n",
            "Epoch: 1303, Train Loss: 0.9596, Valid Loss: 1.0048\n",
            "Epoch: 1304, Train Loss: 0.9583, Valid Loss: 1.0036\n",
            "Epoch: 1305, Train Loss: 0.9590, Valid Loss: 1.0054\n",
            "Epoch: 1306, Train Loss: 0.9605, Valid Loss: 1.0029\n",
            "Epoch: 1307, Train Loss: 0.9598, Valid Loss: 1.0042\n",
            "Epoch: 1308, Train Loss: 0.9599, Valid Loss: 1.0054\n",
            "Epoch: 1309, Train Loss: 0.9597, Valid Loss: 1.0029\n",
            "Epoch: 1310, Train Loss: 0.9595, Valid Loss: 1.0040\n",
            "Epoch: 1311, Train Loss: 0.9592, Valid Loss: 1.0041\n",
            "Epoch: 1312, Train Loss: 0.9588, Valid Loss: 1.0044\n",
            "Epoch: 1313, Train Loss: 0.9597, Valid Loss: 1.0031\n",
            "Epoch: 1314, Train Loss: 0.9592, Valid Loss: 1.0038\n",
            "Epoch: 1315, Train Loss: 0.9597, Valid Loss: 1.0034\n",
            "Epoch: 1316, Train Loss: 0.9595, Valid Loss: 1.0044\n",
            "Epoch: 1317, Train Loss: 0.9589, Valid Loss: 1.0043\n",
            "Epoch: 1318, Train Loss: 0.9597, Valid Loss: 1.0029\n",
            "Epoch: 1319, Train Loss: 0.9594, Valid Loss: 1.0035\n",
            "Epoch: 1320, Train Loss: 0.9589, Valid Loss: 1.0040\n",
            "Epoch: 1321, Train Loss: 0.9575, Valid Loss: 1.0037\n",
            "Epoch: 1322, Train Loss: 0.9597, Valid Loss: 1.0027\n",
            "Epoch: 1323, Train Loss: 0.9588, Valid Loss: 1.0032\n",
            "Epoch: 1324, Train Loss: 0.9580, Valid Loss: 1.0033\n",
            "Epoch: 1325, Train Loss: 0.9585, Valid Loss: 1.0035\n",
            "Epoch: 1326, Train Loss: 0.9584, Valid Loss: 1.0036\n",
            "Epoch: 1327, Train Loss: 0.9593, Valid Loss: 1.0019\n",
            "Epoch: 1328, Train Loss: 0.9590, Valid Loss: 1.0039\n",
            "Epoch: 1329, Train Loss: 0.9584, Valid Loss: 1.0048\n",
            "Epoch: 1330, Train Loss: 0.9583, Valid Loss: 1.0028\n",
            "Epoch: 1331, Train Loss: 0.9588, Valid Loss: 1.0039\n",
            "Epoch: 1332, Train Loss: 0.9591, Valid Loss: 1.0020\n",
            "Epoch: 1333, Train Loss: 0.9583, Valid Loss: 1.0023\n",
            "Epoch: 1334, Train Loss: 0.9582, Valid Loss: 1.0022\n",
            "Epoch: 1335, Train Loss: 0.9573, Valid Loss: 1.0031\n",
            "Epoch: 1336, Train Loss: 0.9576, Valid Loss: 1.0032\n",
            "Epoch: 1337, Train Loss: 0.9574, Valid Loss: 1.0026\n",
            "Epoch: 1338, Train Loss: 0.9575, Valid Loss: 1.0031\n",
            "Epoch: 1339, Train Loss: 0.9577, Valid Loss: 1.0034\n",
            "Epoch: 1340, Train Loss: 0.9584, Valid Loss: 1.0033\n",
            "Epoch: 1341, Train Loss: 0.9582, Valid Loss: 1.0007\n",
            "Epoch: 1342, Train Loss: 0.9574, Valid Loss: 1.0023\n",
            "Epoch: 1343, Train Loss: 0.9586, Valid Loss: 1.0037\n",
            "Epoch: 1344, Train Loss: 0.9585, Valid Loss: 1.0030\n",
            "Epoch: 1345, Train Loss: 0.9583, Valid Loss: 1.0017\n",
            "Epoch: 1346, Train Loss: 0.9570, Valid Loss: 1.0023\n",
            "Epoch: 1347, Train Loss: 0.9580, Valid Loss: 1.0030\n",
            "Epoch: 1348, Train Loss: 0.9583, Valid Loss: 1.0022\n",
            "Epoch: 1349, Train Loss: 0.9581, Valid Loss: 1.0022\n",
            "Epoch: 1350, Train Loss: 0.9581, Valid Loss: 1.0011\n",
            "Epoch: 1351, Train Loss: 0.9560, Valid Loss: 1.0023\n",
            "Epoch: 1352, Train Loss: 0.9581, Valid Loss: 1.0029\n",
            "Epoch: 1353, Train Loss: 0.9552, Valid Loss: 1.0017\n",
            "Epoch: 1354, Train Loss: 0.9568, Valid Loss: 1.0022\n",
            "Epoch: 1355, Train Loss: 0.9562, Valid Loss: 1.0009\n",
            "Epoch: 1356, Train Loss: 0.9551, Valid Loss: 1.0019\n",
            "Epoch: 1357, Train Loss: 0.9559, Valid Loss: 1.0023\n",
            "Epoch: 1358, Train Loss: 0.9577, Valid Loss: 1.0020\n",
            "Epoch: 1359, Train Loss: 0.9564, Valid Loss: 1.0023\n",
            "Epoch: 1360, Train Loss: 0.9572, Valid Loss: 1.0006\n",
            "Epoch: 1361, Train Loss: 0.9571, Valid Loss: 1.0003\n",
            "Epoch: 1362, Train Loss: 0.9564, Valid Loss: 1.0019\n",
            "Epoch: 1363, Train Loss: 0.9581, Valid Loss: 1.0016\n",
            "Epoch: 1364, Train Loss: 0.9564, Valid Loss: 1.0002\n",
            "Epoch: 1365, Train Loss: 0.9579, Valid Loss: 1.0003\n",
            "Epoch: 1366, Train Loss: 0.9577, Valid Loss: 1.0034\n",
            "Epoch: 1367, Train Loss: 0.9569, Valid Loss: 1.0014\n",
            "Epoch: 1368, Train Loss: 0.9565, Valid Loss: 1.0007\n",
            "Epoch: 1369, Train Loss: 0.9572, Valid Loss: 1.0012\n",
            "Epoch: 1370, Train Loss: 0.9563, Valid Loss: 1.0008\n",
            "Epoch: 1371, Train Loss: 0.9566, Valid Loss: 1.0024\n",
            "Epoch: 1372, Train Loss: 0.9566, Valid Loss: 0.9991\n",
            "Epoch: 1373, Train Loss: 0.9559, Valid Loss: 0.9998\n",
            "Epoch: 1374, Train Loss: 0.9566, Valid Loss: 1.0020\n",
            "Epoch: 1375, Train Loss: 0.9562, Valid Loss: 1.0019\n",
            "Epoch: 1376, Train Loss: 0.9556, Valid Loss: 1.0007\n",
            "Epoch: 1377, Train Loss: 0.9560, Valid Loss: 1.0001\n",
            "Epoch: 1378, Train Loss: 0.9571, Valid Loss: 1.0014\n",
            "Epoch: 1379, Train Loss: 0.9572, Valid Loss: 1.0002\n",
            "Epoch: 1380, Train Loss: 0.9564, Valid Loss: 1.0007\n",
            "Epoch: 1381, Train Loss: 0.9573, Valid Loss: 1.0006\n",
            "Epoch: 1382, Train Loss: 0.9571, Valid Loss: 1.0005\n",
            "Epoch: 1383, Train Loss: 0.9552, Valid Loss: 1.0008\n",
            "Epoch: 1384, Train Loss: 0.9553, Valid Loss: 0.9999\n",
            "Epoch: 1385, Train Loss: 0.9566, Valid Loss: 1.0017\n",
            "Epoch: 1386, Train Loss: 0.9569, Valid Loss: 1.0014\n",
            "Epoch: 1387, Train Loss: 0.9565, Valid Loss: 1.0006\n",
            "Epoch: 1388, Train Loss: 0.9554, Valid Loss: 0.9992\n",
            "Epoch: 1389, Train Loss: 0.9551, Valid Loss: 1.0015\n",
            "Epoch: 1390, Train Loss: 0.9564, Valid Loss: 0.9991\n",
            "Epoch: 1391, Train Loss: 0.9557, Valid Loss: 0.9998\n",
            "Epoch: 1392, Train Loss: 0.9564, Valid Loss: 1.0000\n",
            "Epoch: 1393, Train Loss: 0.9553, Valid Loss: 1.0012\n",
            "Epoch: 1394, Train Loss: 0.9561, Valid Loss: 1.0009\n",
            "Epoch: 1395, Train Loss: 0.9561, Valid Loss: 0.9991\n",
            "Epoch: 1396, Train Loss: 0.9562, Valid Loss: 1.0000\n",
            "Epoch: 1397, Train Loss: 0.9560, Valid Loss: 1.0003\n",
            "Epoch: 1398, Train Loss: 0.9555, Valid Loss: 0.9996\n",
            "Epoch: 1399, Train Loss: 0.9560, Valid Loss: 0.9995\n",
            "Epoch: 1400, Train Loss: 0.9554, Valid Loss: 1.0009\n",
            "Epoch: 1401, Train Loss: 0.9561, Valid Loss: 1.0000\n",
            "Epoch: 1402, Train Loss: 0.9550, Valid Loss: 0.9978\n",
            "Epoch: 1403, Train Loss: 0.9567, Valid Loss: 0.9992\n",
            "Epoch: 1404, Train Loss: 0.9558, Valid Loss: 1.0016\n",
            "Epoch: 1405, Train Loss: 0.9561, Valid Loss: 0.9993\n",
            "Epoch: 1406, Train Loss: 0.9563, Valid Loss: 1.0001\n",
            "Epoch: 1407, Train Loss: 0.9549, Valid Loss: 0.9991\n",
            "Epoch: 1408, Train Loss: 0.9559, Valid Loss: 1.0010\n",
            "Epoch: 1409, Train Loss: 0.9551, Valid Loss: 0.9994\n",
            "Epoch: 1410, Train Loss: 0.9554, Valid Loss: 0.9986\n",
            "Epoch: 1411, Train Loss: 0.9546, Valid Loss: 0.9999\n",
            "Epoch: 1412, Train Loss: 0.9559, Valid Loss: 0.9996\n",
            "Epoch: 1413, Train Loss: 0.9557, Valid Loss: 0.9991\n",
            "Epoch: 1414, Train Loss: 0.9561, Valid Loss: 0.9988\n",
            "Epoch: 1415, Train Loss: 0.9557, Valid Loss: 1.0000\n",
            "Epoch: 1416, Train Loss: 0.9554, Valid Loss: 0.9983\n",
            "Epoch: 1417, Train Loss: 0.9561, Valid Loss: 1.0003\n",
            "Epoch: 1418, Train Loss: 0.9557, Valid Loss: 1.0008\n",
            "Epoch: 1419, Train Loss: 0.9551, Valid Loss: 0.9971\n",
            "Epoch: 1420, Train Loss: 0.9549, Valid Loss: 0.9991\n",
            "Epoch: 1421, Train Loss: 0.9550, Valid Loss: 0.9998\n",
            "Epoch: 1422, Train Loss: 0.9550, Valid Loss: 0.9989\n",
            "Epoch: 1423, Train Loss: 0.9537, Valid Loss: 0.9991\n",
            "Epoch: 1424, Train Loss: 0.9555, Valid Loss: 1.0003\n",
            "Epoch: 1425, Train Loss: 0.9551, Valid Loss: 0.9981\n",
            "Epoch: 1426, Train Loss: 0.9553, Valid Loss: 0.9989\n",
            "Epoch: 1427, Train Loss: 0.9556, Valid Loss: 0.9990\n",
            "Epoch: 1428, Train Loss: 0.9548, Valid Loss: 0.9983\n",
            "Epoch: 1429, Train Loss: 0.9555, Valid Loss: 0.9987\n",
            "Epoch: 1430, Train Loss: 0.9555, Valid Loss: 0.9994\n",
            "Epoch: 1431, Train Loss: 0.9552, Valid Loss: 0.9992\n",
            "Epoch: 1432, Train Loss: 0.9547, Valid Loss: 0.9976\n",
            "Epoch: 1433, Train Loss: 0.9537, Valid Loss: 0.9989\n",
            "Epoch: 1434, Train Loss: 0.9543, Valid Loss: 0.9992\n",
            "Epoch: 1435, Train Loss: 0.9539, Valid Loss: 0.9983\n",
            "Epoch: 1436, Train Loss: 0.9546, Valid Loss: 0.9978\n",
            "Epoch: 1437, Train Loss: 0.9527, Valid Loss: 0.9983\n",
            "Epoch: 1438, Train Loss: 0.9538, Valid Loss: 0.9983\n",
            "Epoch: 1439, Train Loss: 0.9543, Valid Loss: 0.9984\n",
            "Epoch: 1440, Train Loss: 0.9539, Valid Loss: 0.9999\n",
            "Epoch: 1441, Train Loss: 0.9538, Valid Loss: 0.9975\n",
            "Epoch: 1442, Train Loss: 0.9548, Valid Loss: 0.9971\n",
            "Epoch: 1443, Train Loss: 0.9541, Valid Loss: 0.9984\n",
            "Epoch: 1444, Train Loss: 0.9548, Valid Loss: 0.9992\n",
            "Epoch: 1445, Train Loss: 0.9545, Valid Loss: 0.9985\n",
            "Epoch: 1446, Train Loss: 0.9547, Valid Loss: 0.9967\n",
            "Epoch: 1447, Train Loss: 0.9535, Valid Loss: 0.9980\n",
            "Epoch: 1448, Train Loss: 0.9535, Valid Loss: 0.9981\n",
            "Epoch: 1449, Train Loss: 0.9535, Valid Loss: 0.9976\n",
            "Epoch: 1450, Train Loss: 0.9545, Valid Loss: 0.9975\n",
            "Epoch: 1451, Train Loss: 0.9523, Valid Loss: 0.9980\n",
            "Epoch: 1452, Train Loss: 0.9537, Valid Loss: 0.9981\n",
            "Epoch: 1453, Train Loss: 0.9538, Valid Loss: 0.9972\n",
            "Epoch: 1454, Train Loss: 0.9546, Valid Loss: 0.9978\n",
            "Epoch: 1455, Train Loss: 0.9534, Valid Loss: 0.9981\n",
            "Epoch: 1456, Train Loss: 0.9534, Valid Loss: 0.9980\n",
            "Epoch: 1457, Train Loss: 0.9527, Valid Loss: 0.9980\n",
            "Epoch: 1458, Train Loss: 0.9543, Valid Loss: 0.9978\n",
            "Epoch: 1459, Train Loss: 0.9537, Valid Loss: 0.9976\n",
            "Epoch: 1460, Train Loss: 0.9540, Valid Loss: 0.9970\n",
            "Epoch: 1461, Train Loss: 0.9537, Valid Loss: 0.9973\n",
            "Epoch: 1462, Train Loss: 0.9531, Valid Loss: 0.9973\n",
            "Epoch: 1463, Train Loss: 0.9538, Valid Loss: 0.9970\n",
            "Epoch: 1464, Train Loss: 0.9522, Valid Loss: 0.9970\n",
            "Epoch: 1465, Train Loss: 0.9520, Valid Loss: 0.9984\n",
            "Epoch: 1466, Train Loss: 0.9540, Valid Loss: 0.9970\n",
            "Epoch: 1467, Train Loss: 0.9544, Valid Loss: 0.9991\n",
            "Epoch: 1468, Train Loss: 0.9519, Valid Loss: 0.9966\n",
            "Epoch: 1469, Train Loss: 0.9525, Valid Loss: 0.9975\n",
            "Epoch: 1470, Train Loss: 0.9530, Valid Loss: 0.9981\n",
            "Epoch: 1471, Train Loss: 0.9539, Valid Loss: 0.9976\n",
            "Epoch: 1472, Train Loss: 0.9535, Valid Loss: 0.9972\n",
            "Epoch: 1473, Train Loss: 0.9527, Valid Loss: 0.9962\n",
            "Epoch: 1474, Train Loss: 0.9535, Valid Loss: 0.9967\n",
            "Epoch: 1475, Train Loss: 0.9541, Valid Loss: 0.9965\n",
            "Epoch: 1476, Train Loss: 0.9539, Valid Loss: 0.9979\n",
            "Epoch: 1477, Train Loss: 0.9521, Valid Loss: 0.9965\n",
            "Epoch: 1478, Train Loss: 0.9533, Valid Loss: 0.9964\n",
            "Epoch: 1479, Train Loss: 0.9531, Valid Loss: 0.9970\n",
            "Epoch: 1480, Train Loss: 0.9536, Valid Loss: 0.9956\n",
            "Epoch: 1481, Train Loss: 0.9529, Valid Loss: 0.9966\n",
            "Epoch: 1482, Train Loss: 0.9541, Valid Loss: 0.9974\n",
            "Epoch: 1483, Train Loss: 0.9530, Valid Loss: 0.9963\n",
            "Epoch: 1484, Train Loss: 0.9527, Valid Loss: 0.9969\n",
            "Epoch: 1485, Train Loss: 0.9532, Valid Loss: 0.9958\n",
            "Epoch: 1486, Train Loss: 0.9522, Valid Loss: 0.9954\n",
            "Epoch: 1487, Train Loss: 0.9529, Valid Loss: 0.9970\n",
            "Epoch: 1488, Train Loss: 0.9530, Valid Loss: 0.9962\n",
            "Epoch: 1489, Train Loss: 0.9521, Valid Loss: 0.9971\n",
            "Epoch: 1490, Train Loss: 0.9509, Valid Loss: 0.9965\n",
            "Epoch: 1491, Train Loss: 0.9531, Valid Loss: 0.9975\n",
            "Epoch: 1492, Train Loss: 0.9529, Valid Loss: 0.9952\n",
            "Epoch: 1493, Train Loss: 0.9535, Valid Loss: 0.9957\n",
            "Epoch: 1494, Train Loss: 0.9535, Valid Loss: 0.9960\n",
            "Epoch: 1495, Train Loss: 0.9537, Valid Loss: 0.9984\n",
            "Epoch: 1496, Train Loss: 0.9525, Valid Loss: 0.9955\n",
            "Epoch: 1497, Train Loss: 0.9527, Valid Loss: 0.9960\n",
            "Epoch: 1498, Train Loss: 0.9510, Valid Loss: 0.9968\n",
            "Epoch: 1499, Train Loss: 0.9531, Valid Loss: 0.9957\n",
            "Epoch: 1500, Train Loss: 0.9525, Valid Loss: 0.9960\n",
            "Epoch: 1501, Train Loss: 0.9501, Valid Loss: 0.9960\n",
            "Epoch: 1502, Train Loss: 0.9521, Valid Loss: 0.9965\n",
            "Epoch: 1503, Train Loss: 0.9522, Valid Loss: 0.9965\n",
            "Epoch: 1504, Train Loss: 0.9527, Valid Loss: 0.9962\n",
            "Epoch: 1505, Train Loss: 0.9523, Valid Loss: 0.9959\n",
            "Epoch: 1506, Train Loss: 0.9517, Valid Loss: 0.9951\n",
            "Epoch: 1507, Train Loss: 0.9534, Valid Loss: 0.9952\n",
            "Epoch: 1508, Train Loss: 0.9524, Valid Loss: 0.9966\n",
            "Epoch: 1509, Train Loss: 0.9517, Valid Loss: 0.9954\n",
            "Epoch: 1510, Train Loss: 0.9524, Valid Loss: 0.9965\n",
            "Epoch: 1511, Train Loss: 0.9525, Valid Loss: 0.9961\n",
            "Epoch: 1512, Train Loss: 0.9524, Valid Loss: 0.9957\n",
            "Epoch: 1513, Train Loss: 0.9515, Valid Loss: 0.9963\n",
            "Epoch: 1514, Train Loss: 0.9517, Valid Loss: 0.9961\n",
            "Epoch: 1515, Train Loss: 0.9522, Valid Loss: 0.9953\n",
            "Epoch: 1516, Train Loss: 0.9518, Valid Loss: 0.9950\n",
            "Epoch: 1517, Train Loss: 0.9520, Valid Loss: 0.9950\n",
            "Epoch: 1518, Train Loss: 0.9524, Valid Loss: 0.9953\n",
            "Epoch: 1519, Train Loss: 0.9519, Valid Loss: 0.9951\n",
            "Epoch: 1520, Train Loss: 0.9518, Valid Loss: 0.9964\n",
            "Epoch: 1521, Train Loss: 0.9521, Valid Loss: 0.9957\n",
            "Epoch: 1522, Train Loss: 0.9518, Valid Loss: 0.9953\n",
            "Epoch: 1523, Train Loss: 0.9526, Valid Loss: 0.9952\n",
            "Epoch: 1524, Train Loss: 0.9530, Valid Loss: 0.9962\n",
            "Epoch: 1525, Train Loss: 0.9526, Valid Loss: 0.9954\n",
            "Epoch: 1526, Train Loss: 0.9518, Valid Loss: 0.9944\n",
            "Epoch: 1527, Train Loss: 0.9526, Valid Loss: 0.9957\n",
            "Epoch: 1528, Train Loss: 0.9523, Valid Loss: 0.9949\n",
            "Epoch: 1529, Train Loss: 0.9518, Valid Loss: 0.9954\n",
            "Epoch: 1530, Train Loss: 0.9523, Valid Loss: 0.9952\n",
            "Epoch: 1531, Train Loss: 0.9526, Valid Loss: 0.9957\n",
            "Epoch: 1532, Train Loss: 0.9524, Valid Loss: 0.9963\n",
            "Epoch: 1533, Train Loss: 0.9499, Valid Loss: 0.9950\n",
            "Epoch: 1534, Train Loss: 0.9517, Valid Loss: 0.9942\n",
            "Epoch: 1535, Train Loss: 0.9522, Valid Loss: 0.9949\n",
            "Epoch: 1536, Train Loss: 0.9509, Valid Loss: 0.9953\n",
            "Epoch: 1537, Train Loss: 0.9517, Valid Loss: 0.9953\n",
            "Epoch: 1538, Train Loss: 0.9519, Valid Loss: 0.9943\n",
            "Epoch: 1539, Train Loss: 0.9526, Valid Loss: 0.9956\n",
            "Epoch: 1540, Train Loss: 0.9519, Valid Loss: 0.9949\n",
            "Epoch: 1541, Train Loss: 0.9518, Valid Loss: 0.9948\n",
            "Epoch: 1542, Train Loss: 0.9521, Valid Loss: 0.9944\n",
            "Epoch: 1543, Train Loss: 0.9519, Valid Loss: 0.9952\n",
            "Epoch: 1544, Train Loss: 0.9514, Valid Loss: 0.9952\n",
            "Epoch: 1545, Train Loss: 0.9524, Valid Loss: 0.9953\n",
            "Epoch: 1546, Train Loss: 0.9521, Valid Loss: 0.9960\n",
            "Epoch: 1547, Train Loss: 0.9525, Valid Loss: 0.9942\n",
            "Epoch: 1548, Train Loss: 0.9520, Valid Loss: 0.9949\n",
            "Epoch: 1549, Train Loss: 0.9513, Valid Loss: 0.9940\n",
            "Epoch: 1550, Train Loss: 0.9502, Valid Loss: 0.9943\n",
            "Epoch: 1551, Train Loss: 0.9519, Valid Loss: 0.9950\n",
            "Epoch: 1552, Train Loss: 0.9515, Valid Loss: 0.9949\n",
            "Epoch: 1553, Train Loss: 0.9511, Valid Loss: 0.9939\n",
            "Epoch: 1554, Train Loss: 0.9514, Valid Loss: 0.9946\n",
            "Epoch: 1555, Train Loss: 0.9516, Valid Loss: 0.9943\n",
            "Epoch: 1556, Train Loss: 0.9522, Valid Loss: 0.9943\n",
            "Epoch: 1557, Train Loss: 0.9500, Valid Loss: 0.9957\n",
            "Epoch: 1558, Train Loss: 0.9513, Valid Loss: 0.9957\n",
            "Epoch: 1559, Train Loss: 0.9513, Valid Loss: 0.9937\n",
            "Epoch: 1560, Train Loss: 0.9509, Valid Loss: 0.9932\n",
            "Epoch: 1561, Train Loss: 0.9519, Valid Loss: 0.9936\n",
            "Epoch: 1562, Train Loss: 0.9503, Valid Loss: 0.9943\n",
            "Epoch: 1563, Train Loss: 0.9496, Valid Loss: 0.9950\n",
            "Epoch: 1564, Train Loss: 0.9506, Valid Loss: 0.9951\n",
            "Epoch: 1565, Train Loss: 0.9511, Valid Loss: 0.9937\n",
            "Epoch: 1566, Train Loss: 0.9510, Valid Loss: 0.9947\n",
            "Epoch: 1567, Train Loss: 0.9511, Valid Loss: 0.9932\n",
            "Epoch: 1568, Train Loss: 0.9510, Valid Loss: 0.9947\n",
            "Epoch: 1569, Train Loss: 0.9518, Valid Loss: 0.9947\n",
            "Epoch: 1570, Train Loss: 0.9510, Valid Loss: 0.9928\n",
            "Epoch: 1571, Train Loss: 0.9520, Valid Loss: 0.9928\n",
            "Epoch: 1572, Train Loss: 0.9519, Valid Loss: 0.9952\n",
            "Epoch: 1573, Train Loss: 0.9519, Valid Loss: 0.9942\n",
            "Epoch: 1574, Train Loss: 0.9518, Valid Loss: 0.9939\n",
            "Epoch: 1575, Train Loss: 0.9519, Valid Loss: 0.9930\n",
            "Epoch: 1576, Train Loss: 0.9510, Valid Loss: 0.9933\n",
            "Epoch: 1577, Train Loss: 0.9518, Valid Loss: 0.9939\n",
            "Epoch: 1578, Train Loss: 0.9483, Valid Loss: 0.9943\n",
            "Epoch: 1579, Train Loss: 0.9506, Valid Loss: 0.9932\n",
            "Epoch: 1580, Train Loss: 0.9504, Valid Loss: 0.9918\n",
            "Epoch: 1581, Train Loss: 0.9512, Valid Loss: 0.9945\n",
            "Epoch: 1582, Train Loss: 0.9513, Valid Loss: 0.9950\n",
            "Epoch: 1583, Train Loss: 0.9498, Valid Loss: 0.9922\n",
            "Epoch: 1584, Train Loss: 0.9500, Valid Loss: 0.9927\n",
            "Epoch: 1585, Train Loss: 0.9510, Valid Loss: 0.9933\n",
            "Epoch: 1586, Train Loss: 0.9503, Valid Loss: 0.9935\n",
            "Epoch: 1587, Train Loss: 0.9508, Valid Loss: 0.9948\n",
            "Epoch: 1588, Train Loss: 0.9508, Valid Loss: 0.9946\n",
            "Epoch: 1589, Train Loss: 0.9501, Valid Loss: 0.9932\n",
            "Epoch: 1590, Train Loss: 0.9512, Valid Loss: 0.9926\n",
            "Epoch: 1591, Train Loss: 0.9509, Valid Loss: 0.9927\n",
            "Epoch: 1592, Train Loss: 0.9501, Valid Loss: 0.9940\n",
            "Epoch: 1593, Train Loss: 0.9508, Valid Loss: 0.9942\n",
            "Epoch: 1594, Train Loss: 0.9511, Valid Loss: 0.9925\n",
            "Epoch: 1595, Train Loss: 0.9508, Valid Loss: 0.9942\n",
            "Epoch: 1596, Train Loss: 0.9502, Valid Loss: 0.9917\n",
            "Epoch: 1597, Train Loss: 0.9498, Valid Loss: 0.9933\n",
            "Epoch: 1598, Train Loss: 0.9501, Valid Loss: 0.9937\n",
            "Epoch: 1599, Train Loss: 0.9509, Valid Loss: 0.9930\n",
            "Epoch: 1600, Train Loss: 0.9498, Valid Loss: 0.9923\n",
            "Epoch: 1601, Train Loss: 0.9491, Valid Loss: 0.9936\n",
            "Epoch: 1602, Train Loss: 0.9483, Valid Loss: 0.9942\n",
            "Epoch: 1603, Train Loss: 0.9499, Valid Loss: 0.9918\n",
            "Epoch: 1604, Train Loss: 0.9511, Valid Loss: 0.9918\n",
            "Epoch: 1605, Train Loss: 0.9492, Valid Loss: 0.9943\n",
            "Epoch: 1606, Train Loss: 0.9517, Valid Loss: 0.9923\n",
            "Epoch: 1607, Train Loss: 0.9504, Valid Loss: 0.9934\n",
            "Epoch: 1608, Train Loss: 0.9497, Valid Loss: 0.9931\n",
            "Epoch: 1609, Train Loss: 0.9504, Valid Loss: 0.9921\n",
            "Epoch: 1610, Train Loss: 0.9494, Valid Loss: 0.9923\n",
            "Epoch: 1611, Train Loss: 0.9500, Valid Loss: 0.9925\n",
            "Epoch: 1612, Train Loss: 0.9504, Valid Loss: 0.9932\n",
            "Epoch: 1613, Train Loss: 0.9500, Valid Loss: 0.9923\n",
            "Epoch: 1614, Train Loss: 0.9480, Valid Loss: 0.9921\n",
            "Epoch: 1615, Train Loss: 0.9503, Valid Loss: 0.9933\n",
            "Epoch: 1616, Train Loss: 0.9489, Valid Loss: 0.9913\n",
            "Epoch: 1617, Train Loss: 0.9499, Valid Loss: 0.9918\n",
            "Epoch: 1618, Train Loss: 0.9503, Valid Loss: 0.9930\n",
            "Epoch: 1619, Train Loss: 0.9504, Valid Loss: 0.9915\n",
            "Epoch: 1620, Train Loss: 0.9490, Valid Loss: 0.9926\n",
            "Epoch: 1621, Train Loss: 0.9486, Valid Loss: 0.9928\n",
            "Epoch: 1622, Train Loss: 0.9503, Valid Loss: 0.9920\n",
            "Epoch: 1623, Train Loss: 0.9499, Valid Loss: 0.9922\n",
            "Epoch: 1624, Train Loss: 0.9496, Valid Loss: 0.9923\n",
            "Epoch: 1625, Train Loss: 0.9486, Valid Loss: 0.9917\n",
            "Epoch: 1626, Train Loss: 0.9503, Valid Loss: 0.9921\n",
            "Epoch: 1627, Train Loss: 0.9508, Valid Loss: 0.9928\n",
            "Epoch: 1628, Train Loss: 0.9492, Valid Loss: 0.9918\n",
            "Epoch: 1629, Train Loss: 0.9491, Valid Loss: 0.9919\n",
            "Epoch: 1630, Train Loss: 0.9498, Valid Loss: 0.9947\n",
            "Epoch: 1631, Train Loss: 0.9506, Valid Loss: 0.9911\n",
            "Epoch: 1632, Train Loss: 0.9495, Valid Loss: 0.9913\n",
            "Epoch: 1633, Train Loss: 0.9503, Valid Loss: 0.9917\n",
            "Epoch: 1634, Train Loss: 0.9492, Valid Loss: 0.9926\n",
            "Epoch: 1635, Train Loss: 0.9494, Valid Loss: 0.9914\n",
            "Epoch: 1636, Train Loss: 0.9506, Valid Loss: 0.9907\n",
            "Epoch: 1637, Train Loss: 0.9499, Valid Loss: 0.9938\n",
            "Epoch: 1638, Train Loss: 0.9504, Valid Loss: 0.9913\n",
            "Epoch: 1639, Train Loss: 0.9494, Valid Loss: 0.9916\n",
            "Epoch: 1640, Train Loss: 0.9503, Valid Loss: 0.9913\n",
            "Epoch: 1641, Train Loss: 0.9493, Valid Loss: 0.9922\n",
            "Epoch: 1642, Train Loss: 0.9492, Valid Loss: 0.9923\n",
            "Epoch: 1643, Train Loss: 0.9492, Valid Loss: 0.9917\n",
            "Epoch: 1644, Train Loss: 0.9480, Valid Loss: 0.9907\n",
            "Epoch: 1645, Train Loss: 0.9482, Valid Loss: 0.9920\n",
            "Epoch: 1646, Train Loss: 0.9493, Valid Loss: 0.9917\n",
            "Epoch: 1647, Train Loss: 0.9488, Valid Loss: 0.9918\n",
            "Epoch: 1648, Train Loss: 0.9500, Valid Loss: 0.9918\n",
            "Epoch: 1649, Train Loss: 0.9499, Valid Loss: 0.9922\n",
            "Epoch: 1650, Train Loss: 0.9489, Valid Loss: 0.9915\n",
            "Epoch: 1651, Train Loss: 0.9489, Valid Loss: 0.9902\n",
            "Epoch: 1652, Train Loss: 0.9495, Valid Loss: 0.9927\n",
            "Epoch: 1653, Train Loss: 0.9500, Valid Loss: 0.9914\n",
            "Epoch: 1654, Train Loss: 0.9498, Valid Loss: 0.9905\n",
            "Epoch: 1655, Train Loss: 0.9479, Valid Loss: 0.9924\n",
            "Epoch: 1656, Train Loss: 0.9500, Valid Loss: 0.9903\n",
            "Epoch: 1657, Train Loss: 0.9500, Valid Loss: 0.9915\n",
            "Epoch: 1658, Train Loss: 0.9497, Valid Loss: 0.9924\n",
            "Epoch: 1659, Train Loss: 0.9501, Valid Loss: 0.9922\n",
            "Epoch: 1660, Train Loss: 0.9503, Valid Loss: 0.9901\n",
            "Epoch: 1661, Train Loss: 0.9498, Valid Loss: 0.9911\n",
            "Epoch: 1662, Train Loss: 0.9503, Valid Loss: 0.9902\n",
            "Epoch: 1663, Train Loss: 0.9472, Valid Loss: 0.9921\n",
            "Epoch: 1664, Train Loss: 0.9493, Valid Loss: 0.9906\n",
            "Epoch: 1665, Train Loss: 0.9481, Valid Loss: 0.9915\n",
            "Epoch: 1666, Train Loss: 0.9500, Valid Loss: 0.9909\n",
            "Epoch: 1667, Train Loss: 0.9491, Valid Loss: 0.9912\n",
            "Epoch: 1668, Train Loss: 0.9498, Valid Loss: 0.9907\n",
            "Epoch: 1669, Train Loss: 0.9494, Valid Loss: 0.9904\n",
            "Epoch: 1670, Train Loss: 0.9492, Valid Loss: 0.9919\n",
            "Epoch: 1671, Train Loss: 0.9495, Valid Loss: 0.9908\n",
            "Epoch: 1672, Train Loss: 0.9490, Valid Loss: 0.9905\n",
            "Epoch: 1673, Train Loss: 0.9492, Valid Loss: 0.9909\n",
            "Epoch: 1674, Train Loss: 0.9494, Valid Loss: 0.9905\n",
            "Epoch: 1675, Train Loss: 0.9496, Valid Loss: 0.9901\n",
            "Epoch: 1676, Train Loss: 0.9499, Valid Loss: 0.9920\n",
            "Epoch: 1677, Train Loss: 0.9502, Valid Loss: 0.9894\n",
            "Epoch: 1678, Train Loss: 0.9499, Valid Loss: 0.9918\n",
            "Epoch: 1679, Train Loss: 0.9491, Valid Loss: 0.9902\n",
            "Epoch: 1680, Train Loss: 0.9481, Valid Loss: 0.9906\n",
            "Epoch: 1681, Train Loss: 0.9493, Valid Loss: 0.9912\n",
            "Epoch: 1682, Train Loss: 0.9496, Valid Loss: 0.9909\n",
            "Epoch: 1683, Train Loss: 0.9496, Valid Loss: 0.9901\n",
            "Epoch: 1684, Train Loss: 0.9500, Valid Loss: 0.9898\n",
            "Epoch: 1685, Train Loss: 0.9495, Valid Loss: 0.9905\n",
            "Epoch: 1686, Train Loss: 0.9477, Valid Loss: 0.9914\n",
            "Epoch: 1687, Train Loss: 0.9490, Valid Loss: 0.9906\n",
            "Epoch: 1688, Train Loss: 0.9499, Valid Loss: 0.9903\n",
            "Epoch: 1689, Train Loss: 0.9491, Valid Loss: 0.9902\n",
            "Epoch: 1690, Train Loss: 0.9496, Valid Loss: 0.9907\n",
            "Epoch: 1691, Train Loss: 0.9485, Valid Loss: 0.9896\n",
            "Epoch: 1692, Train Loss: 0.9452, Valid Loss: 0.9900\n",
            "Epoch: 1693, Train Loss: 0.9490, Valid Loss: 0.9894\n",
            "Epoch: 1694, Train Loss: 0.9491, Valid Loss: 0.9918\n",
            "Epoch: 1695, Train Loss: 0.9494, Valid Loss: 0.9894\n",
            "Epoch: 1696, Train Loss: 0.9473, Valid Loss: 0.9903\n",
            "Epoch: 1697, Train Loss: 0.9484, Valid Loss: 0.9903\n",
            "Epoch: 1698, Train Loss: 0.9482, Valid Loss: 0.9901\n",
            "Epoch: 1699, Train Loss: 0.9482, Valid Loss: 0.9897\n",
            "Epoch: 1700, Train Loss: 0.9493, Valid Loss: 0.9905\n",
            "Epoch: 1701, Train Loss: 0.9495, Valid Loss: 0.9900\n",
            "Epoch: 1702, Train Loss: 0.9491, Valid Loss: 0.9897\n",
            "Epoch: 1703, Train Loss: 0.9479, Valid Loss: 0.9902\n",
            "Epoch: 1704, Train Loss: 0.9494, Valid Loss: 0.9908\n",
            "Epoch: 1705, Train Loss: 0.9484, Valid Loss: 0.9902\n",
            "Epoch: 1706, Train Loss: 0.9492, Valid Loss: 0.9900\n",
            "Epoch: 1707, Train Loss: 0.9479, Valid Loss: 0.9904\n",
            "Epoch: 1708, Train Loss: 0.9498, Valid Loss: 0.9892\n",
            "Epoch: 1709, Train Loss: 0.9492, Valid Loss: 0.9899\n",
            "Epoch: 1710, Train Loss: 0.9484, Valid Loss: 0.9908\n",
            "Epoch: 1711, Train Loss: 0.9486, Valid Loss: 0.9901\n",
            "Epoch: 1712, Train Loss: 0.9493, Valid Loss: 0.9887\n",
            "Epoch: 1713, Train Loss: 0.9489, Valid Loss: 0.9885\n",
            "Epoch: 1714, Train Loss: 0.9484, Valid Loss: 0.9900\n",
            "Epoch: 1715, Train Loss: 0.9481, Valid Loss: 0.9913\n",
            "Epoch: 1716, Train Loss: 0.9489, Valid Loss: 0.9884\n",
            "Epoch: 1717, Train Loss: 0.9478, Valid Loss: 0.9889\n",
            "Epoch: 1718, Train Loss: 0.9483, Valid Loss: 0.9896\n",
            "Epoch: 1719, Train Loss: 0.9486, Valid Loss: 0.9901\n",
            "Epoch: 1720, Train Loss: 0.9469, Valid Loss: 0.9904\n",
            "Epoch: 1721, Train Loss: 0.9482, Valid Loss: 0.9884\n",
            "Epoch: 1722, Train Loss: 0.9492, Valid Loss: 0.9904\n",
            "Epoch: 1723, Train Loss: 0.9492, Valid Loss: 0.9883\n",
            "Epoch: 1724, Train Loss: 0.9480, Valid Loss: 0.9905\n",
            "Epoch: 1725, Train Loss: 0.9492, Valid Loss: 0.9894\n",
            "Epoch: 1726, Train Loss: 0.9488, Valid Loss: 0.9895\n",
            "Epoch: 1727, Train Loss: 0.9484, Valid Loss: 0.9890\n",
            "Epoch: 1728, Train Loss: 0.9486, Valid Loss: 0.9904\n",
            "Epoch: 1729, Train Loss: 0.9474, Valid Loss: 0.9894\n",
            "Epoch: 1730, Train Loss: 0.9491, Valid Loss: 0.9895\n",
            "Epoch: 1731, Train Loss: 0.9472, Valid Loss: 0.9887\n",
            "Epoch: 1732, Train Loss: 0.9466, Valid Loss: 0.9889\n",
            "Epoch: 1733, Train Loss: 0.9488, Valid Loss: 0.9892\n",
            "Epoch: 1734, Train Loss: 0.9477, Valid Loss: 0.9901\n",
            "Epoch: 1735, Train Loss: 0.9487, Valid Loss: 0.9879\n",
            "Epoch: 1736, Train Loss: 0.9475, Valid Loss: 0.9887\n",
            "Epoch: 1737, Train Loss: 0.9482, Valid Loss: 0.9890\n",
            "Epoch: 1738, Train Loss: 0.9491, Valid Loss: 0.9892\n",
            "Epoch: 1739, Train Loss: 0.9487, Valid Loss: 0.9900\n",
            "Epoch: 1740, Train Loss: 0.9482, Valid Loss: 0.9890\n",
            "Epoch: 1741, Train Loss: 0.9484, Valid Loss: 0.9876\n",
            "Epoch: 1742, Train Loss: 0.9486, Valid Loss: 0.9892\n",
            "Epoch: 1743, Train Loss: 0.9482, Valid Loss: 0.9898\n",
            "Epoch: 1744, Train Loss: 0.9487, Valid Loss: 0.9897\n",
            "Epoch: 1745, Train Loss: 0.9481, Valid Loss: 0.9894\n",
            "Epoch: 1746, Train Loss: 0.9475, Valid Loss: 0.9868\n",
            "Epoch: 1747, Train Loss: 0.9485, Valid Loss: 0.9868\n",
            "Epoch: 1748, Train Loss: 0.9489, Valid Loss: 0.9898\n",
            "Epoch: 1749, Train Loss: 0.9487, Valid Loss: 0.9908\n",
            "Epoch: 1750, Train Loss: 0.9475, Valid Loss: 0.9880\n",
            "Epoch: 1751, Train Loss: 0.9488, Valid Loss: 0.9885\n",
            "Epoch: 1752, Train Loss: 0.9485, Valid Loss: 0.9885\n",
            "Epoch: 1753, Train Loss: 0.9481, Valid Loss: 0.9888\n",
            "Epoch: 1754, Train Loss: 0.9488, Valid Loss: 0.9887\n",
            "Epoch: 1755, Train Loss: 0.9477, Valid Loss: 0.9879\n",
            "Epoch: 1756, Train Loss: 0.9472, Valid Loss: 0.9896\n",
            "Epoch: 1757, Train Loss: 0.9481, Valid Loss: 0.9883\n",
            "Epoch: 1758, Train Loss: 0.9469, Valid Loss: 0.9889\n",
            "Epoch: 1759, Train Loss: 0.9481, Valid Loss: 0.9892\n",
            "Epoch: 1760, Train Loss: 0.9488, Valid Loss: 0.9872\n",
            "Epoch: 1761, Train Loss: 0.9463, Valid Loss: 0.9891\n",
            "Epoch: 1762, Train Loss: 0.9471, Valid Loss: 0.9884\n",
            "Epoch: 1763, Train Loss: 0.9491, Valid Loss: 0.9879\n",
            "Epoch: 1764, Train Loss: 0.9472, Valid Loss: 0.9884\n",
            "Epoch: 1765, Train Loss: 0.9489, Valid Loss: 0.9888\n",
            "Epoch: 1766, Train Loss: 0.9466, Valid Loss: 0.9891\n",
            "Epoch: 1767, Train Loss: 0.9476, Valid Loss: 0.9875\n",
            "Epoch: 1768, Train Loss: 0.9482, Valid Loss: 0.9885\n",
            "Epoch: 1769, Train Loss: 0.9475, Valid Loss: 0.9881\n",
            "Epoch: 1770, Train Loss: 0.9484, Valid Loss: 0.9869\n",
            "Epoch: 1771, Train Loss: 0.9473, Valid Loss: 0.9897\n",
            "Epoch: 1772, Train Loss: 0.9477, Valid Loss: 0.9896\n",
            "Epoch: 1773, Train Loss: 0.9473, Valid Loss: 0.9891\n",
            "Epoch: 1774, Train Loss: 0.9464, Valid Loss: 0.9884\n",
            "Epoch: 1775, Train Loss: 0.9480, Valid Loss: 0.9869\n",
            "Epoch: 1776, Train Loss: 0.9482, Valid Loss: 0.9871\n",
            "Epoch: 1777, Train Loss: 0.9462, Valid Loss: 0.9882\n",
            "Epoch: 1778, Train Loss: 0.9461, Valid Loss: 0.9877\n",
            "Epoch: 1779, Train Loss: 0.9468, Valid Loss: 0.9886\n",
            "Epoch: 1780, Train Loss: 0.9486, Valid Loss: 0.9879\n",
            "Epoch: 1781, Train Loss: 0.9481, Valid Loss: 0.9874\n",
            "Epoch: 1782, Train Loss: 0.9475, Valid Loss: 0.9888\n",
            "Epoch: 1783, Train Loss: 0.9481, Valid Loss: 0.9881\n",
            "Epoch: 1784, Train Loss: 0.9474, Valid Loss: 0.9874\n",
            "Epoch: 1785, Train Loss: 0.9447, Valid Loss: 0.9884\n",
            "Epoch: 1786, Train Loss: 0.9478, Valid Loss: 0.9884\n",
            "Epoch: 1787, Train Loss: 0.9478, Valid Loss: 0.9876\n",
            "Epoch: 1788, Train Loss: 0.9471, Valid Loss: 0.9874\n",
            "Epoch: 1789, Train Loss: 0.9461, Valid Loss: 0.9870\n",
            "Epoch: 1790, Train Loss: 0.9489, Valid Loss: 0.9900\n",
            "Epoch: 1791, Train Loss: 0.9483, Valid Loss: 0.9863\n",
            "Epoch: 1792, Train Loss: 0.9469, Valid Loss: 0.9881\n",
            "Epoch: 1793, Train Loss: 0.9465, Valid Loss: 0.9876\n",
            "Epoch: 1794, Train Loss: 0.9474, Valid Loss: 0.9878\n",
            "Epoch: 1795, Train Loss: 0.9479, Valid Loss: 0.9879\n",
            "Epoch: 1796, Train Loss: 0.9479, Valid Loss: 0.9875\n",
            "Epoch: 1797, Train Loss: 0.9476, Valid Loss: 0.9875\n",
            "Epoch: 1798, Train Loss: 0.9477, Valid Loss: 0.9866\n",
            "Epoch: 1799, Train Loss: 0.9479, Valid Loss: 0.9862\n",
            "Epoch: 1800, Train Loss: 0.9470, Valid Loss: 0.9888\n",
            "Epoch: 1801, Train Loss: 0.9442, Valid Loss: 0.9883\n",
            "Epoch: 1802, Train Loss: 0.9478, Valid Loss: 0.9870\n",
            "Epoch: 1803, Train Loss: 0.9457, Valid Loss: 0.9874\n",
            "Epoch: 1804, Train Loss: 0.9465, Valid Loss: 0.9877\n",
            "Epoch: 1805, Train Loss: 0.9472, Valid Loss: 0.9877\n",
            "Epoch: 1806, Train Loss: 0.9480, Valid Loss: 0.9875\n",
            "Epoch: 1807, Train Loss: 0.9477, Valid Loss: 0.9866\n",
            "Epoch: 1808, Train Loss: 0.9476, Valid Loss: 0.9875\n",
            "Epoch: 1809, Train Loss: 0.9473, Valid Loss: 0.9867\n",
            "Epoch: 1810, Train Loss: 0.9467, Valid Loss: 0.9883\n",
            "Epoch: 1811, Train Loss: 0.9475, Valid Loss: 0.9876\n",
            "Epoch: 1812, Train Loss: 0.9473, Valid Loss: 0.9868\n",
            "Epoch: 1813, Train Loss: 0.9462, Valid Loss: 0.9886\n",
            "Epoch: 1814, Train Loss: 0.9471, Valid Loss: 0.9868\n",
            "Epoch: 1815, Train Loss: 0.9468, Valid Loss: 0.9874\n",
            "Epoch: 1816, Train Loss: 0.9465, Valid Loss: 0.9878\n",
            "Epoch: 1817, Train Loss: 0.9470, Valid Loss: 0.9863\n",
            "Epoch: 1818, Train Loss: 0.9471, Valid Loss: 0.9879\n",
            "Epoch: 1819, Train Loss: 0.9477, Valid Loss: 0.9870\n",
            "Epoch: 1820, Train Loss: 0.9480, Valid Loss: 0.9892\n",
            "Epoch: 1821, Train Loss: 0.9481, Valid Loss: 0.9868\n",
            "Epoch: 1822, Train Loss: 0.9464, Valid Loss: 0.9859\n",
            "Epoch: 1823, Train Loss: 0.9478, Valid Loss: 0.9876\n",
            "Epoch: 1824, Train Loss: 0.9465, Valid Loss: 0.9876\n",
            "Epoch: 1825, Train Loss: 0.9471, Valid Loss: 0.9863\n",
            "Epoch: 1826, Train Loss: 0.9480, Valid Loss: 0.9868\n",
            "Epoch: 1827, Train Loss: 0.9468, Valid Loss: 0.9867\n",
            "Epoch: 1828, Train Loss: 0.9482, Valid Loss: 0.9878\n",
            "Epoch: 1829, Train Loss: 0.9479, Valid Loss: 0.9872\n",
            "Epoch: 1830, Train Loss: 0.9472, Valid Loss: 0.9871\n",
            "Epoch: 1831, Train Loss: 0.9471, Valid Loss: 0.9867\n",
            "Epoch: 1832, Train Loss: 0.9480, Valid Loss: 0.9867\n",
            "Epoch: 1833, Train Loss: 0.9480, Valid Loss: 0.9866\n",
            "Epoch: 1834, Train Loss: 0.9476, Valid Loss: 0.9881\n",
            "Epoch: 1835, Train Loss: 0.9468, Valid Loss: 0.9855\n",
            "Epoch: 1836, Train Loss: 0.9476, Valid Loss: 0.9859\n",
            "Epoch: 1837, Train Loss: 0.9481, Valid Loss: 0.9897\n",
            "Epoch: 1838, Train Loss: 0.9475, Valid Loss: 0.9875\n",
            "Epoch: 1839, Train Loss: 0.9470, Valid Loss: 0.9862\n",
            "Epoch: 1840, Train Loss: 0.9475, Valid Loss: 0.9848\n",
            "Epoch: 1841, Train Loss: 0.9474, Valid Loss: 0.9870\n",
            "Epoch: 1842, Train Loss: 0.9469, Valid Loss: 0.9869\n",
            "Epoch: 1843, Train Loss: 0.9473, Valid Loss: 0.9873\n",
            "Epoch: 1844, Train Loss: 0.9478, Valid Loss: 0.9857\n",
            "Epoch: 1845, Train Loss: 0.9461, Valid Loss: 0.9859\n",
            "Epoch: 1846, Train Loss: 0.9469, Valid Loss: 0.9873\n",
            "Epoch: 1847, Train Loss: 0.9453, Valid Loss: 0.9877\n",
            "Epoch: 1848, Train Loss: 0.9462, Valid Loss: 0.9862\n",
            "Epoch: 1849, Train Loss: 0.9451, Valid Loss: 0.9865\n",
            "Epoch: 1850, Train Loss: 0.9460, Valid Loss: 0.9868\n",
            "Epoch: 1851, Train Loss: 0.9478, Valid Loss: 0.9882\n",
            "Epoch: 1852, Train Loss: 0.9440, Valid Loss: 0.9851\n",
            "Epoch: 1853, Train Loss: 0.9475, Valid Loss: 0.9858\n",
            "Epoch: 1854, Train Loss: 0.9476, Valid Loss: 0.9861\n",
            "Epoch: 1855, Train Loss: 0.9475, Valid Loss: 0.9876\n",
            "Epoch: 1856, Train Loss: 0.9468, Valid Loss: 0.9862\n",
            "Epoch: 1857, Train Loss: 0.9476, Valid Loss: 0.9873\n",
            "Epoch: 1858, Train Loss: 0.9463, Valid Loss: 0.9865\n",
            "Epoch: 1859, Train Loss: 0.9477, Valid Loss: 0.9861\n",
            "Epoch: 1860, Train Loss: 0.9477, Valid Loss: 0.9867\n",
            "Epoch: 1861, Train Loss: 0.9474, Valid Loss: 0.9858\n",
            "Epoch: 1862, Train Loss: 0.9469, Valid Loss: 0.9857\n",
            "Epoch: 1863, Train Loss: 0.9472, Valid Loss: 0.9864\n",
            "Epoch: 1864, Train Loss: 0.9460, Valid Loss: 0.9866\n",
            "Epoch: 1865, Train Loss: 0.9458, Valid Loss: 0.9873\n",
            "Epoch: 1866, Train Loss: 0.9475, Valid Loss: 0.9861\n",
            "Epoch: 1867, Train Loss: 0.9461, Valid Loss: 0.9864\n",
            "Epoch: 1868, Train Loss: 0.9474, Valid Loss: 0.9863\n",
            "Epoch: 1869, Train Loss: 0.9468, Valid Loss: 0.9856\n",
            "Epoch: 1870, Train Loss: 0.9467, Valid Loss: 0.9856\n",
            "Epoch: 1871, Train Loss: 0.9453, Valid Loss: 0.9868\n",
            "Epoch: 1872, Train Loss: 0.9463, Valid Loss: 0.9870\n",
            "Epoch: 1873, Train Loss: 0.9455, Valid Loss: 0.9857\n",
            "Epoch: 1874, Train Loss: 0.9473, Valid Loss: 0.9853\n",
            "Epoch: 1875, Train Loss: 0.9458, Valid Loss: 0.9856\n",
            "Epoch: 1876, Train Loss: 0.9467, Valid Loss: 0.9860\n",
            "Epoch: 1877, Train Loss: 0.9471, Valid Loss: 0.9861\n",
            "Epoch: 1878, Train Loss: 0.9471, Valid Loss: 0.9856\n",
            "Epoch: 1879, Train Loss: 0.9480, Valid Loss: 0.9872\n",
            "Epoch: 1880, Train Loss: 0.9452, Valid Loss: 0.9853\n",
            "Epoch: 1881, Train Loss: 0.9465, Valid Loss: 0.9851\n",
            "Epoch: 1882, Train Loss: 0.9471, Valid Loss: 0.9846\n",
            "Epoch: 1883, Train Loss: 0.9466, Valid Loss: 0.9867\n",
            "Epoch: 1884, Train Loss: 0.9458, Valid Loss: 0.9875\n",
            "Epoch: 1885, Train Loss: 0.9469, Valid Loss: 0.9844\n",
            "Epoch: 1886, Train Loss: 0.9470, Valid Loss: 0.9851\n",
            "Epoch: 1887, Train Loss: 0.9467, Valid Loss: 0.9853\n",
            "Epoch: 1888, Train Loss: 0.9478, Valid Loss: 0.9853\n",
            "Epoch: 1889, Train Loss: 0.9470, Valid Loss: 0.9855\n",
            "Epoch: 1890, Train Loss: 0.9466, Valid Loss: 0.9851\n",
            "Epoch: 1891, Train Loss: 0.9470, Valid Loss: 0.9858\n",
            "Epoch: 1892, Train Loss: 0.9468, Valid Loss: 0.9858\n",
            "Epoch: 1893, Train Loss: 0.9468, Valid Loss: 0.9862\n",
            "Epoch: 1894, Train Loss: 0.9461, Valid Loss: 0.9852\n",
            "Epoch: 1895, Train Loss: 0.9468, Valid Loss: 0.9840\n",
            "Epoch: 1896, Train Loss: 0.9472, Valid Loss: 0.9861\n",
            "Epoch: 1897, Train Loss: 0.9475, Valid Loss: 0.9862\n",
            "Epoch: 1898, Train Loss: 0.9470, Valid Loss: 0.9859\n",
            "Epoch: 1899, Train Loss: 0.9468, Valid Loss: 0.9836\n",
            "Epoch: 1900, Train Loss: 0.9477, Valid Loss: 0.9858\n",
            "Epoch: 1901, Train Loss: 0.9471, Valid Loss: 0.9859\n",
            "Epoch: 1902, Train Loss: 0.9462, Valid Loss: 0.9855\n",
            "Epoch: 1903, Train Loss: 0.9465, Valid Loss: 0.9849\n",
            "Epoch: 1904, Train Loss: 0.9467, Valid Loss: 0.9865\n",
            "Epoch: 1905, Train Loss: 0.9473, Valid Loss: 0.9847\n",
            "Epoch: 1906, Train Loss: 0.9469, Valid Loss: 0.9853\n",
            "Epoch: 1907, Train Loss: 0.9464, Valid Loss: 0.9857\n",
            "Epoch: 1908, Train Loss: 0.9468, Valid Loss: 0.9848\n",
            "Epoch: 1909, Train Loss: 0.9462, Valid Loss: 0.9854\n",
            "Epoch: 1910, Train Loss: 0.9466, Valid Loss: 0.9852\n",
            "Epoch: 1911, Train Loss: 0.9476, Valid Loss: 0.9847\n",
            "Epoch: 1912, Train Loss: 0.9465, Valid Loss: 0.9868\n",
            "Epoch: 1913, Train Loss: 0.9464, Valid Loss: 0.9856\n",
            "Epoch: 1914, Train Loss: 0.9462, Valid Loss: 0.9841\n",
            "Epoch: 1915, Train Loss: 0.9449, Valid Loss: 0.9844\n",
            "Epoch: 1916, Train Loss: 0.9461, Valid Loss: 0.9854\n",
            "Epoch: 1917, Train Loss: 0.9469, Valid Loss: 0.9844\n",
            "Epoch: 1918, Train Loss: 0.9457, Valid Loss: 0.9854\n",
            "Epoch: 1919, Train Loss: 0.9456, Valid Loss: 0.9852\n",
            "Epoch: 1920, Train Loss: 0.9455, Valid Loss: 0.9853\n",
            "Epoch: 1921, Train Loss: 0.9470, Valid Loss: 0.9846\n",
            "Epoch: 1922, Train Loss: 0.9469, Valid Loss: 0.9851\n",
            "Epoch: 1923, Train Loss: 0.9470, Valid Loss: 0.9850\n",
            "Epoch: 1924, Train Loss: 0.9456, Valid Loss: 0.9850\n",
            "Epoch: 1925, Train Loss: 0.9462, Valid Loss: 0.9851\n",
            "Epoch: 1926, Train Loss: 0.9465, Valid Loss: 0.9854\n",
            "Epoch: 1927, Train Loss: 0.9464, Valid Loss: 0.9849\n",
            "Epoch: 1928, Train Loss: 0.9460, Valid Loss: 0.9859\n",
            "Epoch: 1929, Train Loss: 0.9471, Valid Loss: 0.9847\n",
            "Epoch: 1930, Train Loss: 0.9462, Valid Loss: 0.9846\n",
            "Epoch: 1931, Train Loss: 0.9460, Valid Loss: 0.9855\n",
            "Epoch: 1932, Train Loss: 0.9472, Valid Loss: 0.9853\n",
            "Epoch: 1933, Train Loss: 0.9461, Valid Loss: 0.9833\n",
            "Epoch: 1934, Train Loss: 0.9454, Valid Loss: 0.9846\n",
            "Epoch: 1935, Train Loss: 0.9462, Valid Loss: 0.9851\n",
            "Epoch: 1936, Train Loss: 0.9452, Valid Loss: 0.9850\n",
            "Epoch: 1937, Train Loss: 0.9462, Valid Loss: 0.9833\n",
            "Epoch: 1938, Train Loss: 0.9462, Valid Loss: 0.9851\n",
            "Epoch: 1939, Train Loss: 0.9459, Valid Loss: 0.9858\n",
            "Epoch: 1940, Train Loss: 0.9455, Valid Loss: 0.9840\n",
            "Epoch: 1941, Train Loss: 0.9467, Valid Loss: 0.9833\n",
            "Epoch: 1942, Train Loss: 0.9445, Valid Loss: 0.9843\n",
            "Epoch: 1943, Train Loss: 0.9468, Valid Loss: 0.9864\n",
            "Epoch: 1944, Train Loss: 0.9452, Valid Loss: 0.9843\n",
            "Epoch: 1945, Train Loss: 0.9472, Valid Loss: 0.9844\n",
            "Epoch: 1946, Train Loss: 0.9464, Valid Loss: 0.9840\n",
            "Epoch: 1947, Train Loss: 0.9459, Valid Loss: 0.9837\n",
            "Epoch: 1948, Train Loss: 0.9459, Valid Loss: 0.9849\n",
            "Epoch: 1949, Train Loss: 0.9472, Valid Loss: 0.9854\n",
            "Epoch: 1950, Train Loss: 0.9474, Valid Loss: 0.9842\n",
            "Epoch: 1951, Train Loss: 0.9465, Valid Loss: 0.9853\n",
            "Epoch: 1952, Train Loss: 0.9470, Valid Loss: 0.9856\n",
            "Epoch: 1953, Train Loss: 0.9470, Valid Loss: 0.9825\n",
            "Epoch: 1954, Train Loss: 0.9455, Valid Loss: 0.9836\n",
            "Epoch: 1955, Train Loss: 0.9451, Valid Loss: 0.9867\n",
            "Epoch: 1956, Train Loss: 0.9456, Valid Loss: 0.9842\n",
            "Epoch: 1957, Train Loss: 0.9464, Valid Loss: 0.9840\n",
            "Epoch: 1958, Train Loss: 0.9462, Valid Loss: 0.9843\n",
            "Epoch: 1959, Train Loss: 0.9458, Valid Loss: 0.9838\n",
            "Epoch: 1960, Train Loss: 0.9460, Valid Loss: 0.9836\n",
            "Epoch: 1961, Train Loss: 0.9464, Valid Loss: 0.9846\n",
            "Epoch: 1962, Train Loss: 0.9452, Valid Loss: 0.9838\n",
            "Epoch: 1963, Train Loss: 0.9468, Valid Loss: 0.9838\n",
            "Epoch: 1964, Train Loss: 0.9458, Valid Loss: 0.9841\n",
            "Epoch: 1965, Train Loss: 0.9453, Valid Loss: 0.9832\n",
            "Epoch: 1966, Train Loss: 0.9466, Valid Loss: 0.9851\n",
            "Epoch: 1967, Train Loss: 0.9474, Valid Loss: 0.9840\n",
            "Epoch: 1968, Train Loss: 0.9468, Valid Loss: 0.9828\n",
            "Epoch: 1969, Train Loss: 0.9460, Valid Loss: 0.9838\n",
            "Epoch: 1970, Train Loss: 0.9458, Valid Loss: 0.9852\n",
            "Epoch: 1971, Train Loss: 0.9459, Valid Loss: 0.9842\n",
            "Epoch: 1972, Train Loss: 0.9452, Valid Loss: 0.9843\n",
            "Epoch: 1973, Train Loss: 0.9456, Valid Loss: 0.9834\n",
            "Epoch: 1974, Train Loss: 0.9467, Valid Loss: 0.9834\n",
            "Epoch: 1975, Train Loss: 0.9457, Valid Loss: 0.9849\n",
            "Epoch: 1976, Train Loss: 0.9462, Valid Loss: 0.9843\n",
            "Epoch: 1977, Train Loss: 0.9452, Valid Loss: 0.9834\n",
            "Epoch: 1978, Train Loss: 0.9467, Valid Loss: 0.9816\n",
            "Epoch: 1979, Train Loss: 0.9453, Valid Loss: 0.9849\n",
            "Epoch: 1980, Train Loss: 0.9465, Valid Loss: 0.9858\n",
            "Epoch: 1981, Train Loss: 0.9448, Valid Loss: 0.9832\n",
            "Epoch: 1982, Train Loss: 0.9467, Valid Loss: 0.9840\n",
            "Epoch: 1983, Train Loss: 0.9466, Valid Loss: 0.9828\n",
            "Epoch: 1984, Train Loss: 0.9462, Valid Loss: 0.9837\n",
            "Epoch: 1985, Train Loss: 0.9455, Valid Loss: 0.9834\n",
            "Epoch: 1986, Train Loss: 0.9452, Valid Loss: 0.9841\n",
            "Epoch: 1987, Train Loss: 0.9469, Valid Loss: 0.9824\n",
            "Epoch: 1988, Train Loss: 0.9452, Valid Loss: 0.9845\n",
            "Epoch: 1989, Train Loss: 0.9452, Valid Loss: 0.9836\n",
            "Epoch: 1990, Train Loss: 0.9461, Valid Loss: 0.9844\n",
            "Epoch: 1991, Train Loss: 0.9462, Valid Loss: 0.9825\n",
            "Epoch: 1992, Train Loss: 0.9456, Valid Loss: 0.9831\n",
            "Epoch: 1993, Train Loss: 0.9456, Valid Loss: 0.9845\n",
            "Epoch: 1994, Train Loss: 0.9458, Valid Loss: 0.9850\n",
            "Epoch: 1995, Train Loss: 0.9453, Valid Loss: 0.9826\n",
            "Epoch: 1996, Train Loss: 0.9470, Valid Loss: 0.9840\n",
            "Epoch: 1997, Train Loss: 0.9466, Valid Loss: 0.9832\n",
            "Epoch: 1998, Train Loss: 0.9467, Valid Loss: 0.9824\n",
            "Epoch: 1999, Train Loss: 0.9455, Valid Loss: 0.9836\n",
            "Epoch: 2000, Train Loss: 0.9463, Valid Loss: 0.9834\n",
            "Epoch: 2001, Train Loss: 0.9461, Valid Loss: 0.9836\n",
            "Epoch: 2002, Train Loss: 0.9469, Valid Loss: 0.9828\n",
            "Epoch: 2003, Train Loss: 0.9455, Valid Loss: 0.9831\n",
            "Epoch: 2004, Train Loss: 0.9471, Valid Loss: 0.9843\n",
            "Epoch: 2005, Train Loss: 0.9466, Valid Loss: 0.9824\n",
            "Epoch: 2006, Train Loss: 0.9464, Valid Loss: 0.9834\n",
            "Epoch: 2007, Train Loss: 0.9467, Valid Loss: 0.9829\n",
            "Epoch: 2008, Train Loss: 0.9453, Valid Loss: 0.9834\n",
            "Epoch: 2009, Train Loss: 0.9447, Valid Loss: 0.9828\n",
            "Epoch: 2010, Train Loss: 0.9462, Valid Loss: 0.9827\n",
            "Epoch: 2011, Train Loss: 0.9465, Valid Loss: 0.9825\n",
            "Epoch: 2012, Train Loss: 0.9470, Valid Loss: 0.9851\n",
            "Epoch: 2013, Train Loss: 0.9452, Valid Loss: 0.9843\n",
            "Epoch: 2014, Train Loss: 0.9469, Valid Loss: 0.9821\n",
            "Epoch: 2015, Train Loss: 0.9461, Valid Loss: 0.9820\n",
            "Epoch: 2016, Train Loss: 0.9457, Valid Loss: 0.9837\n",
            "Epoch: 2017, Train Loss: 0.9466, Valid Loss: 0.9841\n",
            "Epoch: 2018, Train Loss: 0.9461, Valid Loss: 0.9824\n",
            "Epoch: 2019, Train Loss: 0.9456, Valid Loss: 0.9826\n",
            "Epoch: 2020, Train Loss: 0.9455, Valid Loss: 0.9824\n",
            "Epoch: 2021, Train Loss: 0.9448, Valid Loss: 0.9834\n",
            "Epoch: 2022, Train Loss: 0.9452, Valid Loss: 0.9844\n",
            "Epoch: 2023, Train Loss: 0.9455, Valid Loss: 0.9831\n",
            "Epoch: 2024, Train Loss: 0.9451, Valid Loss: 0.9823\n",
            "Epoch: 2025, Train Loss: 0.9457, Valid Loss: 0.9841\n",
            "Epoch: 2026, Train Loss: 0.9453, Valid Loss: 0.9836\n",
            "Epoch: 2027, Train Loss: 0.9459, Valid Loss: 0.9823\n",
            "Epoch: 2028, Train Loss: 0.9455, Valid Loss: 0.9826\n",
            "Epoch: 2029, Train Loss: 0.9461, Valid Loss: 0.9821\n",
            "Epoch: 2030, Train Loss: 0.9457, Valid Loss: 0.9832\n",
            "Epoch: 2031, Train Loss: 0.9459, Valid Loss: 0.9821\n",
            "Epoch: 2032, Train Loss: 0.9451, Valid Loss: 0.9822\n",
            "Epoch: 2033, Train Loss: 0.9451, Valid Loss: 0.9832\n",
            "Epoch: 2034, Train Loss: 0.9453, Valid Loss: 0.9843\n",
            "Epoch: 2035, Train Loss: 0.9448, Valid Loss: 0.9812\n",
            "Epoch: 2036, Train Loss: 0.9461, Valid Loss: 0.9823\n",
            "Epoch: 2037, Train Loss: 0.9464, Valid Loss: 0.9842\n",
            "Epoch: 2038, Train Loss: 0.9464, Valid Loss: 0.9836\n",
            "Epoch: 2039, Train Loss: 0.9462, Valid Loss: 0.9820\n",
            "Epoch: 2040, Train Loss: 0.9448, Valid Loss: 0.9811\n",
            "Epoch: 2041, Train Loss: 0.9459, Valid Loss: 0.9852\n",
            "Epoch: 2042, Train Loss: 0.9458, Valid Loss: 0.9840\n",
            "Epoch: 2043, Train Loss: 0.9452, Valid Loss: 0.9813\n",
            "Epoch: 2044, Train Loss: 0.9464, Valid Loss: 0.9808\n",
            "Epoch: 2045, Train Loss: 0.9450, Valid Loss: 0.9819\n",
            "Epoch: 2046, Train Loss: 0.9454, Valid Loss: 0.9841\n",
            "Epoch: 2047, Train Loss: 0.9463, Valid Loss: 0.9824\n",
            "Epoch: 2048, Train Loss: 0.9460, Valid Loss: 0.9831\n",
            "Epoch: 2049, Train Loss: 0.9458, Valid Loss: 0.9821\n",
            "Epoch: 2050, Train Loss: 0.9463, Valid Loss: 0.9815\n",
            "Epoch: 2051, Train Loss: 0.9456, Valid Loss: 0.9824\n",
            "Epoch: 2052, Train Loss: 0.9469, Valid Loss: 0.9839\n",
            "Epoch: 2053, Train Loss: 0.9459, Valid Loss: 0.9822\n",
            "Epoch: 2054, Train Loss: 0.9450, Valid Loss: 0.9824\n",
            "Epoch: 2055, Train Loss: 0.9463, Valid Loss: 0.9828\n",
            "Epoch: 2056, Train Loss: 0.9463, Valid Loss: 0.9815\n",
            "Epoch: 2057, Train Loss: 0.9450, Valid Loss: 0.9830\n",
            "Epoch: 2058, Train Loss: 0.9455, Valid Loss: 0.9823\n",
            "Epoch: 2059, Train Loss: 0.9455, Valid Loss: 0.9842\n",
            "Epoch: 2060, Train Loss: 0.9457, Valid Loss: 0.9817\n",
            "Epoch: 2061, Train Loss: 0.9459, Valid Loss: 0.9812\n",
            "Epoch: 2062, Train Loss: 0.9451, Valid Loss: 0.9832\n",
            "Epoch: 2063, Train Loss: 0.9456, Valid Loss: 0.9835\n",
            "Epoch: 2064, Train Loss: 0.9442, Valid Loss: 0.9804\n",
            "Epoch: 2065, Train Loss: 0.9466, Valid Loss: 0.9819\n",
            "Epoch: 2066, Train Loss: 0.9464, Valid Loss: 0.9826\n",
            "Epoch: 2067, Train Loss: 0.9460, Valid Loss: 0.9832\n",
            "Epoch: 2068, Train Loss: 0.9453, Valid Loss: 0.9820\n",
            "Epoch: 2069, Train Loss: 0.9454, Valid Loss: 0.9816\n",
            "Epoch: 2070, Train Loss: 0.9454, Valid Loss: 0.9835\n",
            "Epoch: 2071, Train Loss: 0.9460, Valid Loss: 0.9818\n",
            "Epoch: 2072, Train Loss: 0.9459, Valid Loss: 0.9814\n",
            "Epoch: 2073, Train Loss: 0.9460, Valid Loss: 0.9818\n",
            "Epoch: 2074, Train Loss: 0.9463, Valid Loss: 0.9821\n",
            "Epoch: 2075, Train Loss: 0.9460, Valid Loss: 0.9827\n",
            "Epoch: 2076, Train Loss: 0.9461, Valid Loss: 0.9815\n",
            "Epoch: 2077, Train Loss: 0.9455, Valid Loss: 0.9816\n",
            "Epoch: 2078, Train Loss: 0.9464, Valid Loss: 0.9812\n",
            "Epoch: 2079, Train Loss: 0.9437, Valid Loss: 0.9829\n",
            "Epoch: 2080, Train Loss: 0.9452, Valid Loss: 0.9816\n",
            "Epoch: 2081, Train Loss: 0.9453, Valid Loss: 0.9827\n",
            "Epoch: 2082, Train Loss: 0.9458, Valid Loss: 0.9816\n",
            "Epoch: 2083, Train Loss: 0.9452, Valid Loss: 0.9821\n",
            "Epoch: 2084, Train Loss: 0.9457, Valid Loss: 0.9818\n",
            "Epoch: 2085, Train Loss: 0.9452, Valid Loss: 0.9803\n",
            "Epoch: 2086, Train Loss: 0.9448, Valid Loss: 0.9817\n",
            "Epoch: 2087, Train Loss: 0.9449, Valid Loss: 0.9825\n",
            "Epoch: 2088, Train Loss: 0.9464, Valid Loss: 0.9814\n",
            "Epoch: 2089, Train Loss: 0.9455, Valid Loss: 0.9814\n",
            "Epoch: 2090, Train Loss: 0.9445, Valid Loss: 0.9824\n",
            "Epoch: 2091, Train Loss: 0.9456, Valid Loss: 0.9818\n",
            "Epoch: 2092, Train Loss: 0.9461, Valid Loss: 0.9813\n",
            "Epoch: 2093, Train Loss: 0.9457, Valid Loss: 0.9819\n",
            "Epoch: 2094, Train Loss: 0.9451, Valid Loss: 0.9809\n",
            "Epoch: 2095, Train Loss: 0.9465, Valid Loss: 0.9819\n",
            "Epoch: 2096, Train Loss: 0.9443, Valid Loss: 0.9817\n",
            "Epoch: 2097, Train Loss: 0.9462, Valid Loss: 0.9812\n",
            "Epoch: 2098, Train Loss: 0.9454, Valid Loss: 0.9824\n",
            "Epoch: 2099, Train Loss: 0.9438, Valid Loss: 0.9813\n",
            "Epoch: 2100, Train Loss: 0.9455, Valid Loss: 0.9822\n",
            "Epoch: 2101, Train Loss: 0.9460, Valid Loss: 0.9809\n",
            "Epoch: 2102, Train Loss: 0.9452, Valid Loss: 0.9807\n",
            "Epoch: 2103, Train Loss: 0.9454, Valid Loss: 0.9817\n",
            "Epoch: 2104, Train Loss: 0.9454, Valid Loss: 0.9821\n",
            "Epoch: 2105, Train Loss: 0.9457, Valid Loss: 0.9819\n",
            "Epoch: 2106, Train Loss: 0.9449, Valid Loss: 0.9811\n",
            "Epoch: 2107, Train Loss: 0.9454, Valid Loss: 0.9803\n",
            "Epoch: 2108, Train Loss: 0.9457, Valid Loss: 0.9824\n",
            "Epoch: 2109, Train Loss: 0.9457, Valid Loss: 0.9814\n",
            "Epoch: 2110, Train Loss: 0.9454, Valid Loss: 0.9805\n",
            "Epoch: 2111, Train Loss: 0.9449, Valid Loss: 0.9814\n",
            "Epoch: 2112, Train Loss: 0.9460, Valid Loss: 0.9819\n",
            "Epoch: 2113, Train Loss: 0.9448, Valid Loss: 0.9813\n",
            "Epoch: 2114, Train Loss: 0.9450, Valid Loss: 0.9826\n",
            "Epoch: 2115, Train Loss: 0.9451, Valid Loss: 0.9800\n",
            "Epoch: 2116, Train Loss: 0.9455, Valid Loss: 0.9810\n",
            "Epoch: 2117, Train Loss: 0.9463, Valid Loss: 0.9817\n",
            "Epoch: 2118, Train Loss: 0.9457, Valid Loss: 0.9808\n",
            "Epoch: 2119, Train Loss: 0.9459, Valid Loss: 0.9805\n",
            "Epoch: 2120, Train Loss: 0.9459, Valid Loss: 0.9812\n",
            "Epoch: 2121, Train Loss: 0.9453, Valid Loss: 0.9821\n",
            "Epoch: 2122, Train Loss: 0.9456, Valid Loss: 0.9815\n",
            "Epoch: 2123, Train Loss: 0.9443, Valid Loss: 0.9806\n",
            "Epoch: 2124, Train Loss: 0.9457, Valid Loss: 0.9819\n",
            "Epoch: 2125, Train Loss: 0.9459, Valid Loss: 0.9811\n",
            "Epoch: 2126, Train Loss: 0.9459, Valid Loss: 0.9805\n",
            "Epoch: 2127, Train Loss: 0.9450, Valid Loss: 0.9812\n",
            "Epoch: 2128, Train Loss: 0.9446, Valid Loss: 0.9815\n",
            "Epoch: 2129, Train Loss: 0.9459, Valid Loss: 0.9802\n",
            "Epoch: 2130, Train Loss: 0.9448, Valid Loss: 0.9813\n",
            "Epoch: 2131, Train Loss: 0.9463, Valid Loss: 0.9826\n",
            "Epoch: 2132, Train Loss: 0.9459, Valid Loss: 0.9795\n",
            "Epoch: 2133, Train Loss: 0.9439, Valid Loss: 0.9812\n",
            "Epoch: 2134, Train Loss: 0.9455, Valid Loss: 0.9810\n",
            "Epoch: 2135, Train Loss: 0.9452, Valid Loss: 0.9833\n",
            "Epoch: 2136, Train Loss: 0.9451, Valid Loss: 0.9809\n",
            "Epoch: 2137, Train Loss: 0.9465, Valid Loss: 0.9785\n",
            "Epoch: 2138, Train Loss: 0.9443, Valid Loss: 0.9811\n",
            "Epoch: 2139, Train Loss: 0.9453, Valid Loss: 0.9804\n",
            "Epoch: 2140, Train Loss: 0.9453, Valid Loss: 0.9820\n",
            "Epoch: 2141, Train Loss: 0.9465, Valid Loss: 0.9804\n",
            "Epoch: 2142, Train Loss: 0.9456, Valid Loss: 0.9803\n",
            "Epoch: 2143, Train Loss: 0.9429, Valid Loss: 0.9810\n",
            "Epoch: 2144, Train Loss: 0.9458, Valid Loss: 0.9818\n",
            "Epoch: 2145, Train Loss: 0.9445, Valid Loss: 0.9812\n",
            "Epoch: 2146, Train Loss: 0.9444, Valid Loss: 0.9794\n",
            "Epoch: 2147, Train Loss: 0.9465, Valid Loss: 0.9807\n",
            "Epoch: 2148, Train Loss: 0.9454, Valid Loss: 0.9819\n",
            "Epoch: 2149, Train Loss: 0.9447, Valid Loss: 0.9791\n",
            "Epoch: 2150, Train Loss: 0.9452, Valid Loss: 0.9799\n",
            "Epoch: 2151, Train Loss: 0.9446, Valid Loss: 0.9817\n",
            "Epoch: 2152, Train Loss: 0.9468, Valid Loss: 0.9825\n",
            "Epoch: 2153, Train Loss: 0.9457, Valid Loss: 0.9792\n",
            "Epoch: 2154, Train Loss: 0.9457, Valid Loss: 0.9804\n",
            "Epoch: 2155, Train Loss: 0.9456, Valid Loss: 0.9808\n",
            "Epoch: 2156, Train Loss: 0.9446, Valid Loss: 0.9808\n",
            "Epoch: 2157, Train Loss: 0.9448, Valid Loss: 0.9802\n",
            "Epoch: 2158, Train Loss: 0.9463, Valid Loss: 0.9804\n",
            "Epoch: 2159, Train Loss: 0.9460, Valid Loss: 0.9811\n",
            "Epoch: 2160, Train Loss: 0.9444, Valid Loss: 0.9799\n",
            "Epoch: 2161, Train Loss: 0.9454, Valid Loss: 0.9805\n",
            "Epoch: 2162, Train Loss: 0.9454, Valid Loss: 0.9804\n",
            "Epoch: 2163, Train Loss: 0.9455, Valid Loss: 0.9797\n",
            "Epoch: 2164, Train Loss: 0.9463, Valid Loss: 0.9810\n",
            "Epoch: 2165, Train Loss: 0.9456, Valid Loss: 0.9806\n",
            "Epoch: 2166, Train Loss: 0.9449, Valid Loss: 0.9796\n",
            "Epoch: 2167, Train Loss: 0.9454, Valid Loss: 0.9800\n",
            "Epoch: 2168, Train Loss: 0.9459, Valid Loss: 0.9807\n",
            "Epoch: 2169, Train Loss: 0.9441, Valid Loss: 0.9798\n",
            "Epoch: 2170, Train Loss: 0.9455, Valid Loss: 0.9814\n",
            "Epoch: 2171, Train Loss: 0.9459, Valid Loss: 0.9805\n",
            "Epoch: 2172, Train Loss: 0.9455, Valid Loss: 0.9804\n",
            "Epoch: 2173, Train Loss: 0.9448, Valid Loss: 0.9803\n",
            "Epoch: 2174, Train Loss: 0.9448, Valid Loss: 0.9803\n",
            "Epoch: 2175, Train Loss: 0.9455, Valid Loss: 0.9804\n",
            "Epoch: 2176, Train Loss: 0.9451, Valid Loss: 0.9788\n",
            "Epoch: 2177, Train Loss: 0.9457, Valid Loss: 0.9816\n",
            "Epoch: 2178, Train Loss: 0.9448, Valid Loss: 0.9807\n",
            "Epoch: 2179, Train Loss: 0.9450, Valid Loss: 0.9793\n",
            "Epoch: 2180, Train Loss: 0.9459, Valid Loss: 0.9803\n",
            "Epoch: 2181, Train Loss: 0.9445, Valid Loss: 0.9805\n",
            "Epoch: 2182, Train Loss: 0.9462, Valid Loss: 0.9798\n",
            "Epoch: 2183, Train Loss: 0.9453, Valid Loss: 0.9793\n",
            "Epoch: 2184, Train Loss: 0.9461, Valid Loss: 0.9801\n",
            "Epoch: 2185, Train Loss: 0.9448, Valid Loss: 0.9801\n",
            "Epoch: 2186, Train Loss: 0.9457, Valid Loss: 0.9795\n",
            "Epoch: 2187, Train Loss: 0.9425, Valid Loss: 0.9796\n",
            "Epoch: 2188, Train Loss: 0.9442, Valid Loss: 0.9792\n",
            "Epoch: 2189, Train Loss: 0.9454, Valid Loss: 0.9802\n",
            "Epoch: 2190, Train Loss: 0.9460, Valid Loss: 0.9803\n",
            "Epoch: 2191, Train Loss: 0.9455, Valid Loss: 0.9807\n",
            "Epoch: 2192, Train Loss: 0.9446, Valid Loss: 0.9794\n",
            "Epoch: 2193, Train Loss: 0.9448, Valid Loss: 0.9789\n",
            "Epoch: 2194, Train Loss: 0.9452, Valid Loss: 0.9795\n",
            "Epoch: 2195, Train Loss: 0.9453, Valid Loss: 0.9809\n",
            "Epoch: 2196, Train Loss: 0.9455, Valid Loss: 0.9804\n",
            "Epoch: 2197, Train Loss: 0.9445, Valid Loss: 0.9802\n",
            "Epoch: 2198, Train Loss: 0.9457, Valid Loss: 0.9794\n",
            "Epoch: 2199, Train Loss: 0.9461, Valid Loss: 0.9789\n",
            "Epoch: 2200, Train Loss: 0.9440, Valid Loss: 0.9806\n",
            "Epoch: 2201, Train Loss: 0.9449, Valid Loss: 0.9797\n",
            "Epoch: 2202, Train Loss: 0.9423, Valid Loss: 0.9795\n",
            "Epoch: 2203, Train Loss: 0.9461, Valid Loss: 0.9815\n",
            "Epoch: 2204, Train Loss: 0.9456, Valid Loss: 0.9791\n",
            "Epoch: 2205, Train Loss: 0.9450, Valid Loss: 0.9793\n",
            "Epoch: 2206, Train Loss: 0.9452, Valid Loss: 0.9809\n",
            "Epoch: 2207, Train Loss: 0.9447, Valid Loss: 0.9785\n",
            "Epoch: 2208, Train Loss: 0.9421, Valid Loss: 0.9797\n",
            "Epoch: 2209, Train Loss: 0.9461, Valid Loss: 0.9824\n",
            "Epoch: 2210, Train Loss: 0.9440, Valid Loss: 0.9792\n",
            "Epoch: 2211, Train Loss: 0.9445, Valid Loss: 0.9778\n",
            "Epoch: 2212, Train Loss: 0.9449, Valid Loss: 0.9800\n",
            "Epoch: 2213, Train Loss: 0.9463, Valid Loss: 0.9809\n",
            "Epoch: 2214, Train Loss: 0.9458, Valid Loss: 0.9784\n",
            "Epoch: 2215, Train Loss: 0.9444, Valid Loss: 0.9803\n",
            "Epoch: 2216, Train Loss: 0.9461, Valid Loss: 0.9791\n",
            "Epoch: 2217, Train Loss: 0.9454, Valid Loss: 0.9789\n",
            "Epoch: 2218, Train Loss: 0.9452, Valid Loss: 0.9800\n",
            "Epoch: 2219, Train Loss: 0.9446, Valid Loss: 0.9795\n",
            "Epoch: 2220, Train Loss: 0.9462, Valid Loss: 0.9787\n",
            "Epoch: 2221, Train Loss: 0.9448, Valid Loss: 0.9804\n",
            "Epoch: 2222, Train Loss: 0.9453, Valid Loss: 0.9790\n",
            "Epoch: 2223, Train Loss: 0.9456, Valid Loss: 0.9792\n",
            "Epoch: 2224, Train Loss: 0.9444, Valid Loss: 0.9802\n",
            "Epoch: 2225, Train Loss: 0.9460, Valid Loss: 0.9789\n",
            "Epoch: 2226, Train Loss: 0.9450, Valid Loss: 0.9783\n",
            "Epoch: 2227, Train Loss: 0.9460, Valid Loss: 0.9797\n",
            "Epoch: 2228, Train Loss: 0.9447, Valid Loss: 0.9782\n",
            "Epoch: 2229, Train Loss: 0.9460, Valid Loss: 0.9803\n",
            "Epoch: 2230, Train Loss: 0.9455, Valid Loss: 0.9801\n",
            "Epoch: 2231, Train Loss: 0.9455, Valid Loss: 0.9796\n",
            "Epoch: 2232, Train Loss: 0.9455, Valid Loss: 0.9784\n",
            "Epoch: 2233, Train Loss: 0.9451, Valid Loss: 0.9785\n",
            "Epoch: 2234, Train Loss: 0.9461, Valid Loss: 0.9801\n",
            "Epoch: 2235, Train Loss: 0.9457, Valid Loss: 0.9793\n",
            "Epoch: 2236, Train Loss: 0.9443, Valid Loss: 0.9795\n",
            "Epoch: 2237, Train Loss: 0.9445, Valid Loss: 0.9785\n",
            "Epoch: 2238, Train Loss: 0.9455, Valid Loss: 0.9789\n",
            "Epoch: 2239, Train Loss: 0.9441, Valid Loss: 0.9803\n",
            "Epoch: 2240, Train Loss: 0.9451, Valid Loss: 0.9789\n",
            "Epoch: 2241, Train Loss: 0.9448, Valid Loss: 0.9791\n",
            "Epoch: 2242, Train Loss: 0.9461, Valid Loss: 0.9791\n",
            "Epoch: 2243, Train Loss: 0.9448, Valid Loss: 0.9799\n",
            "Epoch: 2244, Train Loss: 0.9460, Valid Loss: 0.9771\n",
            "Epoch: 2245, Train Loss: 0.9464, Valid Loss: 0.9797\n",
            "Epoch: 2246, Train Loss: 0.9458, Valid Loss: 0.9807\n",
            "Epoch: 2247, Train Loss: 0.9454, Valid Loss: 0.9787\n",
            "Epoch: 2248, Train Loss: 0.9452, Valid Loss: 0.9786\n",
            "Epoch: 2249, Train Loss: 0.9454, Valid Loss: 0.9788\n",
            "Epoch: 2250, Train Loss: 0.9439, Valid Loss: 0.9783\n",
            "Epoch: 2251, Train Loss: 0.9453, Valid Loss: 0.9794\n",
            "Epoch: 2252, Train Loss: 0.9447, Valid Loss: 0.9794\n",
            "Epoch: 2253, Train Loss: 0.9456, Valid Loss: 0.9788\n",
            "Epoch: 2254, Train Loss: 0.9443, Valid Loss: 0.9788\n",
            "Epoch: 2255, Train Loss: 0.9449, Valid Loss: 0.9798\n",
            "Epoch: 2256, Train Loss: 0.9456, Valid Loss: 0.9790\n",
            "Epoch: 2257, Train Loss: 0.9450, Valid Loss: 0.9789\n",
            "Epoch: 2258, Train Loss: 0.9456, Valid Loss: 0.9786\n",
            "Epoch: 2259, Train Loss: 0.9460, Valid Loss: 0.9788\n",
            "Epoch: 2260, Train Loss: 0.9456, Valid Loss: 0.9781\n",
            "Epoch: 2261, Train Loss: 0.9460, Valid Loss: 0.9801\n",
            "Epoch: 2262, Train Loss: 0.9435, Valid Loss: 0.9790\n",
            "Epoch: 2263, Train Loss: 0.9458, Valid Loss: 0.9787\n",
            "Epoch: 2264, Train Loss: 0.9456, Valid Loss: 0.9788\n",
            "Epoch: 2265, Train Loss: 0.9455, Valid Loss: 0.9787\n",
            "Epoch: 2266, Train Loss: 0.9451, Valid Loss: 0.9784\n",
            "Epoch: 2267, Train Loss: 0.9455, Valid Loss: 0.9785\n",
            "Epoch: 2268, Train Loss: 0.9454, Valid Loss: 0.9809\n",
            "Epoch: 2269, Train Loss: 0.9443, Valid Loss: 0.9787\n",
            "Epoch: 2270, Train Loss: 0.9455, Valid Loss: 0.9776\n",
            "Epoch: 2271, Train Loss: 0.9458, Valid Loss: 0.9783\n",
            "Epoch: 2272, Train Loss: 0.9451, Valid Loss: 0.9797\n",
            "Epoch: 2273, Train Loss: 0.9464, Valid Loss: 0.9798\n",
            "Epoch: 2274, Train Loss: 0.9443, Valid Loss: 0.9776\n",
            "Epoch: 2275, Train Loss: 0.9456, Valid Loss: 0.9773\n",
            "Epoch: 2276, Train Loss: 0.9449, Valid Loss: 0.9791\n",
            "Epoch: 2277, Train Loss: 0.9457, Valid Loss: 0.9799\n",
            "Epoch: 2278, Train Loss: 0.9458, Valid Loss: 0.9794\n",
            "Epoch: 2279, Train Loss: 0.9462, Valid Loss: 0.9781\n",
            "Epoch: 2280, Train Loss: 0.9444, Valid Loss: 0.9782\n",
            "Epoch: 2281, Train Loss: 0.9445, Valid Loss: 0.9798\n",
            "Epoch: 2282, Train Loss: 0.9454, Valid Loss: 0.9782\n",
            "Epoch: 2283, Train Loss: 0.9466, Valid Loss: 0.9771\n",
            "Epoch: 2284, Train Loss: 0.9446, Valid Loss: 0.9793\n",
            "Epoch: 2285, Train Loss: 0.9463, Valid Loss: 0.9787\n",
            "Epoch: 2286, Train Loss: 0.9444, Valid Loss: 0.9788\n",
            "Epoch: 2287, Train Loss: 0.9457, Valid Loss: 0.9786\n",
            "Epoch: 2288, Train Loss: 0.9455, Valid Loss: 0.9785\n",
            "Epoch: 2289, Train Loss: 0.9452, Valid Loss: 0.9782\n",
            "Epoch: 2290, Train Loss: 0.9453, Valid Loss: 0.9793\n",
            "Epoch: 2291, Train Loss: 0.9449, Valid Loss: 0.9782\n",
            "Epoch: 2292, Train Loss: 0.9456, Valid Loss: 0.9783\n",
            "Epoch: 2293, Train Loss: 0.9464, Valid Loss: 0.9787\n",
            "Epoch: 2294, Train Loss: 0.9442, Valid Loss: 0.9781\n",
            "Epoch: 2295, Train Loss: 0.9447, Valid Loss: 0.9777\n",
            "Epoch: 2296, Train Loss: 0.9455, Valid Loss: 0.9783\n",
            "Epoch: 2297, Train Loss: 0.9454, Valid Loss: 0.9786\n",
            "Epoch: 2298, Train Loss: 0.9457, Valid Loss: 0.9789\n",
            "Epoch: 2299, Train Loss: 0.9452, Valid Loss: 0.9781\n",
            "Epoch: 2300, Train Loss: 0.9456, Valid Loss: 0.9778\n",
            "Epoch: 2301, Train Loss: 0.9451, Valid Loss: 0.9795\n",
            "Epoch: 2302, Train Loss: 0.9434, Valid Loss: 0.9776\n",
            "Epoch: 2303, Train Loss: 0.9462, Valid Loss: 0.9773\n",
            "Epoch: 2304, Train Loss: 0.9459, Valid Loss: 0.9792\n",
            "Epoch: 2305, Train Loss: 0.9454, Valid Loss: 0.9784\n",
            "Epoch: 2306, Train Loss: 0.9464, Valid Loss: 0.9776\n",
            "Epoch: 2307, Train Loss: 0.9449, Valid Loss: 0.9790\n",
            "Epoch: 2308, Train Loss: 0.9455, Valid Loss: 0.9780\n",
            "Epoch: 2309, Train Loss: 0.9461, Valid Loss: 0.9782\n",
            "Epoch: 2310, Train Loss: 0.9453, Valid Loss: 0.9780\n",
            "Epoch: 2311, Train Loss: 0.9459, Valid Loss: 0.9787\n",
            "Epoch: 2312, Train Loss: 0.9448, Valid Loss: 0.9793\n",
            "Epoch: 2313, Train Loss: 0.9456, Valid Loss: 0.9770\n",
            "Epoch: 2314, Train Loss: 0.9454, Valid Loss: 0.9767\n",
            "Epoch: 2315, Train Loss: 0.9454, Valid Loss: 0.9778\n",
            "Epoch: 2316, Train Loss: 0.9449, Valid Loss: 0.9795\n",
            "Epoch: 2317, Train Loss: 0.9444, Valid Loss: 0.9784\n",
            "Epoch: 2318, Train Loss: 0.9455, Valid Loss: 0.9776\n",
            "Epoch: 2319, Train Loss: 0.9465, Valid Loss: 0.9792\n",
            "Epoch: 2320, Train Loss: 0.9447, Valid Loss: 0.9770\n",
            "Epoch: 2321, Train Loss: 0.9436, Valid Loss: 0.9772\n",
            "Epoch: 2322, Train Loss: 0.9458, Valid Loss: 0.9793\n",
            "Epoch: 2323, Train Loss: 0.9463, Valid Loss: 0.9779\n",
            "Epoch: 2324, Train Loss: 0.9455, Valid Loss: 0.9781\n",
            "Epoch: 2325, Train Loss: 0.9459, Valid Loss: 0.9771\n",
            "Epoch: 2326, Train Loss: 0.9445, Valid Loss: 0.9783\n",
            "Epoch: 2327, Train Loss: 0.9440, Valid Loss: 0.9795\n",
            "Epoch: 2328, Train Loss: 0.9457, Valid Loss: 0.9771\n",
            "Epoch: 2329, Train Loss: 0.9462, Valid Loss: 0.9778\n",
            "Epoch: 2330, Train Loss: 0.9457, Valid Loss: 0.9785\n",
            "Epoch: 2331, Train Loss: 0.9460, Valid Loss: 0.9785\n",
            "Epoch: 2332, Train Loss: 0.9461, Valid Loss: 0.9761\n",
            "Epoch: 2333, Train Loss: 0.9447, Valid Loss: 0.9777\n",
            "Epoch: 2334, Train Loss: 0.9458, Valid Loss: 0.9784\n",
            "Epoch: 2335, Train Loss: 0.9455, Valid Loss: 0.9780\n",
            "Epoch: 2336, Train Loss: 0.9462, Valid Loss: 0.9782\n",
            "Epoch: 2337, Train Loss: 0.9453, Valid Loss: 0.9778\n",
            "Epoch: 2338, Train Loss: 0.9436, Valid Loss: 0.9776\n",
            "Epoch: 2339, Train Loss: 0.9461, Valid Loss: 0.9788\n",
            "Epoch: 2340, Train Loss: 0.9452, Valid Loss: 0.9778\n",
            "Epoch: 2341, Train Loss: 0.9455, Valid Loss: 0.9762\n",
            "Epoch: 2342, Train Loss: 0.9458, Valid Loss: 0.9801\n",
            "Epoch: 2343, Train Loss: 0.9460, Valid Loss: 0.9779\n",
            "Epoch: 2344, Train Loss: 0.9452, Valid Loss: 0.9775\n",
            "Epoch: 2345, Train Loss: 0.9433, Valid Loss: 0.9772\n",
            "Epoch: 2346, Train Loss: 0.9447, Valid Loss: 0.9791\n",
            "Epoch: 2347, Train Loss: 0.9453, Valid Loss: 0.9766\n",
            "Epoch: 2348, Train Loss: 0.9458, Valid Loss: 0.9790\n",
            "Epoch: 2349, Train Loss: 0.9453, Valid Loss: 0.9779\n",
            "Epoch: 2350, Train Loss: 0.9455, Valid Loss: 0.9770\n",
            "Epoch: 2351, Train Loss: 0.9448, Valid Loss: 0.9789\n",
            "Epoch: 2352, Train Loss: 0.9459, Valid Loss: 0.9771\n",
            "Epoch: 2353, Train Loss: 0.9451, Valid Loss: 0.9777\n",
            "Epoch: 2354, Train Loss: 0.9449, Valid Loss: 0.9771\n",
            "Epoch: 2355, Train Loss: 0.9456, Valid Loss: 0.9780\n",
            "Epoch: 2356, Train Loss: 0.9457, Valid Loss: 0.9773\n",
            "Epoch: 2357, Train Loss: 0.9451, Valid Loss: 0.9776\n",
            "Epoch: 2358, Train Loss: 0.9459, Valid Loss: 0.9781\n",
            "Epoch: 2359, Train Loss: 0.9459, Valid Loss: 0.9771\n",
            "Epoch: 2360, Train Loss: 0.9453, Valid Loss: 0.9772\n",
            "Epoch: 2361, Train Loss: 0.9443, Valid Loss: 0.9789\n",
            "Epoch: 2362, Train Loss: 0.9464, Valid Loss: 0.9784\n",
            "Epoch: 2363, Train Loss: 0.9464, Valid Loss: 0.9762\n",
            "Epoch: 2364, Train Loss: 0.9460, Valid Loss: 0.9782\n",
            "Epoch: 2365, Train Loss: 0.9443, Valid Loss: 0.9772\n",
            "Epoch: 2366, Train Loss: 0.9444, Valid Loss: 0.9784\n",
            "Epoch: 2367, Train Loss: 0.9459, Valid Loss: 0.9774\n",
            "Epoch: 2368, Train Loss: 0.9461, Valid Loss: 0.9772\n",
            "Epoch: 2369, Train Loss: 0.9455, Valid Loss: 0.9783\n",
            "Epoch: 2370, Train Loss: 0.9430, Valid Loss: 0.9783\n",
            "Epoch: 2371, Train Loss: 0.9439, Valid Loss: 0.9776\n",
            "Epoch: 2372, Train Loss: 0.9451, Valid Loss: 0.9764\n",
            "Epoch: 2373, Train Loss: 0.9461, Valid Loss: 0.9778\n",
            "Epoch: 2374, Train Loss: 0.9458, Valid Loss: 0.9784\n",
            "Epoch: 2375, Train Loss: 0.9446, Valid Loss: 0.9779\n",
            "Epoch: 2376, Train Loss: 0.9452, Valid Loss: 0.9769\n",
            "Epoch: 2377, Train Loss: 0.9461, Valid Loss: 0.9773\n",
            "Epoch: 2378, Train Loss: 0.9462, Valid Loss: 0.9780\n",
            "Epoch: 2379, Train Loss: 0.9448, Valid Loss: 0.9779\n",
            "Epoch: 2380, Train Loss: 0.9445, Valid Loss: 0.9768\n",
            "Epoch: 2381, Train Loss: 0.9448, Valid Loss: 0.9780\n",
            "Epoch: 2382, Train Loss: 0.9457, Valid Loss: 0.9786\n",
            "Epoch: 2383, Train Loss: 0.9455, Valid Loss: 0.9768\n",
            "Epoch: 2384, Train Loss: 0.9460, Valid Loss: 0.9767\n",
            "Epoch: 2385, Train Loss: 0.9454, Valid Loss: 0.9769\n",
            "Epoch: 2386, Train Loss: 0.9461, Valid Loss: 0.9792\n",
            "Epoch: 2387, Train Loss: 0.9430, Valid Loss: 0.9767\n",
            "Epoch: 2388, Train Loss: 0.9453, Valid Loss: 0.9775\n",
            "Epoch: 2389, Train Loss: 0.9441, Valid Loss: 0.9767\n",
            "Epoch: 2390, Train Loss: 0.9456, Valid Loss: 0.9774\n",
            "Epoch: 2391, Train Loss: 0.9453, Valid Loss: 0.9773\n",
            "Epoch: 2392, Train Loss: 0.9445, Valid Loss: 0.9777\n",
            "Epoch: 2393, Train Loss: 0.9460, Valid Loss: 0.9776\n",
            "Epoch: 2394, Train Loss: 0.9448, Valid Loss: 0.9781\n",
            "Epoch: 2395, Train Loss: 0.9454, Valid Loss: 0.9773\n",
            "Epoch: 2396, Train Loss: 0.9458, Valid Loss: 0.9785\n",
            "Epoch: 2397, Train Loss: 0.9453, Valid Loss: 0.9776\n",
            "Epoch: 2398, Train Loss: 0.9457, Valid Loss: 0.9760\n",
            "Epoch: 2399, Train Loss: 0.9462, Valid Loss: 0.9770\n",
            "Epoch: 2400, Train Loss: 0.9442, Valid Loss: 0.9775\n",
            "Epoch: 2401, Train Loss: 0.9444, Valid Loss: 0.9782\n",
            "Epoch: 2402, Train Loss: 0.9454, Valid Loss: 0.9778\n",
            "Epoch: 2403, Train Loss: 0.9446, Valid Loss: 0.9764\n",
            "Epoch: 2404, Train Loss: 0.9455, Valid Loss: 0.9782\n",
            "Epoch: 2405, Train Loss: 0.9458, Valid Loss: 0.9767\n",
            "Epoch: 2406, Train Loss: 0.9460, Valid Loss: 0.9774\n",
            "Epoch: 2407, Train Loss: 0.9457, Valid Loss: 0.9770\n",
            "Epoch: 2408, Train Loss: 0.9466, Valid Loss: 0.9753\n",
            "Epoch: 2409, Train Loss: 0.9441, Valid Loss: 0.9787\n",
            "Epoch: 2410, Train Loss: 0.9459, Valid Loss: 0.9774\n",
            "Epoch: 2411, Train Loss: 0.9438, Valid Loss: 0.9768\n",
            "Epoch: 2412, Train Loss: 0.9457, Valid Loss: 0.9778\n",
            "Epoch: 2413, Train Loss: 0.9462, Valid Loss: 0.9769\n",
            "Epoch: 2414, Train Loss: 0.9459, Valid Loss: 0.9769\n",
            "Epoch: 2415, Train Loss: 0.9451, Valid Loss: 0.9770\n",
            "Epoch: 2416, Train Loss: 0.9454, Valid Loss: 0.9768\n",
            "Epoch: 2417, Train Loss: 0.9452, Valid Loss: 0.9773\n",
            "Epoch: 2418, Train Loss: 0.9450, Valid Loss: 0.9778\n",
            "Epoch: 2419, Train Loss: 0.9441, Valid Loss: 0.9771\n",
            "Epoch: 2420, Train Loss: 0.9445, Valid Loss: 0.9777\n",
            "Epoch: 2421, Train Loss: 0.9448, Valid Loss: 0.9777\n",
            "Epoch: 2422, Train Loss: 0.9460, Valid Loss: 0.9756\n",
            "Epoch: 2423, Train Loss: 0.9450, Valid Loss: 0.9780\n",
            "Epoch: 2424, Train Loss: 0.9457, Valid Loss: 0.9776\n",
            "Epoch: 2425, Train Loss: 0.9455, Valid Loss: 0.9769\n",
            "Epoch: 2426, Train Loss: 0.9462, Valid Loss: 0.9767\n",
            "Epoch: 2427, Train Loss: 0.9436, Valid Loss: 0.9775\n",
            "Epoch: 2428, Train Loss: 0.9461, Valid Loss: 0.9765\n",
            "Epoch: 2429, Train Loss: 0.9451, Valid Loss: 0.9764\n",
            "Epoch: 2430, Train Loss: 0.9461, Valid Loss: 0.9762\n",
            "Epoch: 2431, Train Loss: 0.9433, Valid Loss: 0.9788\n",
            "Epoch: 2432, Train Loss: 0.9457, Valid Loss: 0.9770\n",
            "Epoch: 2433, Train Loss: 0.9451, Valid Loss: 0.9753\n",
            "Epoch: 2434, Train Loss: 0.9446, Valid Loss: 0.9767\n",
            "Epoch: 2435, Train Loss: 0.9439, Valid Loss: 0.9782\n",
            "Epoch: 2436, Train Loss: 0.9463, Valid Loss: 0.9781\n",
            "Epoch: 2437, Train Loss: 0.9448, Valid Loss: 0.9754\n",
            "Epoch: 2438, Train Loss: 0.9450, Valid Loss: 0.9773\n",
            "Epoch: 2439, Train Loss: 0.9457, Valid Loss: 0.9767\n",
            "Epoch: 2440, Train Loss: 0.9450, Valid Loss: 0.9754\n",
            "Epoch: 2441, Train Loss: 0.9453, Valid Loss: 0.9776\n",
            "Epoch: 2442, Train Loss: 0.9465, Valid Loss: 0.9791\n",
            "Epoch: 2443, Train Loss: 0.9442, Valid Loss: 0.9759\n",
            "Epoch: 2444, Train Loss: 0.9452, Valid Loss: 0.9760\n",
            "Epoch: 2445, Train Loss: 0.9459, Valid Loss: 0.9769\n",
            "Epoch: 2446, Train Loss: 0.9460, Valid Loss: 0.9768\n",
            "Epoch: 2447, Train Loss: 0.9437, Valid Loss: 0.9767\n",
            "Epoch: 2448, Train Loss: 0.9452, Valid Loss: 0.9772\n",
            "Epoch: 2449, Train Loss: 0.9451, Valid Loss: 0.9772\n",
            "Epoch: 2450, Train Loss: 0.9446, Valid Loss: 0.9764\n",
            "Epoch: 2451, Train Loss: 0.9449, Valid Loss: 0.9774\n",
            "Epoch: 2452, Train Loss: 0.9458, Valid Loss: 0.9776\n",
            "Epoch: 2453, Train Loss: 0.9460, Valid Loss: 0.9757\n",
            "Epoch: 2454, Train Loss: 0.9457, Valid Loss: 0.9771\n",
            "Epoch: 2455, Train Loss: 0.9439, Valid Loss: 0.9779\n",
            "Epoch: 2456, Train Loss: 0.9449, Valid Loss: 0.9765\n",
            "Epoch: 2457, Train Loss: 0.9452, Valid Loss: 0.9764\n",
            "Epoch: 2458, Train Loss: 0.9446, Valid Loss: 0.9769\n",
            "Epoch: 2459, Train Loss: 0.9447, Valid Loss: 0.9775\n",
            "Epoch: 2460, Train Loss: 0.9456, Valid Loss: 0.9761\n",
            "Epoch: 2461, Train Loss: 0.9455, Valid Loss: 0.9767\n",
            "Epoch: 2462, Train Loss: 0.9451, Valid Loss: 0.9780\n",
            "Epoch: 2463, Train Loss: 0.9456, Valid Loss: 0.9755\n",
            "Epoch: 2464, Train Loss: 0.9446, Valid Loss: 0.9768\n",
            "Epoch: 2465, Train Loss: 0.9459, Valid Loss: 0.9773\n",
            "Epoch: 2466, Train Loss: 0.9449, Valid Loss: 0.9764\n",
            "Epoch: 2467, Train Loss: 0.9450, Valid Loss: 0.9765\n",
            "Epoch: 2468, Train Loss: 0.9458, Valid Loss: 0.9775\n",
            "Epoch: 2469, Train Loss: 0.9446, Valid Loss: 0.9763\n",
            "Epoch: 2470, Train Loss: 0.9447, Valid Loss: 0.9768\n",
            "Epoch: 2471, Train Loss: 0.9454, Valid Loss: 0.9769\n",
            "Epoch: 2472, Train Loss: 0.9464, Valid Loss: 0.9769\n",
            "Epoch: 2473, Train Loss: 0.9454, Valid Loss: 0.9765\n",
            "Epoch: 2474, Train Loss: 0.9456, Valid Loss: 0.9760\n",
            "Epoch: 2475, Train Loss: 0.9458, Valid Loss: 0.9776\n",
            "Epoch: 2476, Train Loss: 0.9466, Valid Loss: 0.9766\n",
            "Epoch: 2477, Train Loss: 0.9456, Valid Loss: 0.9777\n",
            "Epoch: 2478, Train Loss: 0.9458, Valid Loss: 0.9768\n",
            "Epoch: 2479, Train Loss: 0.9458, Valid Loss: 0.9765\n",
            "Epoch: 2480, Train Loss: 0.9455, Valid Loss: 0.9770\n",
            "Epoch: 2481, Train Loss: 0.9455, Valid Loss: 0.9776\n",
            "Epoch: 2482, Train Loss: 0.9449, Valid Loss: 0.9771\n",
            "Epoch: 2483, Train Loss: 0.9454, Valid Loss: 0.9755\n",
            "Epoch: 2484, Train Loss: 0.9456, Valid Loss: 0.9772\n",
            "Epoch: 2485, Train Loss: 0.9452, Valid Loss: 0.9764\n",
            "Epoch: 2486, Train Loss: 0.9448, Valid Loss: 0.9764\n",
            "Epoch: 2487, Train Loss: 0.9458, Valid Loss: 0.9776\n",
            "Epoch: 2488, Train Loss: 0.9464, Valid Loss: 0.9772\n",
            "Epoch: 2489, Train Loss: 0.9462, Valid Loss: 0.9766\n",
            "Epoch: 2490, Train Loss: 0.9457, Valid Loss: 0.9774\n",
            "Epoch: 2491, Train Loss: 0.9457, Valid Loss: 0.9763\n",
            "Epoch: 2492, Train Loss: 0.9458, Valid Loss: 0.9751\n",
            "Epoch: 2493, Train Loss: 0.9460, Valid Loss: 0.9770\n",
            "Epoch: 2494, Train Loss: 0.9445, Valid Loss: 0.9770\n",
            "Epoch: 2495, Train Loss: 0.9455, Valid Loss: 0.9775\n",
            "Epoch: 2496, Train Loss: 0.9455, Valid Loss: 0.9760\n",
            "Epoch: 2497, Train Loss: 0.9457, Valid Loss: 0.9773\n",
            "Epoch: 2498, Train Loss: 0.9457, Valid Loss: 0.9768\n",
            "Epoch: 2499, Train Loss: 0.9458, Valid Loss: 0.9771\n",
            "Epoch: 2500, Train Loss: 0.9453, Valid Loss: 0.9767\n",
            "Epoch: 2501, Train Loss: 0.9453, Valid Loss: 0.9760\n",
            "Epoch: 2502, Train Loss: 0.9454, Valid Loss: 0.9761\n",
            "Epoch: 2503, Train Loss: 0.9425, Valid Loss: 0.9767\n",
            "Epoch: 2504, Train Loss: 0.9459, Valid Loss: 0.9758\n",
            "Epoch: 2505, Train Loss: 0.9464, Valid Loss: 0.9771\n",
            "Epoch: 2506, Train Loss: 0.9454, Valid Loss: 0.9766\n",
            "Epoch: 2507, Train Loss: 0.9460, Valid Loss: 0.9750\n",
            "Epoch: 2508, Train Loss: 0.9449, Valid Loss: 0.9773\n",
            "Epoch: 2509, Train Loss: 0.9453, Valid Loss: 0.9772\n",
            "Epoch: 2510, Train Loss: 0.9452, Valid Loss: 0.9763\n",
            "Epoch: 2511, Train Loss: 0.9455, Valid Loss: 0.9761\n",
            "Epoch: 2512, Train Loss: 0.9452, Valid Loss: 0.9769\n",
            "Epoch: 2513, Train Loss: 0.9460, Valid Loss: 0.9762\n",
            "Epoch: 2514, Train Loss: 0.9466, Valid Loss: 0.9768\n",
            "Epoch: 2515, Train Loss: 0.9454, Valid Loss: 0.9760\n",
            "Epoch: 2516, Train Loss: 0.9443, Valid Loss: 0.9757\n",
            "Epoch: 2517, Train Loss: 0.9453, Valid Loss: 0.9775\n",
            "Epoch: 2518, Train Loss: 0.9442, Valid Loss: 0.9773\n",
            "Epoch: 2519, Train Loss: 0.9447, Valid Loss: 0.9759\n",
            "Epoch: 2520, Train Loss: 0.9456, Valid Loss: 0.9766\n",
            "Epoch: 2521, Train Loss: 0.9461, Valid Loss: 0.9765\n",
            "Epoch: 2522, Train Loss: 0.9448, Valid Loss: 0.9760\n",
            "Epoch: 2523, Train Loss: 0.9449, Valid Loss: 0.9773\n",
            "Epoch: 2524, Train Loss: 0.9447, Valid Loss: 0.9764\n",
            "Epoch: 2525, Train Loss: 0.9460, Valid Loss: 0.9769\n",
            "Epoch: 2526, Train Loss: 0.9450, Valid Loss: 0.9766\n",
            "Epoch: 2527, Train Loss: 0.9464, Valid Loss: 0.9758\n",
            "Epoch: 2528, Train Loss: 0.9461, Valid Loss: 0.9769\n",
            "Epoch: 2529, Train Loss: 0.9460, Valid Loss: 0.9760\n",
            "Epoch: 2530, Train Loss: 0.9457, Valid Loss: 0.9761\n",
            "Epoch: 2531, Train Loss: 0.9454, Valid Loss: 0.9765\n",
            "Epoch: 2532, Train Loss: 0.9456, Valid Loss: 0.9778\n",
            "Epoch: 2533, Train Loss: 0.9455, Valid Loss: 0.9770\n",
            "Epoch: 2534, Train Loss: 0.9458, Valid Loss: 0.9764\n",
            "Epoch: 2535, Train Loss: 0.9459, Valid Loss: 0.9761\n",
            "Epoch: 2536, Train Loss: 0.9465, Valid Loss: 0.9754\n",
            "Epoch: 2537, Train Loss: 0.9452, Valid Loss: 0.9759\n",
            "Epoch: 2538, Train Loss: 0.9441, Valid Loss: 0.9774\n",
            "Epoch: 2539, Train Loss: 0.9462, Valid Loss: 0.9773\n",
            "Epoch: 2540, Train Loss: 0.9458, Valid Loss: 0.9764\n",
            "Epoch: 2541, Train Loss: 0.9457, Valid Loss: 0.9769\n",
            "Epoch: 2542, Train Loss: 0.9458, Valid Loss: 0.9760\n",
            "Epoch: 2543, Train Loss: 0.9457, Valid Loss: 0.9765\n",
            "Epoch: 2544, Train Loss: 0.9452, Valid Loss: 0.9763\n",
            "Epoch: 2545, Train Loss: 0.9455, Valid Loss: 0.9770\n",
            "Epoch: 2546, Train Loss: 0.9450, Valid Loss: 0.9764\n",
            "Epoch: 2547, Train Loss: 0.9460, Valid Loss: 0.9781\n",
            "Epoch: 2548, Train Loss: 0.9458, Valid Loss: 0.9758\n",
            "Epoch: 2549, Train Loss: 0.9461, Valid Loss: 0.9768\n",
            "Epoch: 2550, Train Loss: 0.9456, Valid Loss: 0.9766\n",
            "Epoch: 2551, Train Loss: 0.9451, Valid Loss: 0.9767\n",
            "Epoch: 2552, Train Loss: 0.9464, Valid Loss: 0.9761\n",
            "Epoch: 2553, Train Loss: 0.9460, Valid Loss: 0.9758\n",
            "Epoch: 2554, Train Loss: 0.9455, Valid Loss: 0.9773\n",
            "Epoch: 2555, Train Loss: 0.9437, Valid Loss: 0.9767\n",
            "Epoch: 2556, Train Loss: 0.9458, Valid Loss: 0.9775\n",
            "Epoch: 2557, Train Loss: 0.9449, Valid Loss: 0.9755\n",
            "Epoch: 2558, Train Loss: 0.9456, Valid Loss: 0.9759\n",
            "Epoch: 2559, Train Loss: 0.9459, Valid Loss: 0.9764\n",
            "Epoch: 2560, Train Loss: 0.9461, Valid Loss: 0.9761\n",
            "Epoch: 2561, Train Loss: 0.9451, Valid Loss: 0.9774\n",
            "Epoch: 2562, Train Loss: 0.9460, Valid Loss: 0.9768\n",
            "Epoch: 2563, Train Loss: 0.9450, Valid Loss: 0.9749\n",
            "Epoch: 2564, Train Loss: 0.9459, Valid Loss: 0.9765\n",
            "Epoch: 2565, Train Loss: 0.9462, Valid Loss: 0.9768\n",
            "Epoch: 2566, Train Loss: 0.9455, Valid Loss: 0.9773\n",
            "Epoch: 2567, Train Loss: 0.9457, Valid Loss: 0.9756\n",
            "Epoch: 2568, Train Loss: 0.9451, Valid Loss: 0.9769\n",
            "Epoch: 2569, Train Loss: 0.9450, Valid Loss: 0.9761\n",
            "Epoch: 2570, Train Loss: 0.9460, Valid Loss: 0.9758\n",
            "Epoch: 2571, Train Loss: 0.9445, Valid Loss: 0.9777\n",
            "Epoch: 2572, Train Loss: 0.9465, Valid Loss: 0.9756\n",
            "Epoch: 2573, Train Loss: 0.9466, Valid Loss: 0.9761\n",
            "Epoch: 2574, Train Loss: 0.9456, Valid Loss: 0.9761\n",
            "Epoch: 2575, Train Loss: 0.9445, Valid Loss: 0.9768\n",
            "Epoch: 2576, Train Loss: 0.9458, Valid Loss: 0.9762\n",
            "Epoch: 2577, Train Loss: 0.9456, Valid Loss: 0.9759\n",
            "Epoch: 2578, Train Loss: 0.9450, Valid Loss: 0.9758\n",
            "Epoch: 2579, Train Loss: 0.9462, Valid Loss: 0.9759\n",
            "Epoch: 2580, Train Loss: 0.9433, Valid Loss: 0.9782\n",
            "Epoch: 2581, Train Loss: 0.9458, Valid Loss: 0.9769\n",
            "Epoch: 2582, Train Loss: 0.9451, Valid Loss: 0.9747\n",
            "Epoch: 2583, Train Loss: 0.9453, Valid Loss: 0.9770\n",
            "Epoch: 2584, Train Loss: 0.9446, Valid Loss: 0.9762\n",
            "Epoch: 2585, Train Loss: 0.9460, Valid Loss: 0.9773\n",
            "Epoch: 2586, Train Loss: 0.9460, Valid Loss: 0.9752\n",
            "Epoch: 2587, Train Loss: 0.9461, Valid Loss: 0.9754\n",
            "Epoch: 2588, Train Loss: 0.9437, Valid Loss: 0.9773\n",
            "Epoch: 2589, Train Loss: 0.9446, Valid Loss: 0.9762\n",
            "Epoch: 2590, Train Loss: 0.9466, Valid Loss: 0.9754\n",
            "Epoch: 2591, Train Loss: 0.9463, Valid Loss: 0.9770\n",
            "Epoch: 2592, Train Loss: 0.9461, Valid Loss: 0.9756\n",
            "Epoch: 2593, Train Loss: 0.9459, Valid Loss: 0.9758\n",
            "Epoch: 2594, Train Loss: 0.9447, Valid Loss: 0.9769\n",
            "Epoch: 2595, Train Loss: 0.9464, Valid Loss: 0.9762\n",
            "Epoch: 2596, Train Loss: 0.9438, Valid Loss: 0.9757\n",
            "Epoch: 2597, Train Loss: 0.9458, Valid Loss: 0.9764\n",
            "Epoch: 2598, Train Loss: 0.9456, Valid Loss: 0.9747\n",
            "Epoch: 2599, Train Loss: 0.9461, Valid Loss: 0.9763\n",
            "Epoch: 2600, Train Loss: 0.9461, Valid Loss: 0.9766\n",
            "Epoch: 2601, Train Loss: 0.9439, Valid Loss: 0.9756\n",
            "Epoch: 2602, Train Loss: 0.9457, Valid Loss: 0.9756\n",
            "Epoch: 2603, Train Loss: 0.9453, Valid Loss: 0.9766\n",
            "Epoch: 2604, Train Loss: 0.9457, Valid Loss: 0.9759\n",
            "Epoch: 2605, Train Loss: 0.9462, Valid Loss: 0.9775\n",
            "Epoch: 2606, Train Loss: 0.9445, Valid Loss: 0.9750\n",
            "Epoch: 2607, Train Loss: 0.9464, Valid Loss: 0.9759\n",
            "Epoch: 2608, Train Loss: 0.9462, Valid Loss: 0.9756\n",
            "Epoch: 2609, Train Loss: 0.9445, Valid Loss: 0.9758\n",
            "Epoch: 2610, Train Loss: 0.9463, Valid Loss: 0.9766\n",
            "Epoch: 2611, Train Loss: 0.9457, Valid Loss: 0.9758\n",
            "Epoch: 2612, Train Loss: 0.9443, Valid Loss: 0.9752\n",
            "Epoch: 2613, Train Loss: 0.9463, Valid Loss: 0.9759\n",
            "Epoch: 2614, Train Loss: 0.9465, Valid Loss: 0.9785\n",
            "Epoch: 2615, Train Loss: 0.9441, Valid Loss: 0.9754\n",
            "Epoch: 2616, Train Loss: 0.9450, Valid Loss: 0.9743\n",
            "Epoch: 2617, Train Loss: 0.9452, Valid Loss: 0.9756\n",
            "Epoch: 2618, Train Loss: 0.9453, Valid Loss: 0.9778\n",
            "Epoch: 2619, Train Loss: 0.9443, Valid Loss: 0.9761\n",
            "Epoch: 2620, Train Loss: 0.9453, Valid Loss: 0.9751\n",
            "Epoch: 2621, Train Loss: 0.9459, Valid Loss: 0.9752\n",
            "Epoch: 2622, Train Loss: 0.9447, Valid Loss: 0.9761\n",
            "Epoch: 2623, Train Loss: 0.9438, Valid Loss: 0.9767\n",
            "Epoch: 2624, Train Loss: 0.9452, Valid Loss: 0.9761\n",
            "Epoch: 2625, Train Loss: 0.9438, Valid Loss: 0.9749\n",
            "Epoch: 2626, Train Loss: 0.9443, Valid Loss: 0.9761\n",
            "Epoch: 2627, Train Loss: 0.9460, Valid Loss: 0.9762\n",
            "Epoch: 2628, Train Loss: 0.9450, Valid Loss: 0.9745\n",
            "Epoch: 2629, Train Loss: 0.9445, Valid Loss: 0.9755\n",
            "Epoch: 2630, Train Loss: 0.9466, Valid Loss: 0.9766\n",
            "Epoch: 2631, Train Loss: 0.9461, Valid Loss: 0.9757\n",
            "Epoch: 2632, Train Loss: 0.9457, Valid Loss: 0.9760\n",
            "Epoch: 2633, Train Loss: 0.9440, Valid Loss: 0.9749\n",
            "Epoch: 2634, Train Loss: 0.9431, Valid Loss: 0.9769\n",
            "Epoch: 2635, Train Loss: 0.9453, Valid Loss: 0.9757\n",
            "Epoch: 2636, Train Loss: 0.9450, Valid Loss: 0.9751\n",
            "Epoch: 2637, Train Loss: 0.9455, Valid Loss: 0.9762\n",
            "Epoch: 2638, Train Loss: 0.9462, Valid Loss: 0.9773\n",
            "Epoch: 2639, Train Loss: 0.9453, Valid Loss: 0.9742\n",
            "Epoch: 2640, Train Loss: 0.9461, Valid Loss: 0.9755\n",
            "Epoch: 2641, Train Loss: 0.9438, Valid Loss: 0.9759\n",
            "Epoch: 2642, Train Loss: 0.9451, Valid Loss: 0.9753\n",
            "Epoch: 2643, Train Loss: 0.9460, Valid Loss: 0.9758\n",
            "Epoch: 2644, Train Loss: 0.9461, Valid Loss: 0.9764\n",
            "Epoch: 2645, Train Loss: 0.9454, Valid Loss: 0.9758\n",
            "Epoch: 2646, Train Loss: 0.9450, Valid Loss: 0.9745\n",
            "Epoch: 2647, Train Loss: 0.9463, Valid Loss: 0.9764\n",
            "Epoch: 2648, Train Loss: 0.9462, Valid Loss: 0.9757\n",
            "Epoch: 2649, Train Loss: 0.9453, Valid Loss: 0.9760\n",
            "Epoch: 2650, Train Loss: 0.9443, Valid Loss: 0.9760\n",
            "Epoch: 2651, Train Loss: 0.9459, Valid Loss: 0.9751\n",
            "Epoch: 2652, Train Loss: 0.9456, Valid Loss: 0.9757\n",
            "Epoch: 2653, Train Loss: 0.9452, Valid Loss: 0.9759\n",
            "Epoch: 2654, Train Loss: 0.9461, Valid Loss: 0.9760\n",
            "Epoch: 2655, Train Loss: 0.9466, Valid Loss: 0.9754\n",
            "Epoch: 2656, Train Loss: 0.9454, Valid Loss: 0.9758\n",
            "Epoch: 2657, Train Loss: 0.9455, Valid Loss: 0.9760\n",
            "Epoch: 2658, Train Loss: 0.9459, Valid Loss: 0.9759\n",
            "Epoch: 2659, Train Loss: 0.9450, Valid Loss: 0.9757\n",
            "Epoch: 2660, Train Loss: 0.9450, Valid Loss: 0.9753\n",
            "Epoch: 2661, Train Loss: 0.9437, Valid Loss: 0.9752\n",
            "Epoch: 2662, Train Loss: 0.9462, Valid Loss: 0.9768\n",
            "Epoch: 2663, Train Loss: 0.9466, Valid Loss: 0.9755\n",
            "Epoch: 2664, Train Loss: 0.9457, Valid Loss: 0.9752\n",
            "Epoch: 2665, Train Loss: 0.9457, Valid Loss: 0.9762\n",
            "Epoch: 2666, Train Loss: 0.9447, Valid Loss: 0.9767\n",
            "Epoch: 2667, Train Loss: 0.9457, Valid Loss: 0.9752\n",
            "Epoch: 2668, Train Loss: 0.9454, Valid Loss: 0.9755\n",
            "Epoch: 2669, Train Loss: 0.9459, Valid Loss: 0.9752\n",
            "Epoch: 2670, Train Loss: 0.9459, Valid Loss: 0.9755\n",
            "Epoch: 2671, Train Loss: 0.9444, Valid Loss: 0.9772\n",
            "Epoch: 2672, Train Loss: 0.9457, Valid Loss: 0.9752\n",
            "Epoch: 2673, Train Loss: 0.9464, Valid Loss: 0.9753\n",
            "Epoch: 2674, Train Loss: 0.9463, Valid Loss: 0.9759\n",
            "Epoch: 2675, Train Loss: 0.9467, Valid Loss: 0.9763\n",
            "Epoch: 2676, Train Loss: 0.9451, Valid Loss: 0.9762\n",
            "Epoch: 2677, Train Loss: 0.9451, Valid Loss: 0.9761\n",
            "Epoch: 2678, Train Loss: 0.9458, Valid Loss: 0.9744\n",
            "Epoch: 2679, Train Loss: 0.9465, Valid Loss: 0.9770\n",
            "Epoch: 2680, Train Loss: 0.9457, Valid Loss: 0.9753\n",
            "Epoch: 2681, Train Loss: 0.9460, Valid Loss: 0.9763\n",
            "Epoch: 2682, Train Loss: 0.9450, Valid Loss: 0.9751\n",
            "Epoch: 2683, Train Loss: 0.9469, Valid Loss: 0.9779\n",
            "Epoch: 2684, Train Loss: 0.9456, Valid Loss: 0.9754\n",
            "Epoch: 2685, Train Loss: 0.9449, Valid Loss: 0.9749\n",
            "Epoch: 2686, Train Loss: 0.9464, Valid Loss: 0.9758\n",
            "Epoch: 2687, Train Loss: 0.9461, Valid Loss: 0.9757\n",
            "Epoch: 2688, Train Loss: 0.9444, Valid Loss: 0.9765\n",
            "Epoch: 2689, Train Loss: 0.9457, Valid Loss: 0.9751\n",
            "Epoch: 2690, Train Loss: 0.9452, Valid Loss: 0.9770\n",
            "Epoch: 2691, Train Loss: 0.9450, Valid Loss: 0.9755\n",
            "Epoch: 2692, Train Loss: 0.9457, Valid Loss: 0.9759\n",
            "Epoch: 2693, Train Loss: 0.9459, Valid Loss: 0.9746\n",
            "Epoch: 2694, Train Loss: 0.9463, Valid Loss: 0.9763\n",
            "Epoch: 2695, Train Loss: 0.9460, Valid Loss: 0.9759\n",
            "Epoch: 2696, Train Loss: 0.9455, Valid Loss: 0.9752\n",
            "Epoch: 2697, Train Loss: 0.9444, Valid Loss: 0.9771\n",
            "Epoch: 2698, Train Loss: 0.9448, Valid Loss: 0.9747\n",
            "Epoch: 2699, Train Loss: 0.9458, Valid Loss: 0.9772\n",
            "Epoch: 2700, Train Loss: 0.9455, Valid Loss: 0.9764\n",
            "Epoch: 2701, Train Loss: 0.9460, Valid Loss: 0.9757\n",
            "Epoch: 2702, Train Loss: 0.9461, Valid Loss: 0.9746\n",
            "Epoch: 2703, Train Loss: 0.9463, Valid Loss: 0.9758\n",
            "Epoch: 2704, Train Loss: 0.9460, Valid Loss: 0.9771\n",
            "Epoch: 2705, Train Loss: 0.9462, Valid Loss: 0.9756\n",
            "Epoch: 2706, Train Loss: 0.9454, Valid Loss: 0.9753\n",
            "Epoch: 2707, Train Loss: 0.9457, Valid Loss: 0.9773\n",
            "Epoch: 2708, Train Loss: 0.9457, Valid Loss: 0.9758\n",
            "Epoch: 2709, Train Loss: 0.9460, Valid Loss: 0.9747\n",
            "Epoch: 2710, Train Loss: 0.9459, Valid Loss: 0.9752\n",
            "Epoch: 2711, Train Loss: 0.9460, Valid Loss: 0.9759\n",
            "Epoch: 2712, Train Loss: 0.9457, Valid Loss: 0.9770\n",
            "Epoch: 2713, Train Loss: 0.9462, Valid Loss: 0.9756\n",
            "Epoch: 2714, Train Loss: 0.9447, Valid Loss: 0.9757\n",
            "Epoch: 2715, Train Loss: 0.9462, Valid Loss: 0.9752\n",
            "Epoch: 2716, Train Loss: 0.9458, Valid Loss: 0.9753\n",
            "Epoch: 2717, Train Loss: 0.9449, Valid Loss: 0.9760\n",
            "Epoch: 2718, Train Loss: 0.9467, Valid Loss: 0.9757\n",
            "Epoch: 2719, Train Loss: 0.9452, Valid Loss: 0.9754\n",
            "Epoch: 2720, Train Loss: 0.9456, Valid Loss: 0.9763\n",
            "Epoch: 2721, Train Loss: 0.9449, Valid Loss: 0.9758\n",
            "Epoch: 2722, Train Loss: 0.9455, Valid Loss: 0.9749\n",
            "Epoch: 2723, Train Loss: 0.9462, Valid Loss: 0.9752\n",
            "Epoch: 2724, Train Loss: 0.9449, Valid Loss: 0.9772\n",
            "Epoch: 2725, Train Loss: 0.9449, Valid Loss: 0.9763\n",
            "Epoch: 2726, Train Loss: 0.9461, Valid Loss: 0.9747\n",
            "Epoch: 2727, Train Loss: 0.9458, Valid Loss: 0.9756\n",
            "Epoch: 2728, Train Loss: 0.9455, Valid Loss: 0.9757\n",
            "Epoch: 2729, Train Loss: 0.9464, Valid Loss: 0.9757\n",
            "Epoch: 2730, Train Loss: 0.9457, Valid Loss: 0.9744\n",
            "Epoch: 2731, Train Loss: 0.9467, Valid Loss: 0.9760\n",
            "Epoch: 2732, Train Loss: 0.9448, Valid Loss: 0.9754\n",
            "Epoch: 2733, Train Loss: 0.9452, Valid Loss: 0.9757\n",
            "Epoch: 2734, Train Loss: 0.9466, Valid Loss: 0.9761\n",
            "Epoch: 2735, Train Loss: 0.9443, Valid Loss: 0.9754\n",
            "Epoch: 2736, Train Loss: 0.9456, Valid Loss: 0.9742\n",
            "Epoch: 2737, Train Loss: 0.9453, Valid Loss: 0.9752\n",
            "Epoch: 2738, Train Loss: 0.9455, Valid Loss: 0.9760\n",
            "Epoch: 2739, Train Loss: 0.9459, Valid Loss: 0.9760\n",
            "Epoch: 2740, Train Loss: 0.9459, Valid Loss: 0.9751\n",
            "Epoch: 2741, Train Loss: 0.9463, Valid Loss: 0.9745\n",
            "Epoch: 2742, Train Loss: 0.9457, Valid Loss: 0.9775\n",
            "Epoch: 2743, Train Loss: 0.9446, Valid Loss: 0.9757\n",
            "Epoch: 2744, Train Loss: 0.9462, Valid Loss: 0.9742\n",
            "Epoch: 2745, Train Loss: 0.9466, Valid Loss: 0.9753\n",
            "Epoch: 2746, Train Loss: 0.9462, Valid Loss: 0.9761\n",
            "Epoch: 2747, Train Loss: 0.9464, Valid Loss: 0.9750\n",
            "Epoch: 2748, Train Loss: 0.9458, Valid Loss: 0.9761\n",
            "Epoch: 2749, Train Loss: 0.9457, Valid Loss: 0.9763\n",
            "Epoch: 2750, Train Loss: 0.9455, Valid Loss: 0.9743\n",
            "Epoch: 2751, Train Loss: 0.9463, Valid Loss: 0.9763\n",
            "Epoch: 2752, Train Loss: 0.9461, Valid Loss: 0.9742\n",
            "Epoch: 2753, Train Loss: 0.9466, Valid Loss: 0.9757\n",
            "Epoch: 2754, Train Loss: 0.9446, Valid Loss: 0.9748\n",
            "Epoch: 2755, Train Loss: 0.9459, Valid Loss: 0.9762\n",
            "Epoch: 2756, Train Loss: 0.9453, Valid Loss: 0.9759\n",
            "Epoch: 2757, Train Loss: 0.9455, Valid Loss: 0.9748\n",
            "Epoch: 2758, Train Loss: 0.9465, Valid Loss: 0.9750\n",
            "Epoch: 2759, Train Loss: 0.9451, Valid Loss: 0.9774\n",
            "Epoch: 2760, Train Loss: 0.9451, Valid Loss: 0.9759\n",
            "Epoch: 2761, Train Loss: 0.9448, Valid Loss: 0.9739\n",
            "Epoch: 2762, Train Loss: 0.9459, Valid Loss: 0.9753\n",
            "Epoch: 2763, Train Loss: 0.9456, Valid Loss: 0.9761\n",
            "Epoch: 2764, Train Loss: 0.9462, Valid Loss: 0.9759\n",
            "Epoch: 2765, Train Loss: 0.9464, Valid Loss: 0.9749\n",
            "Epoch: 2766, Train Loss: 0.9466, Valid Loss: 0.9746\n",
            "Epoch: 2767, Train Loss: 0.9463, Valid Loss: 0.9753\n",
            "Epoch: 2768, Train Loss: 0.9451, Valid Loss: 0.9752\n",
            "Epoch: 2769, Train Loss: 0.9458, Valid Loss: 0.9745\n",
            "Epoch: 2770, Train Loss: 0.9462, Valid Loss: 0.9768\n",
            "Epoch: 2771, Train Loss: 0.9458, Valid Loss: 0.9749\n",
            "Epoch: 2772, Train Loss: 0.9451, Valid Loss: 0.9746\n",
            "Epoch: 2773, Train Loss: 0.9436, Valid Loss: 0.9751\n",
            "Epoch: 2774, Train Loss: 0.9456, Valid Loss: 0.9772\n",
            "Epoch: 2775, Train Loss: 0.9462, Valid Loss: 0.9746\n",
            "Epoch: 2776, Train Loss: 0.9451, Valid Loss: 0.9759\n",
            "Epoch: 2777, Train Loss: 0.9458, Valid Loss: 0.9744\n",
            "Epoch: 2778, Train Loss: 0.9444, Valid Loss: 0.9770\n",
            "Epoch: 2779, Train Loss: 0.9464, Valid Loss: 0.9752\n",
            "Epoch: 2780, Train Loss: 0.9456, Valid Loss: 0.9754\n",
            "Epoch: 2781, Train Loss: 0.9468, Valid Loss: 0.9743\n",
            "Epoch: 2782, Train Loss: 0.9468, Valid Loss: 0.9766\n",
            "Epoch: 2783, Train Loss: 0.9465, Valid Loss: 0.9743\n",
            "Epoch: 2784, Train Loss: 0.9461, Valid Loss: 0.9748\n",
            "Epoch: 2785, Train Loss: 0.9460, Valid Loss: 0.9762\n",
            "Epoch: 2786, Train Loss: 0.9454, Valid Loss: 0.9752\n",
            "Epoch: 2787, Train Loss: 0.9462, Valid Loss: 0.9756\n",
            "Epoch: 2788, Train Loss: 0.9444, Valid Loss: 0.9768\n",
            "Epoch: 2789, Train Loss: 0.9444, Valid Loss: 0.9754\n",
            "Epoch: 2790, Train Loss: 0.9468, Valid Loss: 0.9750\n",
            "Epoch: 2791, Train Loss: 0.9471, Valid Loss: 0.9747\n",
            "Epoch: 2792, Train Loss: 0.9452, Valid Loss: 0.9759\n",
            "Epoch: 2793, Train Loss: 0.9460, Valid Loss: 0.9756\n",
            "Epoch: 2794, Train Loss: 0.9464, Valid Loss: 0.9748\n",
            "Epoch: 2795, Train Loss: 0.9462, Valid Loss: 0.9745\n",
            "Epoch: 2796, Train Loss: 0.9463, Valid Loss: 0.9774\n",
            "Epoch: 2797, Train Loss: 0.9456, Valid Loss: 0.9750\n",
            "Epoch: 2798, Train Loss: 0.9467, Valid Loss: 0.9740\n",
            "Epoch: 2799, Train Loss: 0.9460, Valid Loss: 0.9752\n",
            "Epoch: 2800, Train Loss: 0.9465, Valid Loss: 0.9761\n",
            "Epoch: 2801, Train Loss: 0.9451, Valid Loss: 0.9744\n",
            "Epoch: 2802, Train Loss: 0.9460, Valid Loss: 0.9767\n",
            "Epoch: 2803, Train Loss: 0.9461, Valid Loss: 0.9744\n",
            "Epoch: 2804, Train Loss: 0.9460, Valid Loss: 0.9755\n",
            "Epoch: 2805, Train Loss: 0.9458, Valid Loss: 0.9767\n",
            "Epoch: 2806, Train Loss: 0.9460, Valid Loss: 0.9744\n",
            "Epoch: 2807, Train Loss: 0.9463, Valid Loss: 0.9740\n",
            "Epoch: 2808, Train Loss: 0.9460, Valid Loss: 0.9765\n",
            "Epoch: 2809, Train Loss: 0.9471, Valid Loss: 0.9751\n",
            "Epoch: 2810, Train Loss: 0.9467, Valid Loss: 0.9739\n",
            "Epoch: 2811, Train Loss: 0.9447, Valid Loss: 0.9768\n",
            "Epoch: 2812, Train Loss: 0.9448, Valid Loss: 0.9769\n",
            "Epoch: 2813, Train Loss: 0.9462, Valid Loss: 0.9749\n",
            "Epoch: 2814, Train Loss: 0.9464, Valid Loss: 0.9743\n",
            "Epoch: 2815, Train Loss: 0.9460, Valid Loss: 0.9755\n",
            "Epoch: 2816, Train Loss: 0.9451, Valid Loss: 0.9776\n",
            "Epoch: 2817, Train Loss: 0.9466, Valid Loss: 0.9740\n",
            "Epoch: 2818, Train Loss: 0.9471, Valid Loss: 0.9759\n",
            "Epoch: 2819, Train Loss: 0.9444, Valid Loss: 0.9762\n",
            "Epoch: 2820, Train Loss: 0.9459, Valid Loss: 0.9743\n",
            "Epoch: 2821, Train Loss: 0.9462, Valid Loss: 0.9759\n",
            "Epoch: 2822, Train Loss: 0.9466, Valid Loss: 0.9744\n",
            "Epoch: 2823, Train Loss: 0.9461, Valid Loss: 0.9758\n",
            "Epoch: 2824, Train Loss: 0.9462, Valid Loss: 0.9745\n",
            "Epoch: 2825, Train Loss: 0.9458, Valid Loss: 0.9753\n",
            "Epoch: 2826, Train Loss: 0.9450, Valid Loss: 0.9763\n",
            "Epoch: 2827, Train Loss: 0.9463, Valid Loss: 0.9764\n",
            "Epoch: 2828, Train Loss: 0.9464, Valid Loss: 0.9737\n",
            "Epoch: 2829, Train Loss: 0.9464, Valid Loss: 0.9763\n",
            "Epoch: 2830, Train Loss: 0.9461, Valid Loss: 0.9756\n",
            "Epoch: 2831, Train Loss: 0.9471, Valid Loss: 0.9752\n",
            "Epoch: 2832, Train Loss: 0.9462, Valid Loss: 0.9750\n",
            "Epoch: 2833, Train Loss: 0.9460, Valid Loss: 0.9762\n",
            "Epoch: 2834, Train Loss: 0.9459, Valid Loss: 0.9749\n",
            "Epoch: 2835, Train Loss: 0.9444, Valid Loss: 0.9760\n",
            "Epoch: 2836, Train Loss: 0.9452, Valid Loss: 0.9753\n",
            "Epoch: 2837, Train Loss: 0.9463, Valid Loss: 0.9753\n",
            "Epoch: 2838, Train Loss: 0.9464, Valid Loss: 0.9746\n",
            "Epoch: 2839, Train Loss: 0.9440, Valid Loss: 0.9757\n",
            "Epoch: 2840, Train Loss: 0.9460, Valid Loss: 0.9756\n",
            "Epoch: 2841, Train Loss: 0.9468, Valid Loss: 0.9752\n",
            "Epoch: 2842, Train Loss: 0.9462, Valid Loss: 0.9748\n",
            "Epoch: 2843, Train Loss: 0.9470, Valid Loss: 0.9761\n",
            "Epoch: 2844, Train Loss: 0.9461, Valid Loss: 0.9762\n",
            "Epoch: 2845, Train Loss: 0.9459, Valid Loss: 0.9747\n",
            "Epoch: 2846, Train Loss: 0.9455, Valid Loss: 0.9748\n",
            "Epoch: 2847, Train Loss: 0.9464, Valid Loss: 0.9755\n",
            "Epoch: 2848, Train Loss: 0.9463, Valid Loss: 0.9754\n",
            "Epoch: 2849, Train Loss: 0.9437, Valid Loss: 0.9755\n",
            "Epoch: 2850, Train Loss: 0.9462, Valid Loss: 0.9748\n",
            "Epoch: 2851, Train Loss: 0.9453, Valid Loss: 0.9759\n",
            "Epoch: 2852, Train Loss: 0.9459, Valid Loss: 0.9748\n",
            "Epoch: 2853, Train Loss: 0.9453, Valid Loss: 0.9754\n",
            "Epoch: 2854, Train Loss: 0.9458, Valid Loss: 0.9741\n",
            "Epoch: 2855, Train Loss: 0.9457, Valid Loss: 0.9761\n",
            "Epoch: 2856, Train Loss: 0.9451, Valid Loss: 0.9762\n",
            "Epoch: 2857, Train Loss: 0.9462, Valid Loss: 0.9749\n",
            "Epoch: 2858, Train Loss: 0.9457, Valid Loss: 0.9744\n",
            "Epoch: 2859, Train Loss: 0.9467, Valid Loss: 0.9764\n",
            "Epoch: 2860, Train Loss: 0.9465, Valid Loss: 0.9758\n",
            "Epoch: 2861, Train Loss: 0.9450, Valid Loss: 0.9737\n",
            "Epoch: 2862, Train Loss: 0.9475, Valid Loss: 0.9769\n",
            "Epoch: 2863, Train Loss: 0.9461, Valid Loss: 0.9752\n",
            "Epoch: 2864, Train Loss: 0.9468, Valid Loss: 0.9736\n",
            "Epoch: 2865, Train Loss: 0.9459, Valid Loss: 0.9761\n",
            "Epoch: 2866, Train Loss: 0.9463, Valid Loss: 0.9758\n",
            "Epoch: 2867, Train Loss: 0.9472, Valid Loss: 0.9740\n",
            "Epoch: 2868, Train Loss: 0.9467, Valid Loss: 0.9764\n",
            "Epoch: 2869, Train Loss: 0.9452, Valid Loss: 0.9744\n",
            "Epoch: 2870, Train Loss: 0.9466, Valid Loss: 0.9744\n",
            "Epoch: 2871, Train Loss: 0.9467, Valid Loss: 0.9761\n",
            "Epoch: 2872, Train Loss: 0.9465, Valid Loss: 0.9760\n",
            "Epoch: 2873, Train Loss: 0.9456, Valid Loss: 0.9758\n",
            "Epoch: 2874, Train Loss: 0.9460, Valid Loss: 0.9742\n",
            "Epoch: 2875, Train Loss: 0.9460, Valid Loss: 0.9753\n",
            "Epoch: 2876, Train Loss: 0.9464, Valid Loss: 0.9747\n",
            "Epoch: 2877, Train Loss: 0.9458, Valid Loss: 0.9762\n",
            "Epoch: 2878, Train Loss: 0.9465, Valid Loss: 0.9749\n",
            "Epoch: 2879, Train Loss: 0.9462, Valid Loss: 0.9756\n",
            "Epoch: 2880, Train Loss: 0.9460, Valid Loss: 0.9749\n",
            "Epoch: 2881, Train Loss: 0.9465, Valid Loss: 0.9753\n",
            "Epoch: 2882, Train Loss: 0.9463, Valid Loss: 0.9747\n",
            "Epoch: 2883, Train Loss: 0.9467, Valid Loss: 0.9741\n",
            "Epoch: 2884, Train Loss: 0.9462, Valid Loss: 0.9767\n",
            "Epoch: 2885, Train Loss: 0.9467, Valid Loss: 0.9756\n",
            "Epoch: 2886, Train Loss: 0.9456, Valid Loss: 0.9737\n",
            "Epoch: 2887, Train Loss: 0.9457, Valid Loss: 0.9757\n",
            "Epoch: 2888, Train Loss: 0.9461, Valid Loss: 0.9772\n",
            "Epoch: 2889, Train Loss: 0.9472, Valid Loss: 0.9737\n",
            "Epoch: 2890, Train Loss: 0.9445, Valid Loss: 0.9754\n",
            "Epoch: 2891, Train Loss: 0.9454, Valid Loss: 0.9758\n",
            "Epoch: 2892, Train Loss: 0.9452, Valid Loss: 0.9753\n",
            "Epoch: 2893, Train Loss: 0.9437, Valid Loss: 0.9752\n",
            "Epoch: 2894, Train Loss: 0.9463, Valid Loss: 0.9751\n",
            "Epoch: 2895, Train Loss: 0.9461, Valid Loss: 0.9745\n",
            "Epoch: 2896, Train Loss: 0.9453, Valid Loss: 0.9753\n",
            "Epoch: 2897, Train Loss: 0.9448, Valid Loss: 0.9746\n",
            "Epoch: 2898, Train Loss: 0.9462, Valid Loss: 0.9748\n",
            "Epoch: 2899, Train Loss: 0.9467, Valid Loss: 0.9771\n",
            "Epoch: 2900, Train Loss: 0.9470, Valid Loss: 0.9761\n",
            "Epoch: 2901, Train Loss: 0.9461, Valid Loss: 0.9750\n",
            "Epoch: 2902, Train Loss: 0.9464, Valid Loss: 0.9744\n",
            "Epoch: 2903, Train Loss: 0.9451, Valid Loss: 0.9748\n",
            "Epoch: 2904, Train Loss: 0.9464, Valid Loss: 0.9753\n",
            "Epoch: 2905, Train Loss: 0.9467, Valid Loss: 0.9764\n",
            "Epoch: 2906, Train Loss: 0.9460, Valid Loss: 0.9753\n",
            "Epoch: 2907, Train Loss: 0.9466, Valid Loss: 0.9754\n",
            "Epoch: 2908, Train Loss: 0.9471, Valid Loss: 0.9750\n",
            "Epoch: 2909, Train Loss: 0.9454, Valid Loss: 0.9752\n",
            "Epoch: 2910, Train Loss: 0.9436, Valid Loss: 0.9752\n",
            "Epoch: 2911, Train Loss: 0.9465, Valid Loss: 0.9747\n",
            "Epoch: 2912, Train Loss: 0.9466, Valid Loss: 0.9766\n",
            "Epoch: 2913, Train Loss: 0.9466, Valid Loss: 0.9754\n",
            "Epoch: 2914, Train Loss: 0.9453, Valid Loss: 0.9747\n",
            "Epoch: 2915, Train Loss: 0.9461, Valid Loss: 0.9761\n",
            "Epoch: 2916, Train Loss: 0.9469, Valid Loss: 0.9752\n",
            "Epoch: 2917, Train Loss: 0.9467, Valid Loss: 0.9744\n",
            "Epoch: 2918, Train Loss: 0.9465, Valid Loss: 0.9756\n",
            "Epoch: 2919, Train Loss: 0.9454, Valid Loss: 0.9753\n",
            "Epoch: 2920, Train Loss: 0.9449, Valid Loss: 0.9760\n",
            "Epoch: 2921, Train Loss: 0.9468, Valid Loss: 0.9747\n",
            "Epoch: 2922, Train Loss: 0.9470, Valid Loss: 0.9760\n",
            "Epoch: 2923, Train Loss: 0.9467, Valid Loss: 0.9748\n",
            "Epoch: 2924, Train Loss: 0.9456, Valid Loss: 0.9751\n",
            "Epoch: 2925, Train Loss: 0.9456, Valid Loss: 0.9751\n",
            "Epoch: 2926, Train Loss: 0.9462, Valid Loss: 0.9750\n",
            "Epoch: 2927, Train Loss: 0.9457, Valid Loss: 0.9760\n",
            "Epoch: 2928, Train Loss: 0.9456, Valid Loss: 0.9759\n",
            "Epoch: 2929, Train Loss: 0.9455, Valid Loss: 0.9753\n",
            "Epoch: 2930, Train Loss: 0.9454, Valid Loss: 0.9745\n",
            "Epoch: 2931, Train Loss: 0.9469, Valid Loss: 0.9747\n",
            "Epoch: 2932, Train Loss: 0.9464, Valid Loss: 0.9766\n",
            "Epoch: 2933, Train Loss: 0.9452, Valid Loss: 0.9754\n",
            "Epoch: 2934, Train Loss: 0.9466, Valid Loss: 0.9751\n",
            "Epoch: 2935, Train Loss: 0.9471, Valid Loss: 0.9746\n",
            "Epoch: 2936, Train Loss: 0.9465, Valid Loss: 0.9755\n",
            "Epoch: 2937, Train Loss: 0.9458, Valid Loss: 0.9742\n",
            "Epoch: 2938, Train Loss: 0.9462, Valid Loss: 0.9743\n",
            "Epoch: 2939, Train Loss: 0.9462, Valid Loss: 0.9752\n",
            "Epoch: 2940, Train Loss: 0.9457, Valid Loss: 0.9757\n",
            "Epoch: 2941, Train Loss: 0.9448, Valid Loss: 0.9761\n",
            "Epoch: 2942, Train Loss: 0.9455, Valid Loss: 0.9750\n",
            "Epoch: 2943, Train Loss: 0.9470, Valid Loss: 0.9741\n",
            "Epoch: 2944, Train Loss: 0.9469, Valid Loss: 0.9763\n",
            "Epoch: 2945, Train Loss: 0.9457, Valid Loss: 0.9752\n",
            "Epoch: 2946, Train Loss: 0.9472, Valid Loss: 0.9753\n",
            "Epoch: 2947, Train Loss: 0.9458, Valid Loss: 0.9738\n",
            "Epoch: 2948, Train Loss: 0.9467, Valid Loss: 0.9761\n",
            "Epoch: 2949, Train Loss: 0.9460, Valid Loss: 0.9744\n",
            "Epoch: 2950, Train Loss: 0.9466, Valid Loss: 0.9757\n",
            "Epoch: 2951, Train Loss: 0.9461, Valid Loss: 0.9751\n",
            "Epoch: 2952, Train Loss: 0.9467, Valid Loss: 0.9748\n",
            "Epoch: 2953, Train Loss: 0.9452, Valid Loss: 0.9760\n",
            "Epoch: 2954, Train Loss: 0.9468, Valid Loss: 0.9744\n",
            "Epoch: 2955, Train Loss: 0.9456, Valid Loss: 0.9759\n",
            "Epoch: 2956, Train Loss: 0.9463, Valid Loss: 0.9746\n",
            "Epoch: 2957, Train Loss: 0.9456, Valid Loss: 0.9745\n",
            "Epoch: 2958, Train Loss: 0.9459, Valid Loss: 0.9749\n",
            "Epoch: 2959, Train Loss: 0.9463, Valid Loss: 0.9751\n",
            "Epoch: 2960, Train Loss: 0.9463, Valid Loss: 0.9750\n",
            "Epoch: 2961, Train Loss: 0.9465, Valid Loss: 0.9769\n",
            "Epoch: 2962, Train Loss: 0.9468, Valid Loss: 0.9761\n",
            "Epoch: 2963, Train Loss: 0.9459, Valid Loss: 0.9743\n",
            "Epoch: 2964, Train Loss: 0.9459, Valid Loss: 0.9743\n",
            "Epoch: 2965, Train Loss: 0.9468, Valid Loss: 0.9745\n",
            "Epoch: 2966, Train Loss: 0.9465, Valid Loss: 0.9764\n",
            "Epoch: 2967, Train Loss: 0.9474, Valid Loss: 0.9765\n",
            "Epoch: 2968, Train Loss: 0.9458, Valid Loss: 0.9740\n",
            "Epoch: 2969, Train Loss: 0.9467, Valid Loss: 0.9746\n",
            "Epoch: 2970, Train Loss: 0.9468, Valid Loss: 0.9758\n",
            "Epoch: 2971, Train Loss: 0.9462, Valid Loss: 0.9750\n",
            "Epoch: 2972, Train Loss: 0.9462, Valid Loss: 0.9743\n",
            "Epoch: 2973, Train Loss: 0.9471, Valid Loss: 0.9754\n",
            "Epoch: 2974, Train Loss: 0.9472, Valid Loss: 0.9749\n",
            "Epoch: 2975, Train Loss: 0.9457, Valid Loss: 0.9746\n",
            "Epoch: 2976, Train Loss: 0.9459, Valid Loss: 0.9754\n",
            "Epoch: 2977, Train Loss: 0.9453, Valid Loss: 0.9745\n",
            "Epoch: 2978, Train Loss: 0.9462, Valid Loss: 0.9757\n",
            "Epoch: 2979, Train Loss: 0.9466, Valid Loss: 0.9758\n",
            "Epoch: 2980, Train Loss: 0.9458, Valid Loss: 0.9751\n",
            "Epoch: 2981, Train Loss: 0.9462, Valid Loss: 0.9750\n",
            "Epoch: 2982, Train Loss: 0.9437, Valid Loss: 0.9750\n",
            "Epoch: 2983, Train Loss: 0.9463, Valid Loss: 0.9750\n",
            "Epoch: 2984, Train Loss: 0.9458, Valid Loss: 0.9750\n",
            "Epoch: 2985, Train Loss: 0.9459, Valid Loss: 0.9755\n",
            "Epoch: 2986, Train Loss: 0.9471, Valid Loss: 0.9767\n",
            "Epoch: 2987, Train Loss: 0.9460, Valid Loss: 0.9735\n",
            "Epoch: 2988, Train Loss: 0.9454, Valid Loss: 0.9746\n",
            "Epoch: 2989, Train Loss: 0.9461, Valid Loss: 0.9747\n",
            "Epoch: 2990, Train Loss: 0.9450, Valid Loss: 0.9760\n",
            "Epoch: 2991, Train Loss: 0.9471, Valid Loss: 0.9755\n",
            "Epoch: 2992, Train Loss: 0.9473, Valid Loss: 0.9743\n",
            "Epoch: 2993, Train Loss: 0.9463, Valid Loss: 0.9758\n",
            "Epoch: 2994, Train Loss: 0.9442, Valid Loss: 0.9746\n",
            "Epoch: 2995, Train Loss: 0.9466, Valid Loss: 0.9742\n",
            "Epoch: 2996, Train Loss: 0.9471, Valid Loss: 0.9751\n",
            "Epoch: 2997, Train Loss: 0.9451, Valid Loss: 0.9753\n",
            "Epoch: 2998, Train Loss: 0.9466, Valid Loss: 0.9752\n",
            "Epoch: 2999, Train Loss: 0.9462, Valid Loss: 0.9745\n",
            "Epoch: 3000, Train Loss: 0.9464, Valid Loss: 0.9752\n",
            "Epoch: 3001, Train Loss: 0.9466, Valid Loss: 0.9749\n",
            "Epoch: 3002, Train Loss: 0.9465, Valid Loss: 0.9751\n",
            "Epoch: 3003, Train Loss: 0.9463, Valid Loss: 0.9758\n",
            "Epoch: 3004, Train Loss: 0.9446, Valid Loss: 0.9750\n",
            "Epoch: 3005, Train Loss: 0.9465, Valid Loss: 0.9737\n",
            "Epoch: 3006, Train Loss: 0.9468, Valid Loss: 0.9763\n",
            "Epoch: 3007, Train Loss: 0.9460, Valid Loss: 0.9751\n",
            "Epoch: 3008, Train Loss: 0.9473, Valid Loss: 0.9749\n",
            "Epoch: 3009, Train Loss: 0.9466, Valid Loss: 0.9753\n",
            "Epoch: 3010, Train Loss: 0.9458, Valid Loss: 0.9750\n",
            "Epoch: 3011, Train Loss: 0.9462, Valid Loss: 0.9762\n",
            "Epoch: 3012, Train Loss: 0.9460, Valid Loss: 0.9744\n",
            "Epoch: 3013, Train Loss: 0.9464, Valid Loss: 0.9747\n",
            "Epoch: 3014, Train Loss: 0.9454, Valid Loss: 0.9765\n",
            "Epoch: 3015, Train Loss: 0.9465, Valid Loss: 0.9760\n",
            "Epoch: 3016, Train Loss: 0.9467, Valid Loss: 0.9746\n",
            "Epoch: 3017, Train Loss: 0.9466, Valid Loss: 0.9752\n",
            "Epoch: 3018, Train Loss: 0.9460, Valid Loss: 0.9754\n",
            "Epoch: 3019, Train Loss: 0.9454, Valid Loss: 0.9742\n",
            "Epoch: 3020, Train Loss: 0.9465, Valid Loss: 0.9760\n",
            "Epoch: 3021, Train Loss: 0.9467, Valid Loss: 0.9743\n",
            "Epoch: 3022, Train Loss: 0.9462, Valid Loss: 0.9748\n",
            "Epoch: 3023, Train Loss: 0.9454, Valid Loss: 0.9763\n",
            "Epoch: 3024, Train Loss: 0.9463, Valid Loss: 0.9751\n",
            "Epoch: 3025, Train Loss: 0.9468, Valid Loss: 0.9752\n",
            "Epoch: 3026, Train Loss: 0.9468, Valid Loss: 0.9754\n",
            "Epoch: 3027, Train Loss: 0.9458, Valid Loss: 0.9740\n",
            "Epoch: 3028, Train Loss: 0.9468, Valid Loss: 0.9744\n",
            "Epoch: 3029, Train Loss: 0.9464, Valid Loss: 0.9761\n",
            "Epoch: 3030, Train Loss: 0.9475, Valid Loss: 0.9763\n",
            "Epoch: 3031, Train Loss: 0.9463, Valid Loss: 0.9747\n",
            "Epoch: 3032, Train Loss: 0.9464, Valid Loss: 0.9748\n",
            "Epoch: 3033, Train Loss: 0.9462, Valid Loss: 0.9738\n",
            "Epoch: 3034, Train Loss: 0.9460, Valid Loss: 0.9760\n",
            "Epoch: 3035, Train Loss: 0.9437, Valid Loss: 0.9766\n",
            "Epoch: 3036, Train Loss: 0.9469, Valid Loss: 0.9749\n",
            "Epoch: 3037, Train Loss: 0.9464, Valid Loss: 0.9748\n",
            "Epoch: 3038, Train Loss: 0.9457, Valid Loss: 0.9739\n",
            "Epoch: 3039, Train Loss: 0.9463, Valid Loss: 0.9764\n",
            "Epoch: 3040, Train Loss: 0.9472, Valid Loss: 0.9756\n",
            "Epoch: 3041, Train Loss: 0.9449, Valid Loss: 0.9757\n",
            "Epoch: 3042, Train Loss: 0.9459, Valid Loss: 0.9740\n",
            "Epoch: 3043, Train Loss: 0.9460, Valid Loss: 0.9758\n",
            "Epoch: 3044, Train Loss: 0.9442, Valid Loss: 0.9740\n",
            "Epoch: 3045, Train Loss: 0.9467, Valid Loss: 0.9753\n",
            "Epoch: 3046, Train Loss: 0.9460, Valid Loss: 0.9756\n",
            "Epoch: 3047, Train Loss: 0.9473, Valid Loss: 0.9750\n",
            "Epoch: 3048, Train Loss: 0.9467, Valid Loss: 0.9750\n",
            "Epoch: 3049, Train Loss: 0.9452, Valid Loss: 0.9764\n",
            "Epoch: 3050, Train Loss: 0.9462, Valid Loss: 0.9756\n",
            "Epoch: 3051, Train Loss: 0.9463, Valid Loss: 0.9746\n",
            "Epoch: 3052, Train Loss: 0.9457, Valid Loss: 0.9760\n",
            "Epoch: 3053, Train Loss: 0.9462, Valid Loss: 0.9750\n",
            "Epoch: 3054, Train Loss: 0.9468, Valid Loss: 0.9737\n",
            "Epoch: 3055, Train Loss: 0.9466, Valid Loss: 0.9756\n",
            "Epoch: 3056, Train Loss: 0.9471, Valid Loss: 0.9750\n",
            "Epoch: 3057, Train Loss: 0.9450, Valid Loss: 0.9750\n",
            "Epoch: 3058, Train Loss: 0.9475, Valid Loss: 0.9756\n",
            "Epoch: 3059, Train Loss: 0.9468, Valid Loss: 0.9753\n",
            "Epoch: 3060, Train Loss: 0.9465, Valid Loss: 0.9758\n",
            "Epoch: 3061, Train Loss: 0.9465, Valid Loss: 0.9743\n",
            "Epoch: 3062, Train Loss: 0.9433, Valid Loss: 0.9754\n",
            "Epoch: 3063, Train Loss: 0.9474, Valid Loss: 0.9756\n",
            "Epoch: 3064, Train Loss: 0.9460, Valid Loss: 0.9747\n",
            "Epoch: 3065, Train Loss: 0.9468, Valid Loss: 0.9764\n",
            "Epoch: 3066, Train Loss: 0.9466, Valid Loss: 0.9746\n",
            "Epoch: 3067, Train Loss: 0.9451, Valid Loss: 0.9747\n",
            "Epoch: 3068, Train Loss: 0.9467, Valid Loss: 0.9751\n",
            "Epoch: 3069, Train Loss: 0.9450, Valid Loss: 0.9749\n",
            "Epoch: 3070, Train Loss: 0.9469, Valid Loss: 0.9745\n",
            "Epoch: 3071, Train Loss: 0.9464, Valid Loss: 0.9747\n",
            "Epoch: 3072, Train Loss: 0.9471, Valid Loss: 0.9752\n",
            "Epoch: 3073, Train Loss: 0.9471, Valid Loss: 0.9765\n",
            "Epoch: 3074, Train Loss: 0.9454, Valid Loss: 0.9737\n",
            "Epoch: 3075, Train Loss: 0.9472, Valid Loss: 0.9769\n",
            "Epoch: 3076, Train Loss: 0.9451, Valid Loss: 0.9743\n",
            "Epoch: 3077, Train Loss: 0.9478, Valid Loss: 0.9734\n",
            "Epoch: 3078, Train Loss: 0.9463, Valid Loss: 0.9766\n",
            "Epoch: 3079, Train Loss: 0.9461, Valid Loss: 0.9761\n",
            "Epoch: 3080, Train Loss: 0.9466, Valid Loss: 0.9754\n",
            "Epoch: 3081, Train Loss: 0.9453, Valid Loss: 0.9741\n",
            "Epoch: 3082, Train Loss: 0.9471, Valid Loss: 0.9745\n",
            "Epoch: 3083, Train Loss: 0.9467, Valid Loss: 0.9752\n",
            "Epoch: 3084, Train Loss: 0.9455, Valid Loss: 0.9765\n",
            "Epoch: 3085, Train Loss: 0.9469, Valid Loss: 0.9751\n",
            "Epoch: 3086, Train Loss: 0.9456, Valid Loss: 0.9747\n",
            "Epoch: 3087, Train Loss: 0.9465, Valid Loss: 0.9750\n",
            "Epoch: 3088, Train Loss: 0.9465, Valid Loss: 0.9756\n",
            "Epoch: 3089, Train Loss: 0.9467, Valid Loss: 0.9753\n",
            "Epoch: 3090, Train Loss: 0.9460, Valid Loss: 0.9736\n",
            "Epoch: 3091, Train Loss: 0.9472, Valid Loss: 0.9762\n",
            "Epoch: 3092, Train Loss: 0.9465, Valid Loss: 0.9765\n",
            "Epoch: 3093, Train Loss: 0.9458, Valid Loss: 0.9751\n",
            "Epoch: 3094, Train Loss: 0.9461, Valid Loss: 0.9743\n",
            "Epoch: 3095, Train Loss: 0.9470, Valid Loss: 0.9749\n",
            "Epoch: 3096, Train Loss: 0.9473, Valid Loss: 0.9765\n",
            "Epoch: 3097, Train Loss: 0.9452, Valid Loss: 0.9740\n",
            "Epoch: 3098, Train Loss: 0.9456, Valid Loss: 0.9748\n",
            "Epoch: 3099, Train Loss: 0.9469, Valid Loss: 0.9760\n",
            "Epoch: 3100, Train Loss: 0.9464, Valid Loss: 0.9747\n",
            "Epoch: 3101, Train Loss: 0.9463, Valid Loss: 0.9744\n",
            "Epoch: 3102, Train Loss: 0.9475, Valid Loss: 0.9758\n",
            "Epoch: 3103, Train Loss: 0.9475, Valid Loss: 0.9749\n",
            "Epoch: 3104, Train Loss: 0.9470, Valid Loss: 0.9768\n",
            "Epoch: 3105, Train Loss: 0.9467, Valid Loss: 0.9748\n",
            "Epoch: 3106, Train Loss: 0.9470, Valid Loss: 0.9756\n",
            "Epoch: 3107, Train Loss: 0.9474, Valid Loss: 0.9741\n",
            "Epoch: 3108, Train Loss: 0.9472, Valid Loss: 0.9746\n",
            "Epoch: 3109, Train Loss: 0.9469, Valid Loss: 0.9763\n",
            "Epoch: 3110, Train Loss: 0.9469, Valid Loss: 0.9753\n",
            "Epoch: 3111, Train Loss: 0.9464, Valid Loss: 0.9756\n",
            "Epoch: 3112, Train Loss: 0.9467, Valid Loss: 0.9759\n",
            "Epoch: 3113, Train Loss: 0.9471, Valid Loss: 0.9740\n",
            "Epoch: 3114, Train Loss: 0.9456, Valid Loss: 0.9742\n",
            "Epoch: 3115, Train Loss: 0.9478, Valid Loss: 0.9765\n",
            "Epoch: 3116, Train Loss: 0.9461, Valid Loss: 0.9749\n",
            "Epoch: 3117, Train Loss: 0.9466, Valid Loss: 0.9736\n",
            "Epoch: 3118, Train Loss: 0.9458, Valid Loss: 0.9753\n",
            "Epoch: 3119, Train Loss: 0.9464, Valid Loss: 0.9755\n",
            "Epoch: 3120, Train Loss: 0.9470, Valid Loss: 0.9750\n",
            "Epoch: 3121, Train Loss: 0.9459, Valid Loss: 0.9755\n",
            "Epoch: 3122, Train Loss: 0.9471, Valid Loss: 0.9759\n",
            "Epoch: 3123, Train Loss: 0.9471, Valid Loss: 0.9743\n",
            "Epoch: 3124, Train Loss: 0.9469, Valid Loss: 0.9756\n",
            "Epoch: 3125, Train Loss: 0.9463, Valid Loss: 0.9741\n",
            "Epoch: 3126, Train Loss: 0.9451, Valid Loss: 0.9761\n",
            "Epoch: 3127, Train Loss: 0.9464, Valid Loss: 0.9751\n",
            "Epoch: 3128, Train Loss: 0.9460, Valid Loss: 0.9759\n",
            "Epoch: 3129, Train Loss: 0.9470, Valid Loss: 0.9751\n",
            "Epoch: 3130, Train Loss: 0.9459, Valid Loss: 0.9748\n",
            "Epoch: 3131, Train Loss: 0.9457, Valid Loss: 0.9751\n",
            "Epoch: 3132, Train Loss: 0.9472, Valid Loss: 0.9753\n",
            "Epoch: 3133, Train Loss: 0.9471, Valid Loss: 0.9755\n",
            "Epoch: 3134, Train Loss: 0.9465, Valid Loss: 0.9745\n",
            "Epoch: 3135, Train Loss: 0.9476, Valid Loss: 0.9753\n",
            "Epoch: 3136, Train Loss: 0.9466, Valid Loss: 0.9747\n",
            "Epoch: 3137, Train Loss: 0.9454, Valid Loss: 0.9747\n",
            "Epoch: 3138, Train Loss: 0.9466, Valid Loss: 0.9756\n",
            "Epoch: 3139, Train Loss: 0.9461, Valid Loss: 0.9754\n",
            "Epoch: 3140, Train Loss: 0.9464, Valid Loss: 0.9762\n",
            "Epoch: 3141, Train Loss: 0.9471, Valid Loss: 0.9742\n",
            "Epoch: 3142, Train Loss: 0.9471, Valid Loss: 0.9753\n",
            "Epoch: 3143, Train Loss: 0.9462, Valid Loss: 0.9750\n",
            "Epoch: 3144, Train Loss: 0.9466, Valid Loss: 0.9742\n",
            "Epoch: 3145, Train Loss: 0.9460, Valid Loss: 0.9755\n",
            "Epoch: 3146, Train Loss: 0.9468, Valid Loss: 0.9761\n",
            "Epoch: 3147, Train Loss: 0.9481, Valid Loss: 0.9737\n",
            "Epoch: 3148, Train Loss: 0.9467, Valid Loss: 0.9751\n",
            "Epoch: 3149, Train Loss: 0.9473, Valid Loss: 0.9750\n",
            "Epoch: 3150, Train Loss: 0.9460, Valid Loss: 0.9758\n",
            "Epoch: 3151, Train Loss: 0.9470, Valid Loss: 0.9743\n",
            "Epoch: 3152, Train Loss: 0.9471, Valid Loss: 0.9749\n",
            "Epoch: 3153, Train Loss: 0.9459, Valid Loss: 0.9758\n",
            "Epoch: 3154, Train Loss: 0.9456, Valid Loss: 0.9745\n",
            "Epoch: 3155, Train Loss: 0.9465, Valid Loss: 0.9764\n",
            "Epoch: 3156, Train Loss: 0.9472, Valid Loss: 0.9746\n",
            "Epoch: 3157, Train Loss: 0.9466, Valid Loss: 0.9742\n",
            "Epoch: 3158, Train Loss: 0.9468, Valid Loss: 0.9752\n",
            "Epoch: 3159, Train Loss: 0.9473, Valid Loss: 0.9746\n",
            "Epoch: 3160, Train Loss: 0.9464, Valid Loss: 0.9765\n",
            "Epoch: 3161, Train Loss: 0.9478, Valid Loss: 0.9756\n",
            "Epoch: 3162, Train Loss: 0.9473, Valid Loss: 0.9734\n",
            "Epoch: 3163, Train Loss: 0.9442, Valid Loss: 0.9746\n",
            "Epoch: 3164, Train Loss: 0.9476, Valid Loss: 0.9761\n",
            "Epoch: 3165, Train Loss: 0.9468, Valid Loss: 0.9754\n",
            "Epoch: 3166, Train Loss: 0.9474, Valid Loss: 0.9753\n",
            "Epoch: 3167, Train Loss: 0.9466, Valid Loss: 0.9751\n",
            "Epoch: 3168, Train Loss: 0.9464, Valid Loss: 0.9738\n",
            "Epoch: 3169, Train Loss: 0.9463, Valid Loss: 0.9753\n",
            "Epoch: 3170, Train Loss: 0.9466, Valid Loss: 0.9760\n",
            "Epoch: 3171, Train Loss: 0.9457, Valid Loss: 0.9750\n",
            "Epoch: 3172, Train Loss: 0.9464, Valid Loss: 0.9739\n",
            "Epoch: 3173, Train Loss: 0.9464, Valid Loss: 0.9745\n",
            "Epoch: 3174, Train Loss: 0.9477, Valid Loss: 0.9772\n",
            "Epoch: 3175, Train Loss: 0.9476, Valid Loss: 0.9752\n",
            "Epoch: 3176, Train Loss: 0.9470, Valid Loss: 0.9748\n",
            "Epoch: 3177, Train Loss: 0.9472, Valid Loss: 0.9747\n",
            "Epoch: 3178, Train Loss: 0.9457, Valid Loss: 0.9751\n",
            "Epoch: 3179, Train Loss: 0.9466, Valid Loss: 0.9757\n",
            "Epoch: 3180, Train Loss: 0.9480, Valid Loss: 0.9774\n",
            "Epoch: 3181, Train Loss: 0.9462, Valid Loss: 0.9744\n",
            "Epoch: 3182, Train Loss: 0.9477, Valid Loss: 0.9734\n",
            "Epoch: 3183, Train Loss: 0.9470, Valid Loss: 0.9762\n",
            "Epoch: 3184, Train Loss: 0.9464, Valid Loss: 0.9755\n",
            "Epoch: 3185, Train Loss: 0.9461, Valid Loss: 0.9743\n",
            "Epoch: 3186, Train Loss: 0.9466, Valid Loss: 0.9758\n",
            "Epoch: 3187, Train Loss: 0.9463, Valid Loss: 0.9749\n",
            "Epoch: 3188, Train Loss: 0.9475, Valid Loss: 0.9755\n",
            "Epoch: 3189, Train Loss: 0.9469, Valid Loss: 0.9743\n",
            "Epoch: 3190, Train Loss: 0.9439, Valid Loss: 0.9762\n",
            "Epoch: 3191, Train Loss: 0.9452, Valid Loss: 0.9765\n",
            "Epoch: 3192, Train Loss: 0.9465, Valid Loss: 0.9756\n",
            "Epoch: 3193, Train Loss: 0.9477, Valid Loss: 0.9733\n",
            "Epoch: 3194, Train Loss: 0.9467, Valid Loss: 0.9751\n",
            "Epoch: 3195, Train Loss: 0.9455, Valid Loss: 0.9768\n",
            "Epoch: 3196, Train Loss: 0.9473, Valid Loss: 0.9739\n",
            "Epoch: 3197, Train Loss: 0.9470, Valid Loss: 0.9746\n",
            "Epoch: 3198, Train Loss: 0.9462, Valid Loss: 0.9763\n",
            "Epoch: 3199, Train Loss: 0.9479, Valid Loss: 0.9746\n",
            "Epoch: 3200, Train Loss: 0.9483, Valid Loss: 0.9764\n",
            "Epoch: 3201, Train Loss: 0.9458, Valid Loss: 0.9747\n",
            "Epoch: 3202, Train Loss: 0.9473, Valid Loss: 0.9740\n",
            "Epoch: 3203, Train Loss: 0.9476, Valid Loss: 0.9745\n",
            "Epoch: 3204, Train Loss: 0.9465, Valid Loss: 0.9764\n",
            "Epoch: 3205, Train Loss: 0.9465, Valid Loss: 0.9755\n",
            "Epoch: 3206, Train Loss: 0.9475, Valid Loss: 0.9760\n",
            "Epoch: 3207, Train Loss: 0.9454, Valid Loss: 0.9741\n",
            "Epoch: 3208, Train Loss: 0.9464, Valid Loss: 0.9748\n",
            "Epoch: 3209, Train Loss: 0.9464, Valid Loss: 0.9745\n",
            "Epoch: 3210, Train Loss: 0.9477, Valid Loss: 0.9752\n",
            "Epoch: 3211, Train Loss: 0.9459, Valid Loss: 0.9753\n",
            "Epoch: 3212, Train Loss: 0.9462, Valid Loss: 0.9761\n",
            "Epoch: 3213, Train Loss: 0.9470, Valid Loss: 0.9756\n",
            "Epoch: 3214, Train Loss: 0.9464, Valid Loss: 0.9741\n",
            "Epoch: 3215, Train Loss: 0.9460, Valid Loss: 0.9750\n",
            "Epoch: 3216, Train Loss: 0.9479, Valid Loss: 0.9743\n",
            "Epoch: 3217, Train Loss: 0.9468, Valid Loss: 0.9758\n",
            "Epoch: 3218, Train Loss: 0.9474, Valid Loss: 0.9749\n",
            "Epoch: 3219, Train Loss: 0.9458, Valid Loss: 0.9747\n",
            "Epoch: 3220, Train Loss: 0.9469, Valid Loss: 0.9754\n",
            "Epoch: 3221, Train Loss: 0.9472, Valid Loss: 0.9747\n",
            "Epoch: 3222, Train Loss: 0.9456, Valid Loss: 0.9763\n",
            "Epoch: 3223, Train Loss: 0.9465, Valid Loss: 0.9750\n",
            "Epoch: 3224, Train Loss: 0.9468, Valid Loss: 0.9740\n",
            "Epoch: 3225, Train Loss: 0.9465, Valid Loss: 0.9760\n",
            "Epoch: 3226, Train Loss: 0.9471, Valid Loss: 0.9755\n",
            "Epoch: 3227, Train Loss: 0.9473, Valid Loss: 0.9756\n",
            "Epoch: 3228, Train Loss: 0.9459, Valid Loss: 0.9745\n",
            "Epoch: 3229, Train Loss: 0.9469, Valid Loss: 0.9743\n",
            "Epoch: 3230, Train Loss: 0.9468, Valid Loss: 0.9751\n",
            "Epoch: 3231, Train Loss: 0.9471, Valid Loss: 0.9750\n",
            "Epoch: 3232, Train Loss: 0.9461, Valid Loss: 0.9751\n",
            "Epoch: 3233, Train Loss: 0.9465, Valid Loss: 0.9751\n",
            "Epoch: 3234, Train Loss: 0.9470, Valid Loss: 0.9744\n",
            "Epoch: 3235, Train Loss: 0.9472, Valid Loss: 0.9760\n",
            "Epoch: 3236, Train Loss: 0.9470, Valid Loss: 0.9749\n",
            "Epoch: 3237, Train Loss: 0.9474, Valid Loss: 0.9752\n",
            "Epoch: 3238, Train Loss: 0.9463, Valid Loss: 0.9749\n",
            "Epoch: 3239, Train Loss: 0.9460, Valid Loss: 0.9744\n",
            "Epoch: 3240, Train Loss: 0.9475, Valid Loss: 0.9753\n",
            "Epoch: 3241, Train Loss: 0.9457, Valid Loss: 0.9758\n",
            "Epoch: 3242, Train Loss: 0.9473, Valid Loss: 0.9767\n",
            "Epoch: 3243, Train Loss: 0.9469, Valid Loss: 0.9749\n",
            "Epoch: 3244, Train Loss: 0.9468, Valid Loss: 0.9748\n",
            "Epoch: 3245, Train Loss: 0.9474, Valid Loss: 0.9739\n",
            "Epoch: 3246, Train Loss: 0.9470, Valid Loss: 0.9759\n",
            "Epoch: 3247, Train Loss: 0.9468, Valid Loss: 0.9749\n",
            "Epoch: 3248, Train Loss: 0.9470, Valid Loss: 0.9747\n",
            "Epoch: 3249, Train Loss: 0.9466, Valid Loss: 0.9755\n",
            "Epoch: 3250, Train Loss: 0.9470, Valid Loss: 0.9749\n",
            "Epoch: 3251, Train Loss: 0.9473, Valid Loss: 0.9757\n",
            "Epoch: 3252, Train Loss: 0.9466, Valid Loss: 0.9751\n",
            "Epoch: 3253, Train Loss: 0.9474, Valid Loss: 0.9747\n",
            "Epoch: 3254, Train Loss: 0.9462, Valid Loss: 0.9760\n",
            "Epoch: 3255, Train Loss: 0.9470, Valid Loss: 0.9748\n",
            "Epoch: 3256, Train Loss: 0.9470, Valid Loss: 0.9753\n",
            "Epoch: 3257, Train Loss: 0.9470, Valid Loss: 0.9751\n",
            "Epoch: 3258, Train Loss: 0.9464, Valid Loss: 0.9749\n",
            "Epoch: 3259, Train Loss: 0.9461, Valid Loss: 0.9744\n",
            "Epoch: 3260, Train Loss: 0.9470, Valid Loss: 0.9756\n",
            "Epoch: 3261, Train Loss: 0.9475, Valid Loss: 0.9770\n",
            "Epoch: 3262, Train Loss: 0.9464, Valid Loss: 0.9757\n",
            "Epoch: 3263, Train Loss: 0.9475, Valid Loss: 0.9741\n",
            "Epoch: 3264, Train Loss: 0.9473, Valid Loss: 0.9750\n",
            "Epoch: 3265, Train Loss: 0.9476, Valid Loss: 0.9756\n",
            "Epoch: 3266, Train Loss: 0.9475, Valid Loss: 0.9738\n",
            "Epoch: 3267, Train Loss: 0.9472, Valid Loss: 0.9739\n",
            "Epoch: 3268, Train Loss: 0.9473, Valid Loss: 0.9762\n",
            "Epoch: 3269, Train Loss: 0.9481, Valid Loss: 0.9752\n",
            "Epoch: 3270, Train Loss: 0.9460, Valid Loss: 0.9754\n",
            "Epoch: 3271, Train Loss: 0.9468, Valid Loss: 0.9746\n",
            "Epoch: 3272, Train Loss: 0.9452, Valid Loss: 0.9763\n",
            "Epoch: 3273, Train Loss: 0.9471, Valid Loss: 0.9748\n",
            "Epoch: 3274, Train Loss: 0.9468, Valid Loss: 0.9748\n",
            "Epoch: 3275, Train Loss: 0.9471, Valid Loss: 0.9759\n",
            "Epoch: 3276, Train Loss: 0.9470, Valid Loss: 0.9746\n",
            "Epoch: 3277, Train Loss: 0.9465, Valid Loss: 0.9743\n",
            "Epoch: 3278, Train Loss: 0.9476, Valid Loss: 0.9748\n",
            "Epoch: 3279, Train Loss: 0.9479, Valid Loss: 0.9758\n",
            "Epoch: 3280, Train Loss: 0.9480, Valid Loss: 0.9731\n",
            "Epoch: 3281, Train Loss: 0.9464, Valid Loss: 0.9749\n",
            "Epoch: 3282, Train Loss: 0.9460, Valid Loss: 0.9748\n",
            "Epoch: 3283, Train Loss: 0.9447, Valid Loss: 0.9755\n",
            "Epoch: 3284, Train Loss: 0.9467, Valid Loss: 0.9762\n",
            "Epoch: 3285, Train Loss: 0.9456, Valid Loss: 0.9749\n",
            "Epoch: 3286, Train Loss: 0.9473, Valid Loss: 0.9728\n",
            "Epoch: 3287, Train Loss: 0.9461, Valid Loss: 0.9769\n",
            "Epoch: 3288, Train Loss: 0.9463, Valid Loss: 0.9764\n",
            "Epoch: 3289, Train Loss: 0.9475, Valid Loss: 0.9742\n",
            "Epoch: 3290, Train Loss: 0.9464, Valid Loss: 0.9735\n",
            "Epoch: 3291, Train Loss: 0.9464, Valid Loss: 0.9765\n",
            "Epoch: 3292, Train Loss: 0.9471, Valid Loss: 0.9757\n",
            "Epoch: 3293, Train Loss: 0.9472, Valid Loss: 0.9742\n",
            "Epoch: 3294, Train Loss: 0.9460, Valid Loss: 0.9745\n",
            "Epoch: 3295, Train Loss: 0.9475, Valid Loss: 0.9758\n",
            "Epoch: 3296, Train Loss: 0.9470, Valid Loss: 0.9745\n",
            "Epoch: 3297, Train Loss: 0.9473, Valid Loss: 0.9757\n",
            "Epoch: 3298, Train Loss: 0.9473, Valid Loss: 0.9749\n",
            "Epoch: 3299, Train Loss: 0.9470, Valid Loss: 0.9748\n",
            "Epoch: 3300, Train Loss: 0.9478, Valid Loss: 0.9744\n",
            "Epoch: 3301, Train Loss: 0.9463, Valid Loss: 0.9750\n",
            "Epoch: 3302, Train Loss: 0.9469, Valid Loss: 0.9763\n",
            "Epoch: 3303, Train Loss: 0.9466, Valid Loss: 0.9751\n",
            "Epoch: 3304, Train Loss: 0.9470, Valid Loss: 0.9739\n",
            "Epoch: 3305, Train Loss: 0.9472, Valid Loss: 0.9747\n",
            "Epoch: 3306, Train Loss: 0.9468, Valid Loss: 0.9756\n",
            "Epoch: 3307, Train Loss: 0.9465, Valid Loss: 0.9761\n",
            "Epoch: 3308, Train Loss: 0.9474, Valid Loss: 0.9750\n",
            "Epoch: 3309, Train Loss: 0.9452, Valid Loss: 0.9748\n",
            "Epoch: 3310, Train Loss: 0.9476, Valid Loss: 0.9763\n",
            "Epoch: 3311, Train Loss: 0.9462, Valid Loss: 0.9747\n",
            "Epoch: 3312, Train Loss: 0.9476, Valid Loss: 0.9752\n",
            "Epoch: 3313, Train Loss: 0.9471, Valid Loss: 0.9758\n",
            "Epoch: 3314, Train Loss: 0.9478, Valid Loss: 0.9743\n",
            "Epoch: 3315, Train Loss: 0.9476, Valid Loss: 0.9743\n",
            "Epoch: 3316, Train Loss: 0.9470, Valid Loss: 0.9769\n",
            "Epoch: 3317, Train Loss: 0.9470, Valid Loss: 0.9753\n",
            "Epoch: 3318, Train Loss: 0.9460, Valid Loss: 0.9735\n",
            "Epoch: 3319, Train Loss: 0.9472, Valid Loss: 0.9767\n",
            "Epoch: 3320, Train Loss: 0.9474, Valid Loss: 0.9755\n",
            "Epoch: 3321, Train Loss: 0.9474, Valid Loss: 0.9747\n",
            "Epoch: 3322, Train Loss: 0.9460, Valid Loss: 0.9756\n",
            "Epoch: 3323, Train Loss: 0.9473, Valid Loss: 0.9748\n",
            "Epoch: 3324, Train Loss: 0.9461, Valid Loss: 0.9751\n",
            "Epoch: 3325, Train Loss: 0.9475, Valid Loss: 0.9750\n",
            "Epoch: 3326, Train Loss: 0.9461, Valid Loss: 0.9753\n",
            "Epoch: 3327, Train Loss: 0.9471, Valid Loss: 0.9761\n",
            "Epoch: 3328, Train Loss: 0.9465, Valid Loss: 0.9751\n",
            "Epoch: 3329, Train Loss: 0.9469, Valid Loss: 0.9742\n",
            "Epoch: 3330, Train Loss: 0.9464, Valid Loss: 0.9754\n",
            "Epoch: 3331, Train Loss: 0.9459, Valid Loss: 0.9742\n",
            "Epoch: 3332, Train Loss: 0.9468, Valid Loss: 0.9758\n",
            "Epoch: 3333, Train Loss: 0.9473, Valid Loss: 0.9754\n",
            "Epoch: 3334, Train Loss: 0.9470, Valid Loss: 0.9739\n",
            "Epoch: 3335, Train Loss: 0.9460, Valid Loss: 0.9763\n",
            "Epoch: 3336, Train Loss: 0.9477, Valid Loss: 0.9772\n",
            "Epoch: 3337, Train Loss: 0.9467, Valid Loss: 0.9744\n",
            "Epoch: 3338, Train Loss: 0.9475, Valid Loss: 0.9750\n",
            "Epoch: 3339, Train Loss: 0.9471, Valid Loss: 0.9761\n",
            "Epoch: 3340, Train Loss: 0.9465, Valid Loss: 0.9759\n",
            "Epoch: 3341, Train Loss: 0.9479, Valid Loss: 0.9760\n",
            "Epoch: 3342, Train Loss: 0.9478, Valid Loss: 0.9745\n",
            "Epoch: 3343, Train Loss: 0.9471, Valid Loss: 0.9748\n",
            "Epoch: 3344, Train Loss: 0.9456, Valid Loss: 0.9753\n",
            "Epoch: 3345, Train Loss: 0.9471, Valid Loss: 0.9744\n",
            "Epoch: 3346, Train Loss: 0.9479, Valid Loss: 0.9762\n",
            "Epoch: 3347, Train Loss: 0.9474, Valid Loss: 0.9750\n",
            "Epoch: 3348, Train Loss: 0.9478, Valid Loss: 0.9743\n",
            "Epoch: 3349, Train Loss: 0.9465, Valid Loss: 0.9742\n",
            "Epoch: 3350, Train Loss: 0.9467, Valid Loss: 0.9754\n",
            "Epoch: 3351, Train Loss: 0.9474, Valid Loss: 0.9756\n",
            "Epoch: 3352, Train Loss: 0.9479, Valid Loss: 0.9753\n",
            "Epoch: 3353, Train Loss: 0.9473, Valid Loss: 0.9746\n",
            "Epoch: 3354, Train Loss: 0.9476, Valid Loss: 0.9749\n",
            "Epoch: 3355, Train Loss: 0.9478, Valid Loss: 0.9766\n",
            "Epoch: 3356, Train Loss: 0.9476, Valid Loss: 0.9753\n",
            "Epoch: 3357, Train Loss: 0.9460, Valid Loss: 0.9743\n",
            "Epoch: 3358, Train Loss: 0.9459, Valid Loss: 0.9765\n",
            "Epoch: 3359, Train Loss: 0.9479, Valid Loss: 0.9755\n",
            "Epoch: 3360, Train Loss: 0.9445, Valid Loss: 0.9753\n",
            "Epoch: 3361, Train Loss: 0.9468, Valid Loss: 0.9756\n",
            "Epoch: 3362, Train Loss: 0.9459, Valid Loss: 0.9768\n",
            "Epoch: 3363, Train Loss: 0.9478, Valid Loss: 0.9742\n",
            "Epoch: 3364, Train Loss: 0.9466, Valid Loss: 0.9753\n",
            "Epoch: 3365, Train Loss: 0.9479, Valid Loss: 0.9754\n",
            "Epoch: 3366, Train Loss: 0.9474, Valid Loss: 0.9759\n",
            "Epoch: 3367, Train Loss: 0.9469, Valid Loss: 0.9753\n",
            "Epoch: 3368, Train Loss: 0.9478, Valid Loss: 0.9747\n",
            "Epoch: 3369, Train Loss: 0.9466, Valid Loss: 0.9759\n",
            "Epoch: 3370, Train Loss: 0.9474, Valid Loss: 0.9746\n",
            "Epoch: 3371, Train Loss: 0.9478, Valid Loss: 0.9763\n",
            "Epoch: 3372, Train Loss: 0.9475, Valid Loss: 0.9746\n",
            "Epoch: 3373, Train Loss: 0.9459, Valid Loss: 0.9754\n",
            "Epoch: 3374, Train Loss: 0.9468, Valid Loss: 0.9754\n",
            "Epoch: 3375, Train Loss: 0.9465, Valid Loss: 0.9743\n",
            "Epoch: 3376, Train Loss: 0.9478, Valid Loss: 0.9760\n",
            "Epoch: 3377, Train Loss: 0.9478, Valid Loss: 0.9743\n",
            "Epoch: 3378, Train Loss: 0.9481, Valid Loss: 0.9755\n",
            "Epoch: 3379, Train Loss: 0.9467, Valid Loss: 0.9754\n",
            "Epoch: 3380, Train Loss: 0.9472, Valid Loss: 0.9744\n",
            "Epoch: 3381, Train Loss: 0.9477, Valid Loss: 0.9758\n",
            "Epoch: 3382, Train Loss: 0.9476, Valid Loss: 0.9755\n",
            "Epoch: 3383, Train Loss: 0.9468, Valid Loss: 0.9761\n",
            "Epoch: 3384, Train Loss: 0.9466, Valid Loss: 0.9737\n",
            "Epoch: 3385, Train Loss: 0.9471, Valid Loss: 0.9761\n",
            "Epoch: 3386, Train Loss: 0.9467, Valid Loss: 0.9746\n",
            "Epoch: 3387, Train Loss: 0.9456, Valid Loss: 0.9761\n",
            "Epoch: 3388, Train Loss: 0.9464, Valid Loss: 0.9753\n",
            "Epoch: 3389, Train Loss: 0.9467, Valid Loss: 0.9764\n",
            "Epoch: 3390, Train Loss: 0.9478, Valid Loss: 0.9753\n",
            "Epoch: 3391, Train Loss: 0.9481, Valid Loss: 0.9758\n",
            "Epoch: 3392, Train Loss: 0.9475, Valid Loss: 0.9743\n",
            "Epoch: 3393, Train Loss: 0.9466, Valid Loss: 0.9750\n",
            "Epoch: 3394, Train Loss: 0.9477, Valid Loss: 0.9766\n",
            "Epoch: 3395, Train Loss: 0.9467, Valid Loss: 0.9746\n",
            "Epoch: 3396, Train Loss: 0.9476, Valid Loss: 0.9750\n",
            "Epoch: 3397, Train Loss: 0.9469, Valid Loss: 0.9752\n",
            "Epoch: 3398, Train Loss: 0.9470, Valid Loss: 0.9755\n",
            "Epoch: 3399, Train Loss: 0.9477, Valid Loss: 0.9764\n",
            "Epoch: 3400, Train Loss: 0.9474, Valid Loss: 0.9745\n",
            "Epoch: 3401, Train Loss: 0.9479, Valid Loss: 0.9765\n",
            "Epoch: 3402, Train Loss: 0.9475, Valid Loss: 0.9749\n",
            "Epoch: 3403, Train Loss: 0.9477, Valid Loss: 0.9736\n",
            "Epoch: 3404, Train Loss: 0.9482, Valid Loss: 0.9751\n",
            "Epoch: 3405, Train Loss: 0.9468, Valid Loss: 0.9780\n",
            "Epoch: 3406, Train Loss: 0.9475, Valid Loss: 0.9749\n",
            "Epoch: 3407, Train Loss: 0.9468, Valid Loss: 0.9749\n",
            "Epoch: 3408, Train Loss: 0.9476, Valid Loss: 0.9747\n",
            "Epoch: 3409, Train Loss: 0.9466, Valid Loss: 0.9761\n",
            "Epoch: 3410, Train Loss: 0.9477, Valid Loss: 0.9754\n",
            "Epoch: 3411, Train Loss: 0.9463, Valid Loss: 0.9749\n",
            "Epoch: 3412, Train Loss: 0.9470, Valid Loss: 0.9755\n",
            "Epoch: 3413, Train Loss: 0.9468, Valid Loss: 0.9761\n",
            "Epoch: 3414, Train Loss: 0.9470, Valid Loss: 0.9748\n",
            "Epoch: 3415, Train Loss: 0.9455, Valid Loss: 0.9751\n",
            "Epoch: 3416, Train Loss: 0.9457, Valid Loss: 0.9757\n",
            "Epoch: 3417, Train Loss: 0.9473, Valid Loss: 0.9753\n",
            "Epoch: 3418, Train Loss: 0.9463, Valid Loss: 0.9749\n",
            "Epoch: 3419, Train Loss: 0.9475, Valid Loss: 0.9761\n",
            "Epoch: 3420, Train Loss: 0.9465, Valid Loss: 0.9747\n",
            "Epoch: 3421, Train Loss: 0.9480, Valid Loss: 0.9751\n",
            "Epoch: 3422, Train Loss: 0.9457, Valid Loss: 0.9758\n",
            "Epoch: 3423, Train Loss: 0.9467, Valid Loss: 0.9757\n",
            "Epoch: 3424, Train Loss: 0.9455, Valid Loss: 0.9748\n",
            "Epoch: 3425, Train Loss: 0.9472, Valid Loss: 0.9748\n",
            "Epoch: 3426, Train Loss: 0.9464, Valid Loss: 0.9758\n",
            "Epoch: 3427, Train Loss: 0.9475, Valid Loss: 0.9756\n",
            "Epoch: 3428, Train Loss: 0.9478, Valid Loss: 0.9757\n",
            "Epoch: 3429, Train Loss: 0.9477, Valid Loss: 0.9748\n",
            "Epoch: 3430, Train Loss: 0.9480, Valid Loss: 0.9749\n",
            "Epoch: 3431, Train Loss: 0.9481, Valid Loss: 0.9757\n",
            "Epoch: 3432, Train Loss: 0.9462, Valid Loss: 0.9747\n",
            "Epoch: 3433, Train Loss: 0.9482, Valid Loss: 0.9774\n",
            "Epoch: 3434, Train Loss: 0.9478, Valid Loss: 0.9760\n",
            "Epoch: 3435, Train Loss: 0.9480, Valid Loss: 0.9733\n",
            "Epoch: 3436, Train Loss: 0.9471, Valid Loss: 0.9740\n",
            "Epoch: 3437, Train Loss: 0.9464, Valid Loss: 0.9775\n",
            "Epoch: 3438, Train Loss: 0.9461, Valid Loss: 0.9746\n",
            "Epoch: 3439, Train Loss: 0.9465, Valid Loss: 0.9754\n",
            "Epoch: 3440, Train Loss: 0.9477, Valid Loss: 0.9752\n",
            "Epoch: 3441, Train Loss: 0.9484, Valid Loss: 0.9753\n",
            "Epoch: 3442, Train Loss: 0.9464, Valid Loss: 0.9753\n",
            "Epoch: 3443, Train Loss: 0.9479, Valid Loss: 0.9752\n",
            "Epoch: 3444, Train Loss: 0.9469, Valid Loss: 0.9755\n",
            "Epoch: 3445, Train Loss: 0.9475, Valid Loss: 0.9754\n",
            "Epoch: 3446, Train Loss: 0.9465, Valid Loss: 0.9754\n",
            "Epoch: 3447, Train Loss: 0.9465, Valid Loss: 0.9751\n",
            "Epoch: 3448, Train Loss: 0.9479, Valid Loss: 0.9767\n",
            "Epoch: 3449, Train Loss: 0.9466, Valid Loss: 0.9742\n",
            "Epoch: 3450, Train Loss: 0.9481, Valid Loss: 0.9754\n",
            "Epoch: 3451, Train Loss: 0.9460, Valid Loss: 0.9747\n",
            "Epoch: 3452, Train Loss: 0.9471, Valid Loss: 0.9743\n",
            "Epoch: 3453, Train Loss: 0.9467, Valid Loss: 0.9763\n",
            "Epoch: 3454, Train Loss: 0.9465, Valid Loss: 0.9764\n",
            "Epoch: 3455, Train Loss: 0.9469, Valid Loss: 0.9751\n",
            "Epoch: 3456, Train Loss: 0.9468, Valid Loss: 0.9767\n",
            "Epoch: 3457, Train Loss: 0.9480, Valid Loss: 0.9732\n",
            "Epoch: 3458, Train Loss: 0.9473, Valid Loss: 0.9753\n",
            "Epoch: 3459, Train Loss: 0.9468, Valid Loss: 0.9754\n",
            "Epoch: 3460, Train Loss: 0.9472, Valid Loss: 0.9753\n",
            "Epoch: 3461, Train Loss: 0.9474, Valid Loss: 0.9747\n",
            "Epoch: 3462, Train Loss: 0.9480, Valid Loss: 0.9754\n",
            "Epoch: 3463, Train Loss: 0.9479, Valid Loss: 0.9758\n",
            "Epoch: 3464, Train Loss: 0.9465, Valid Loss: 0.9751\n",
            "Epoch: 3465, Train Loss: 0.9477, Valid Loss: 0.9762\n",
            "Epoch: 3466, Train Loss: 0.9470, Valid Loss: 0.9752\n",
            "Epoch: 3467, Train Loss: 0.9470, Valid Loss: 0.9750\n",
            "Epoch: 3468, Train Loss: 0.9476, Valid Loss: 0.9755\n",
            "Epoch: 3469, Train Loss: 0.9460, Valid Loss: 0.9749\n",
            "Epoch: 3470, Train Loss: 0.9480, Valid Loss: 0.9753\n",
            "Epoch: 3471, Train Loss: 0.9473, Valid Loss: 0.9748\n",
            "Epoch: 3472, Train Loss: 0.9477, Valid Loss: 0.9762\n",
            "Epoch: 3473, Train Loss: 0.9480, Valid Loss: 0.9756\n",
            "Epoch: 3474, Train Loss: 0.9482, Valid Loss: 0.9746\n",
            "Epoch: 3475, Train Loss: 0.9467, Valid Loss: 0.9751\n",
            "Epoch: 3476, Train Loss: 0.9473, Valid Loss: 0.9755\n",
            "Epoch: 3477, Train Loss: 0.9476, Valid Loss: 0.9754\n",
            "Epoch: 3478, Train Loss: 0.9478, Valid Loss: 0.9746\n",
            "Epoch: 3479, Train Loss: 0.9480, Valid Loss: 0.9758\n",
            "Epoch: 3480, Train Loss: 0.9452, Valid Loss: 0.9755\n",
            "Epoch: 3481, Train Loss: 0.9472, Valid Loss: 0.9759\n",
            "Epoch: 3482, Train Loss: 0.9448, Valid Loss: 0.9744\n",
            "Epoch: 3483, Train Loss: 0.9468, Valid Loss: 0.9757\n",
            "Epoch: 3484, Train Loss: 0.9468, Valid Loss: 0.9762\n",
            "Epoch: 3485, Train Loss: 0.9475, Valid Loss: 0.9760\n",
            "Epoch: 3486, Train Loss: 0.9473, Valid Loss: 0.9747\n",
            "Epoch: 3487, Train Loss: 0.9475, Valid Loss: 0.9751\n",
            "Early stop at epoch 3487.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofKvCpzhXAAt"
      },
      "source": [
        "# **Visualize Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "q9Y8hjERe4RK",
        "outputId": "9e8e280d-e30a-46dd-f91b-bb07ce59ce22"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "def plot_pred(dataloader, model, device, lim=35., y_hats=None, y_targets=None):\n",
        "    \"\"\" Plot prediction of your DNN \"\"\"\n",
        "    if y_hats is None or y_targets is None:\n",
        "        model.eval()\n",
        "        y_hats, y_targets = [], []\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                y_hat = model(x)\n",
        "                y_hats.append(y_hat.detach().cpu())\n",
        "                y_targets.append(y.detach().cpu())\n",
        "        y_hats = torch.cat(y_hats, dim=0).numpy()\n",
        "        y_targets = torch.cat(y_targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(y_targets, y_hats, c='r', alpha=0.5)\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel(\"ground truth value\")\n",
        "    plt.ylabel(\"predicted value\")\n",
        "    plt.title(\"Ground Truth v.s. Prediction\")\n",
        "    plt.show()\n",
        "\n",
        "del model\n",
        "model = NeuralNetwork(train_dataloader.dataset.features_num).to(device)\n",
        "ckpt = torch.load(config[\"save_path\"], map_location=\"cpu\")  \n",
        "model.load_state_dict(ckpt)\n",
        "plot_pred(valid_dataloader, model, device)  "
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXbuv2kSYrPf"
      },
      "source": [
        "# **Save Output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FeXkWTmhfOE",
        "outputId": "42784184-6775-4fb3-a9c7-fba0b2283915"
      },
      "source": [
        "def test(test_dataloader, model, device):\n",
        "    model.eval()                               \n",
        "    y_hats = []\n",
        "    for x in test_dataloader:                         \n",
        "        x = x.to(device)                       \n",
        "        with torch.no_grad():                   \n",
        "            y_hat = model(x)                     \n",
        "            y_hats.append(y_hat.detach().cpu())  \n",
        "    y_hats = torch.cat(y_hats, dim=0).numpy()     \n",
        "    return y_hats\n",
        "\n",
        "def save_y_hat(y_hats, file):\n",
        "    \"\"\" Save predictions to specified file \"\"\"\n",
        "    print(\"Saving results to {}\".format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow([\"id\", \"tested_positive\"])\n",
        "        for i, y in enumerate(y_hats):\n",
        "            writer.writerow([i, y])\n",
        "\n",
        "y_hats = test(test_dataloader, model, device) \n",
        "save_y_hat(y_hats, \"y_hat.csv\")      "
      ],
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to y_hat.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmXfZQZ4Rlam"
      },
      "source": [
        "# **Reference**\n",
        "\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ]
    }
  ]
}